
<BugCollection sequence='0' release='' analysisTimestamp='1568279958081' version='3.1.12' timestamp='1568278143000'><Project projectName='Apache HBase - MapReduce'><Jar>./hbase/hbase-mapreduce/target/classes</Jar><AuxClasspathEntry>.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-miscellaneous/2.1.0/hbase-shaded-miscellaneous-2.1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-netty/2.1.0/hbase-shaded-netty-2.1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/thirdparty/hbase-shaded-protobuf/2.1.0/hbase-shaded-protobuf-2.1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-common/2.1.6/hbase-common-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-zookeeper/2.1.6/hbase-zookeeper-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-auth/2.7.7/hadoop-auth-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/httpcomponents/httpclient/4.5.3/httpclient-4.5.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/httpcomponents/httpcore/4.4.6/httpcore-4.4.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-protocol/2.1.6/hbase-protocol-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-protocol-shaded/2.1.6/hbase-protocol-shaded-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-metrics/2.1.6/hbase-metrics-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-metrics-api/2.1.6/hbase-metrics-api-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/io/dropwizard/metrics/metrics-core/3.2.6/metrics-core-3.2.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/htrace/htrace-core4/4.2.0-incubating/htrace-core4-4.2.0-incubating.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-client/2.1.6/hbase-client-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/jruby/jcodings/jcodings/1.0.18/jcodings-1.0.18.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/jruby/joni/joni/2.1.11/joni-2.1.11.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-hadoop-compat/2.1.6/hbase-hadoop-compat-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-hadoop2-compat/2.1.6/hbase-hadoop2-compat-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-server/2.1.6/hbase-server-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-http/2.1.6/hbase-http-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-util/9.3.27.v20190418/jetty-util-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.27.v20190418/jetty-util-ajax-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-http/9.3.27.v20190418/jetty-http-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-security/9.3.27.v20190418/jetty-security-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/core/jersey-server/2.25.1/jersey-server-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/core/jersey-common/2.25.1/jersey-common-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.25.1/jersey-guava-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/core/jersey-client/2.25.1/jersey-client-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.25.1/jersey-media-jaxb-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/hk2-api/2.5.0-b32/hk2-api-2.5.0-b32.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/hk2-utils/2.5.0-b32/hk2-utils-2.5.0-b32.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.5.0-b32/aopalliance-repackaged-2.5.0-b32.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/external/javax.inject/2.5.0-b32/javax.inject-2.5.0-b32.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/hk2/hk2-locator/2.5.0-b32/hk2-locator-2.5.0-b32.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/javassist/javassist/3.20.0-GA/javassist-3.20.0-GA.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.25.1/jersey-container-servlet-core-2.25.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-procedure/2.1.6/hbase-procedure-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-server/9.3.27.v20190418/jetty-server-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-io/9.3.27.v20190418/jetty-io-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.27.v20190418/jetty-servlet-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.27.v20190418/jetty-webapp-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-xml/9.3.27.v20190418/jetty-xml-9.3.27.v20190418.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/web/javax.servlet.jsp/2.3.2/javax.servlet.jsp-2.3.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/glassfish/javax.el/3.0.1-b08/javax.el-3.0.1-b08.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/servlet/jsp/javax.servlet.jsp-api/2.3.1/javax.servlet.jsp-api-2.3.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/lmax/disruptor/3.3.6/disruptor-3.3.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-distcp/2.7.7/hadoop-distcp-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.7/hadoop-annotations-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hbase/hbase-replication/2.1.6/hbase-replication-2.1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jettison/jettison/1.3.8/jettison-1.3.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/asm/asm/3.1/asm-3.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-lang3/3.6/commons-lang3-3.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.9.2/jackson-databind-2.9.9.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.9/jackson-core-2.9.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.9/jackson-annotations-2.9.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-common/2.7.7/hadoop-common-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-client/4.0.0/curator-client-4.0.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-recipes/4.0.0/curator-recipes-4.0.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-hdfs/2.7.7/hadoop-hdfs-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.7/hadoop-mapreduce-client-core-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.7.7/hadoop-yarn-common-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/xml/bind/jaxb-api/2.2.12/jaxb-api-2.2.12.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.7.7/hadoop-yarn-api-2.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar</AuxClasspathEntry><SrcDir>./hbase/hbase-mapreduce/src/main/java</SrcDir><WrkDir>./hbase/hbase-mapreduce/target</WrkDir></Project><BugInstance instanceOccurrenceNum='0' instanceHash='99d8fc6b6624d9d97a02f0ab78dc930b' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>GroupingTableMap.columns not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapred.GroupingTableMap.extractKeyValues(Result)</LongMessage><Class classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' start='39' end='154' sourcepath='org/apache/hadoop/hbase/mapred/GroupingTableMap.java' sourcefile='GroupingTableMap.java'><Message>At GroupingTableMap.java:[lines 39-154]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapred.GroupingTableMap</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' signature='[[B' name='columns' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' sourcepath='org/apache/hadoop/hbase/mapred/GroupingTableMap.java' sourcefile='GroupingTableMap.java'><Message>In GroupingTableMap.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapred.GroupingTableMap.columns</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' signature='(Lorg/apache/hadoop/hbase/client/Result;)[[B' name='extractKeyValues' primary='true'><SourceLine endBytecode='386' classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' start='115' end='133' sourcepath='org/apache/hadoop/hbase/mapred/GroupingTableMap.java' sourcefile='GroupingTableMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapred.GroupingTableMap.extractKeyValues(Result)</Message></Method><SourceLine endBytecode='17' classname='org.apache.hadoop.hbase.mapred.GroupingTableMap' start='117' end='117' sourcepath='org/apache/hadoop/hbase/mapred/GroupingTableMap.java' sourcefile='GroupingTableMap.java' startBytecode='17' primary='true'><Message>At GroupingTableMap.java:[line 117]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4a4bd1936e97299546f92e5a4a526a01' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapred.InputSplit to org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit in org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat.getRecordReader(InputSplit, JobConf, Reporter)</LongMessage><Class classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' start='91' end='127' sourcepath='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' sourcefile='MultiTableSnapshotInputFormat.java'><Message>At MultiTableSnapshotInputFormat.java:[lines 91-127]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' signature='(Lorg/apache/hadoop/mapred/InputSplit;Lorg/apache/hadoop/mapred/JobConf;Lorg/apache/hadoop/mapred/Reporter;)Lorg/apache/hadoop/mapred/RecordReader;' name='getRecordReader' primary='true'><SourceLine endBytecode='6' classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' start='108' end='108' sourcepath='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' sourcefile='MultiTableSnapshotInputFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat.getRecordReader(InputSplit, JobConf, Reporter)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapred/InputSplit;'><SourceLine classname='org.apache.hadoop.mapred.InputSplit' sourcepath='org/apache/hadoop/mapred/InputSplit.java' sourcefile='InputSplit.java'><Message>In InputSplit.java</Message></SourceLine><Message>Actual type org.apache.hadoop.mapred.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapred/TableSnapshotInputFormat$TableSnapshotRegionSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit' start='50' end='86' sourcepath='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java'><Message>At TableSnapshotInputFormat.java:[lines 50-86]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='4' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='5' classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' start='108' end='108' sourcepath='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' sourcefile='MultiTableSnapshotInputFormat.java' startBytecode='5' primary='true'><Message>At MultiTableSnapshotInputFormat.java:[line 108]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a96628d5b543599a7956cf8ea1b67f66' rank='20' abbrev='RI' category='STYLE' priority='3' type='RI_REDUNDANT_INTERFACES' instanceOccurrenceMax='0'><ShortMessage>Class implements same interface as superclass</ShortMessage><LongMessage>Class org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat implements same interface as superclass</LongMessage><Class classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' start='91' end='127' sourcepath='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' sourcefile='MultiTableSnapshotInputFormat.java'><Message>At MultiTableSnapshotInputFormat.java:[lines 91-127]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat</Message></Class><Class role='INTERFACE_TYPE' classname='org.apache.hadoop.mapred.InputFormat'><SourceLine classname='org.apache.hadoop.mapred.InputFormat' sourcepath='org/apache/hadoop/mapred/InputFormat.java' sourcefile='InputFormat.java'><Message>In InputFormat.java</Message></SourceLine><Message>Interface org.apache.hadoop.mapred.InputFormat</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat' start='91' end='127' sourcepath='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' sourcefile='MultiTableSnapshotInputFormat.java'><Message>At MultiTableSnapshotInputFormat.java:[lines 91-127]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='98f020dcb93fef5635dbbad85bf7b188' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapred.InputSplit to org.apache.hadoop.hbase.mapred.TableSplit in org.apache.hadoop.hbase.mapred.TableInputFormatBase.getRecordReader(InputSplit, JobConf, Reporter)</LongMessage><Class classname='org.apache.hadoop.hbase.mapred.TableInputFormatBase' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.TableInputFormatBase' start='80' end='312' sourcepath='org/apache/hadoop/hbase/mapred/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java'><Message>At TableInputFormatBase.java:[lines 80-312]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapred.TableInputFormatBase</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapred.TableInputFormatBase' signature='(Lorg/apache/hadoop/mapred/InputSplit;Lorg/apache/hadoop/mapred/JobConf;Lorg/apache/hadoop/mapred/Reporter;)Lorg/apache/hadoop/mapred/RecordReader;' name='getRecordReader' primary='true'><SourceLine endBytecode='62' classname='org.apache.hadoop.hbase.mapred.TableInputFormatBase' start='108' end='131' sourcepath='org/apache/hadoop/hbase/mapred/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapred.TableInputFormatBase.getRecordReader(InputSplit, JobConf, Reporter)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapred/InputSplit;'><SourceLine classname='org.apache.hadoop.mapred.InputSplit' sourcepath='org/apache/hadoop/mapred/InputSplit.java' sourcefile='InputSplit.java'><Message>In InputSplit.java</Message></SourceLine><Message>Actual type org.apache.hadoop.mapred.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapred/TableSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapred.TableSplit' start='35' end='152' sourcepath='org/apache/hadoop/hbase/mapred/TableSplit.java' sourcefile='TableSplit.java'><Message>At TableSplit.java:[lines 35-152]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapred.TableSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='46' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='47' classname='org.apache.hadoop.hbase.mapred.TableInputFormatBase' start='121' end='121' sourcepath='org/apache/hadoop/hbase/mapred/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java' startBytecode='47' primary='true'><Message>At TableInputFormatBase.java:[line 121]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='81cd4496529f957fff2a411c094fe9e0' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapred.InputSplit to org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit in org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.getRecordReader(InputSplit, JobConf, Reporter)</LongMessage><Class classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat' start='48' end='182' sourcepath='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java'><Message>At TableSnapshotInputFormat.java:[lines 48-182]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat' signature='(Lorg/apache/hadoop/mapred/InputSplit;Lorg/apache/hadoop/mapred/JobConf;Lorg/apache/hadoop/mapred/Reporter;)Lorg/apache/hadoop/mapred/RecordReader;' name='getRecordReader' primary='true'><SourceLine endBytecode='6' classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat' start='151' end='151' sourcepath='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.getRecordReader(InputSplit, JobConf, Reporter)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapred/InputSplit;'><SourceLine classname='org.apache.hadoop.mapred.InputSplit' sourcepath='org/apache/hadoop/mapred/InputSplit.java' sourcefile='InputSplit.java'><Message>In InputSplit.java</Message></SourceLine><Message>Actual type org.apache.hadoop.mapred.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapred/TableSnapshotInputFormat$TableSnapshotRegionSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit' start='50' end='86' sourcepath='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java'><Message>At TableSnapshotInputFormat.java:[lines 50-86]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='4' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='5' classname='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat' start='151' end='151' sourcepath='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java' startBytecode='5' primary='true'><Message>At TableSnapshotInputFormat.java:[line 151]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ff9e1e1cfac804c1af51ddee0967686c' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hbase.mapreduce.CellCounter.getTimeRange(String[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.CellCounter' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellCounter' start='73' end='334' sourcepath='org/apache/hadoop/hbase/mapreduce/CellCounter.java' sourcefile='CellCounter.java'><Message>At CellCounter.java:[lines 73-334]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.CellCounter</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hbase.mapreduce.CellCounter' signature='([Ljava/lang/String;)[J' name='getTimeRange' primary='true'><SourceLine endBytecode='352' classname='org.apache.hadoop.hbase.mapreduce.CellCounter' start='269' end='288' sourcepath='org/apache/hadoop/hbase/mapreduce/CellCounter.java' sourcefile='CellCounter.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.CellCounter.getTimeRange(String[])</Message></Method><SourceLine endBytecode='136' classname='org.apache.hadoop.hbase.mapreduce.CellCounter' start='285' end='285' sourcepath='org/apache/hadoop/hbase/mapreduce/CellCounter.java' sourcefile='CellCounter.java' startBytecode='136' primary='true'><Message>At CellCounter.java:[line 285]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d7353bf8508aeeb32f582047f4d3a2e8' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>CellSerialization$CellDeserializer.dis not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer.close()</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' start='56' end='73' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>At CellSerialization.java:[lines 56-73]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' signature='Ljava/io/DataInputStream;' name='dis' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>In CellSerialization.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer.dis</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' signature='()V' name='close' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' start='61' end='62' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer.close()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer' start='61' end='61' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='4' primary='true'><Message>At CellSerialization.java:[line 61]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='aef147f7394fd58b1ce32f8926b068fd' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>CellSerialization$CellSerializer.dos not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.close()</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='76' end='93' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>At CellSerialization.java:[lines 76-93]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' signature='Ljava/io/DataOutputStream;' name='dos' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>In CellSerialization.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.dos</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' signature='()V' name='close' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='81' end='82' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.close()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='81' end='81' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='4' primary='true'><Message>At CellSerialization.java:[line 81]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e1259f086e1279308a11b727967bd93f' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>CellSerialization$CellSerializer.dos not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.serialize(Cell)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='76' end='93' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>At CellSerialization.java:[lines 76-93]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' signature='Ljava/io/DataOutputStream;' name='dos' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java'><Message>In CellSerialization.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.dos</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' signature='(Lorg/apache/hadoop/hbase/Cell;)V' name='serialize' primary='true'><SourceLine endBytecode='83' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='91' end='93' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer.serialize(Cell)</Message></Method><SourceLine endBytecode='10' classname='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer' start='91' end='91' sourcepath='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' sourcefile='CellSerialization.java' startBytecode='10' primary='true'><Message>At CellSerialization.java:[line 91]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='39998dca31728db88f091392d229c400' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.mapreduce.CopyTable.doCommandLine(String[])</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.CopyTable' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.CopyTable' start='52' end='386' sourcepath='org/apache/hadoop/hbase/mapreduce/CopyTable.java' sourcefile='CopyTable.java'><Message>At CopyTable.java:[lines 52-386]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.CopyTable</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.CopyTable' signature='([Ljava/lang/String;)Z' name='doCommandLine' primary='true'><SourceLine endBytecode='1259' classname='org.apache.hadoop.hbase.mapreduce.CopyTable' start='229' end='347' sourcepath='org/apache/hadoop/hbase/mapreduce/CopyTable.java' sourcefile='CopyTable.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.CopyTable.doCommandLine(String[])</Message></Method><SourceLine endBytecode='574' classname='org.apache.hadoop.hbase.mapreduce.CopyTable' start='342' end='342' sourcepath='org/apache/hadoop/hbase/mapreduce/CopyTable.java' sourcefile='CopyTable.java' startBytecode='574' primary='true'><Message>At CopyTable.java:[line 342]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='12970fa6500dc0e9733faf9d26a266b2' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>GroupingTableMapper.columns not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.extractKeyValues(Result)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' start='38' end='174' sourcepath='org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java' sourcefile='GroupingTableMapper.java'><Message>At GroupingTableMapper.java:[lines 38-174]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.GroupingTableMapper</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' signature='[[B' name='columns' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' sourcepath='org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java' sourcefile='GroupingTableMapper.java'><Message>In GroupingTableMapper.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.columns</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' signature='(Lorg/apache/hadoop/hbase/client/Result;)[[B' name='extractKeyValues' primary='true'><SourceLine endBytecode='386' classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' start='105' end='123' sourcepath='org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java' sourcefile='GroupingTableMapper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.extractKeyValues(Result)</Message></Method><SourceLine endBytecode='17' classname='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper' start='107' end='107' sourcepath='org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java' sourcefile='GroupingTableMapper.java' startBytecode='17' primary='true'><Message>At GroupingTableMapper.java:[line 107]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='85fbae28ff4bfa2d39c0ba5b0cb05108' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.mapreduce.lib.input.FileSplit in org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader.initialize(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader' start='69' end='141' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java' sourcefile='HFileInputFormat.java'><Message>At HFileInputFormat.java:[lines 69-141]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)V' name='initialize' primary='true'><SourceLine endBytecode='37' classname='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader' start='85' end='97' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java' sourcefile='HFileInputFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader.initialize(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/mapreduce/lib/input/FileSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.lib.input.FileSplit' start='48' end='133' sourcepath='org/apache/hadoop/mapreduce/lib/input/FileSplit.java' sourcefile='FileSplit.java'><Message>At FileSplit.java:[lines 48-133]</Message></SourceLine><Message>Expected org.apache.hadoop.mapreduce.lib.input.FileSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='0' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='1' classname='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader' start='85' end='85' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java' sourcefile='HFileInputFormat.java' startBytecode='1' primary='true'><Message>At HFileInputFormat.java:[line 85]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='eddeb27697a221cc4acc58812536914' rank='6' abbrev='EC' category='CORRECTNESS' priority='3' type='EC_ARRAY_AND_NONARRAY' instanceOccurrenceMax='0'><ShortMessage>equals() used to compare array and nonarray</ShortMessage><LongMessage>Calling org.apache.hadoop.hbase.io.ImmutableBytesWritable.equals(byte[]) in org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.writePartitions(Configuration, Path, List, boolean)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' start='106' end='882' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java' sourcefile='HFileOutputFormat2.java'><Message>At HFileOutputFormat2.java:[lines 106-882]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' signature='(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/fs/Path;Ljava/util/List;Z)V' name='writePartitions' primary='true'><SourceLine endBytecode='95' classname='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' start='509' end='544' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java' sourcefile='HFileOutputFormat2.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.writePartitions(Configuration, Path, List, boolean)</Message></Method><Type role='TYPE_FOUND' descriptor='[B'><Message>Actual type byte[]</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/io/ImmutableBytesWritable;'><SourceLine classname='org.apache.hadoop.hbase.io.ImmutableBytesWritable' start='41' end='274' sourcepath='org/apache/hadoop/hbase/io/ImmutableBytesWritable.java' sourcefile='ImmutableBytesWritable.java'><Message>At ImmutableBytesWritable.java:[lines 41-274]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.io.ImmutableBytesWritable</Message></Type><Field isStatic='true' role='FIELD_VALUE_OF' classname='org.apache.hadoop.hbase.HConstants' signature='[B' name='EMPTY_BYTE_ARRAY' primary='true'><SourceLine classname='org.apache.hadoop.hbase.HConstants' sourcepath='org/apache/hadoop/hbase/HConstants.java' sourcefile='HConstants.java'><Message>In HConstants.java</Message></SourceLine><Message>Value loaded from field org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY</Message></Field><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='95' name='first' register='5'><Message>Value loaded from first</Message></LocalVariable><SourceLine endBytecode='98' classname='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' start='524' end='524' sourcepath='org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java' sourcefile='HFileOutputFormat2.java' startBytecode='98' primary='true'><Message>At HFileOutputFormat2.java:[line 524]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8e820348ad30eab2930945874be51f86' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.mapreduce.HashTable.doCommandLine(String[])</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.HashTable' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.HashTable' start='67' end='745' sourcepath='org/apache/hadoop/hbase/mapreduce/HashTable.java' sourcefile='HashTable.java'><Message>At HashTable.java:[lines 67-745]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.HashTable</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.HashTable' signature='([Ljava/lang/String;)Z' name='doCommandLine' primary='true'><SourceLine endBytecode='1072' classname='org.apache.hadoop.hbase.mapreduce.HashTable' start='635' end='720' sourcepath='org/apache/hadoop/hbase/mapreduce/HashTable.java' sourcefile='HashTable.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.HashTable.doCommandLine(String[])</Message></Method><SourceLine endBytecode='517' classname='org.apache.hadoop.hbase.mapreduce.HashTable' start='715' end='715' sourcepath='org/apache/hadoop/hbase/mapreduce/HashTable.java' sourcefile='HashTable.java' startBytecode='517' primary='true'><Message>At HashTable.java:[line 715]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2ec690a6a4f1255558038c7086b49aa9' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hbase.mapreduce.Import$Importer.writeResult(ImmutableBytesWritable, Result, Mapper$Context)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='505' end='634' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java'><Message>At Import.java:[lines 505-634]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.Import$Importer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' signature='(Lorg/apache/hadoop/hbase/io/ImmutableBytesWritable;Lorg/apache/hadoop/hbase/client/Result;Lorg/apache/hadoop/mapreduce/Mapper$Context;)V' name='writeResult' primary='true'><SourceLine endBytecode='45' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='530' end='541' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.Import$Importer.writeResult(ImmutableBytesWritable, Result, Mapper$Context)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='95' name='delete' register='5'><Message>Value loaded from delete</Message></LocalVariable><SourceLine endBytecode='97' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='539' end='539' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java' startBytecode='97' primary='true'><Message>At Import.java:[line 539]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dab6010dd77df1f80df52e0b69102eed' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hbase.mapreduce.Import$Importer.writeResult(ImmutableBytesWritable, Result, Mapper$Context)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='505' end='634' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java'><Message>At Import.java:[lines 505-634]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.Import$Importer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' signature='(Lorg/apache/hadoop/hbase/io/ImmutableBytesWritable;Lorg/apache/hadoop/hbase/client/Result;Lorg/apache/hadoop/mapreduce/Mapper$Context;)V' name='writeResult' primary='true'><SourceLine endBytecode='45' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='530' end='541' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.Import$Importer.writeResult(ImmutableBytesWritable, Result, Mapper$Context)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='94' name='put' register='4'><Message>Value loaded from put</Message></LocalVariable><SourceLine endBytecode='95' classname='org.apache.hadoop.hbase.mapreduce.Import$Importer' start='539' end='539' sourcepath='org/apache/hadoop/hbase/mapreduce/Import.java' sourcefile='Import.java' startBytecode='95' primary='true'><Message>At Import.java:[line 539]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1dc2e7bf663e47f9efd04f848c97eaf3' rank='20' abbrev='ST' category='STYLE' priority='3' type='ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD' instanceOccurrenceMax='0'><ShortMessage>Write to static field from instance method</ShortMessage><LongMessage>Write to static field org.apache.hadoop.hbase.mapreduce.ImportTsv.DRY_RUN_TABLE_CREATED from instance method org.apache.hadoop.hbase.mapreduce.ImportTsv.run(String[])</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' start='78' end='792' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java'><Message>At ImportTsv.java:[lines 78-792]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.ImportTsv</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' signature='([Ljava/lang/String;)I' name='run' primary='true'><SourceLine endBytecode='929' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' start='708' end='786' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.ImportTsv.run(String[])</Message></Method><Field isStatic='true' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' signature='Z' name='DRY_RUN_TABLE_CREATED' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java'><Message>In ImportTsv.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.ImportTsv.DRY_RUN_TABLE_CREATED</Message></Field><SourceLine endBytecode='294' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv' start='775' end='775' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java' startBytecode='294' primary='true'><Message>At ImportTsv.java:[line 775]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f03a3b6061d6f14164ff6a030b6de0aa' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getIndividualAttributes() return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine' start='285' end='411' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java'><Message>At ImportTsv.java:[lines 285-411]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine' signature='()[Ljava/lang/String;' name='getIndividualAttributes' primary='true'><SourceLine endBytecode='95' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine' start='324' end='328' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine.getIndividualAttributes()</Message></Method><SourceLine endBytecode='17' classname='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine' start='328' end='328' sourcepath='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' sourcefile='ImportTsv.java' startBytecode='17' primary='true'><Message>At ImportTsv.java:[line 328]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d68c6b374800fa38ffd1999d02f512bb' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>KeyValueSerialization$KeyValueDeserializer.dis not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.close()</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' start='56' end='73' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java'><Message>At KeyValueSerialization.java:[lines 56-73]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' signature='Ljava/io/DataInputStream;' name='dis' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java'><Message>In KeyValueSerialization.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.dis</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' signature='()V' name='close' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' start='61' end='62' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer.close()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer' start='61' end='61' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java' startBytecode='4' primary='true'><Message>At KeyValueSerialization.java:[line 61]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ba042e2839e1fb1d8308ee71525d3dd' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>KeyValueSerialization$KeyValueSerializer.dos not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.close()</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' start='76' end='92' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java'><Message>At KeyValueSerialization.java:[lines 76-92]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' signature='Ljava/io/DataOutputStream;' name='dos' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java'><Message>In KeyValueSerialization.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.dos</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' signature='()V' name='close' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' start='81' end='82' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer.close()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer' start='81' end='81' sourcepath='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' sourcefile='KeyValueSerialization.java' startBytecode='4' primary='true'><Message>At KeyValueSerialization.java:[line 81]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ec3d2489b6b14776d48ac47915ae5be2' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.hbase.mapreduce.TableSplit in org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.createRecordReader(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='55' end='297' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java'><Message>At MultiTableInputFormatBase.java:[lines 55-297]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)Lorg/apache/hadoop/mapreduce/RecordReader;' name='createRecordReader' primary='true'><SourceLine endBytecode='75' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='83' end='144' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.createRecordReader(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapreduce/TableSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableSplit' start='41' end='393' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSplit.java' sourcefile='TableSplit.java'><Message>At TableSplit.java:[lines 41-393]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapreduce.TableSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='0' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='1' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='83' end='83' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java' startBytecode='1' primary='true'><Message>At MultiTableInputFormatBase.java:[line 83]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ce055326fbce8091753fa33c2dc27c08' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase$1'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase$1' start='105' end='136' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java'><Message>At MultiTableInputFormatBase.java:[lines 105-136]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase$1</Message></Class><Class classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='55' end='297' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java'><Message>At MultiTableInputFormatBase.java:[lines 55-297]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)Lorg/apache/hadoop/mapreduce/RecordReader;' name='createRecordReader' primary='true'><SourceLine endBytecode='422' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='83' end='144' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.createRecordReader(InputSplit, TaskAttemptContext)</Message></Method><SourceLine endBytecode='146' classname='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase' start='105' end='105' sourcepath='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' sourcefile='MultiTableInputFormatBase.java' startBytecode='146' primary='true'><Message>At MultiTableInputFormatBase.java:[line 105]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c57ead02b954494d231812c8346977dd' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.getKeyFromConf(Configuration, String, String) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner' start='47' end='152' sourcepath='org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java' sourcefile='SimpleTotalOrderPartitioner.java'><Message>At SimpleTotalOrderPartitioner.java:[lines 47-152]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;Ljava/lang/String;)[B' name='getKeyFromConf' primary='true'><SourceLine endBytecode='199' classname='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner' start='95' end='105' sourcepath='org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java' sourcefile='SimpleTotalOrderPartitioner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.getKeyFromConf(Configuration, String, String)</Message></Method><SourceLine endBytecode='31' classname='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner' start='101' end='101' sourcepath='org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java' sourcefile='SimpleTotalOrderPartitioner.java' startBytecode='31' primary='true'><Message>At SimpleTotalOrderPartitioner.java:[line 101]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9ff15553db861e460148d7e6593e3af5' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner.nextRow() return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='401' end='473' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java'><Message>At SyncTable.java:[lines 401-473]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' signature='()[B' name='nextRow' primary='true'><SourceLine endBytecode='296' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='410' end='439' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner.nextRow()</Message></Method><SourceLine endBytecode='115' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='430' end='430' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java' startBytecode='115' primary='true'><Message>At SyncTable.java:[line 430]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='51e2c80ba7568c2bb719bd70618bf2ef' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>SyncTable$SyncMapper$CellScanner.currentRow not initialized in constructor and dereferenced in org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner.nextCellInRow()</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='401' end='473' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java'><Message>At SyncTable.java:[lines 401-473]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' signature='[B' name='currentRow' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java'><Message>In SyncTable.java</Message></SourceLine><Message>Field org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner.currentRow</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' signature='()Lorg/apache/hadoop/hbase/Cell;' name='nextCellInRow' primary='true'><SourceLine endBytecode='310' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='446' end='473' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner.nextCellInRow()</Message></Method><SourceLine endBytecode='87' classname='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner' start='457' end='457' sourcepath='org/apache/hadoop/hbase/mapreduce/SyncTable.java' sourcefile='SyncTable.java' startBytecode='87' primary='true'><Message>At SyncTable.java:[line 457]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8102f95d130b495735399c07c98d58f' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.hbase.mapreduce.TableSplit in org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase' start='109' end='661' sourcepath='org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java'><Message>At TableInputFormatBase.java:[lines 109-661]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.TableInputFormatBase</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)Lorg/apache/hadoop/mapreduce/RecordReader;' name='createRecordReader' primary='true'><SourceLine endBytecode='77' classname='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase' start='167' end='188' sourcepath='org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapreduce/TableSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableSplit' start='41' end='393' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSplit.java' sourcefile='TableSplit.java'><Message>At TableSplit.java:[lines 41-393]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapreduce.TableSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='44' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='45' classname='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase' start='179' end='179' sourcepath='org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java' sourcefile='TableInputFormatBase.java' startBytecode='45' primary='true'><Message>At TableInputFormatBase.java:[line 179]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d115ac83b1f660b9e914ad1cdb7d11a7' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.updateCounters(ScanMetrics, long, Method, TaskAttemptContext, long)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl' start='48' end='319' sourcepath='org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java' sourcefile='TableRecordReaderImpl.java'><Message>At TableRecordReaderImpl.java:[lines 48-319]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl' signature='(Lorg/apache/hadoop/hbase/client/metrics/ScanMetrics;JLjava/lang/reflect/Method;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;J)V' name='updateCounters' primary='true'><SourceLine endBytecode='412' classname='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl' start='292' end='310' sourcepath='org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java' sourcefile='TableRecordReaderImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.updateCounters(ScanMetrics, long, Method, TaskAttemptContext, long)</Message></Method><SourceLine endBytecode='158' classname='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl' start='307' end='307' sourcepath='org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java' sourcefile='TableRecordReaderImpl.java' startBytecode='158' primary='true'><Message>At TableRecordReaderImpl.java:[line 307]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f9f755d65d968acfccd214f94732d658' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit in org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.initialize(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader' start='146' end='194' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java'><Message>At TableSnapshotInputFormat.java:[lines 146-194]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)V' name='initialize' primary='true'><SourceLine endBytecode='15' classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader' start='157' end='162' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.initialize(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat$TableSnapshotRegionSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit' start='90' end='142' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java'><Message>At TableSnapshotInputFormat.java:[lines 90-142]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='17' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='18' classname='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader' start='159' end='159' sourcepath='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java' sourcefile='TableSnapshotInputFormat.java' startBytecode='18' primary='true'><Message>At TableSnapshotInputFormat.java:[line 159]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c6191489629511a0b833b42edad847f9' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit in org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.initialize(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader' start='141' end='247' sourcepath='org/apache/hadoop/hbase/mapreduce/WALInputFormat.java' sourcefile='WALInputFormat.java'><Message>At WALInputFormat.java:[lines 141-247]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)V' name='initialize' primary='true'><SourceLine endBytecode='36' classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader' start='154' end='161' sourcepath='org/apache/hadoop/hbase/mapreduce/WALInputFormat.java' sourcefile='WALInputFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.initialize(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/mapreduce/WALInputFormat$WALSplit;'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit' start='74' end='133' sourcepath='org/apache/hadoop/hbase/mapreduce/WALInputFormat.java' sourcefile='WALInputFormat.java'><Message>At WALInputFormat.java:[lines 74-133]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='0' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='1' classname='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader' start='154' end='154' sourcepath='org/apache/hadoop/hbase/mapreduce/WALInputFormat.java' sourcefile='WALInputFormat.java' startBytecode='1' primary='true'><Message>At WALInputFormat.java:[line 154]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e21dc0dc0407ca52fe9f1dbad003a338' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.doCommandLine(String[])</LongMessage><Class classname='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication' primary='true'><SourceLine classname='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication' start='84' end='763' sourcepath='org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java' sourcefile='VerifyReplication.java'><Message>At VerifyReplication.java:[lines 84-763]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication' signature='([Ljava/lang/String;)Z' name='doCommandLine' primary='true'><SourceLine endBytecode='1639' classname='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication' start='522' end='683' sourcepath='org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java' sourcefile='VerifyReplication.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.doCommandLine(String[])</Message></Method><SourceLine endBytecode='755' classname='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication' start='678' end='678' sourcepath='org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java' sourcefile='VerifyReplication.java' startBytecode='755' primary='true'><Message>At VerifyReplication.java:[line 678]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3de011730e8fefc40bc252803c8ba48d' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.regionserver.CompactionTool.run(String[])</LongMessage><Class classname='org.apache.hadoop.hbase.regionserver.CompactionTool' primary='true'><SourceLine classname='org.apache.hadoop.hbase.regionserver.CompactionTool' start='76' end='471' sourcepath='org/apache/hadoop/hbase/regionserver/CompactionTool.java' sourcefile='CompactionTool.java'><Message>At CompactionTool.java:[lines 76-471]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.regionserver.CompactionTool</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.regionserver.CompactionTool' signature='([Ljava/lang/String;)I' name='run' primary='true'><SourceLine endBytecode='632' classname='org.apache.hadoop.hbase.regionserver.CompactionTool' start='393' end='436' sourcepath='org/apache/hadoop/hbase/regionserver/CompactionTool.java' sourcefile='CompactionTool.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.regionserver.CompactionTool.run(String[])</Message></Method><SourceLine endBytecode='181' classname='org.apache.hadoop.hbase.regionserver.CompactionTool' start='422' end='422' sourcepath='org/apache/hadoop/hbase/regionserver/CompactionTool.java' sourcefile='CompactionTool.java' startBytecode='181' primary='true'><Message>At CompactionTool.java:[line 422]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='663c18eeed4f62ca93beedae4808d0e7' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hbase.snapshot.ExportSnapshot.doWork()</LongMessage><Class classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' primary='true'><SourceLine classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' start='92' end='1143' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 92-1143]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.snapshot.ExportSnapshot</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' signature='()I' name='doWork' primary='true'><SourceLine endBytecode='2944' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' start='942' end='1109' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.snapshot.ExportSnapshot.doWork()</Message></Method><SourceLine endBytecode='1554' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' start='1099' end='1099' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='1554' primary='true'><Message>At ExportSnapshot.java:[line 1099]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6ac7a67c8c2f3da990cdb15985e7d20c' rank='20' abbrev='RI' category='STYLE' priority='3' type='RI_REDUNDANT_INTERFACES' instanceOccurrenceMax='0'><ShortMessage>Class implements same interface as superclass</ShortMessage><LongMessage>Class org.apache.hadoop.hbase.snapshot.ExportSnapshot implements same interface as superclass</LongMessage><Class classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' primary='true'><SourceLine classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' start='92' end='1143' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 92-1143]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.snapshot.ExportSnapshot</Message></Class><Class role='INTERFACE_TYPE' classname='org.apache.hadoop.util.Tool'><SourceLine classname='org.apache.hadoop.util.Tool' sourcepath='org/apache/hadoop/util/Tool.java' sourcefile='Tool.java'><Message>In Tool.java</Message></SourceLine><Message>Interface org.apache.hadoop.util.Tool</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot' start='92' end='1143' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 92-1143]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5da575bb94f7c09309e66222fa9ad40e' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.copyFile(Mapper$Context, SnapshotProtos$SnapshotFileInfo, Path)</LongMessage><Class classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper' primary='true'><SourceLine classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper' start='161' end='555' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 161-555]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper' signature='(Lorg/apache/hadoop/mapreduce/Mapper$Context;Lorg/apache/hadoop/hbase/shaded/protobuf/generated/SnapshotProtos$SnapshotFileInfo;Lorg/apache/hadoop/fs/Path;)V' name='copyFile' primary='true'><SourceLine endBytecode='665' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper' start='289' end='328' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.copyFile(Mapper$Context, SnapshotProtos$SnapshotFileInfo, Path)</Message></Method><SourceLine endBytecode='168' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper' start='305' end='305' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='168' primary='true'><Message>At ExportSnapshot.java:[line 305]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a876ecedd336cd7f2e587c5dd4fdaccd' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.mapreduce.InputSplit to org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit in org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat.createRecordReader(InputSplit, TaskAttemptContext)</LongMessage><Class classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat' primary='true'><SourceLine classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat' start='668' end='695' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 668-695]</Message></SourceLine><Message>In class org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat' signature='(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)Lorg/apache/hadoop/mapreduce/RecordReader;' name='createRecordReader' primary='true'><SourceLine endBytecode='6' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat' start='672' end='672' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat.createRecordReader(InputSplit, TaskAttemptContext)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/mapreduce/InputSplit;'><SourceLine classname='org.apache.hadoop.mapreduce.InputSplit' start='44' end='75' sourcepath='org/apache/hadoop/mapreduce/InputSplit.java' sourcefile='InputSplit.java'><Message>At InputSplit.java:[lines 44-75]</Message></SourceLine><Message>Actual type org.apache.hadoop.mapreduce.InputSplit</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hbase/snapshot/ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit;'><SourceLine classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit' start='698' end='750' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java'><Message>At ExportSnapshot.java:[lines 698-750]</Message></SourceLine><Message>Expected org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='4' name='split' register='1'><Message>Value loaded from split</Message></LocalVariable><SourceLine endBytecode='5' classname='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat' start='672' end='672' sourcepath='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' sourcefile='ExportSnapshot.java' startBytecode='5' primary='true'><Message>At ExportSnapshot.java:[line 672]</Message></SourceLine></BugInstance><BugCategory category='CORRECTNESS'><Description>Correctness</Description></BugCategory><BugCategory category='PERFORMANCE'><Description>Performance</Description></BugCategory><BugCategory category='STYLE'><Description>Dodgy code</Description></BugCategory><BugPattern abbrev='UwF' category='STYLE' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR'><ShortDescription>Field not initialized in constructor but dereferenced without null check</ShortDescription><Details>

  &lt;p&gt; This field is never initialized within any constructor, and is therefore could be null after
the object is constructed. Elsewhere, it is loaded and dereferenced without a null check.
This could be a either an error or a questionable design, since
it means a null pointer exception will be generated if that field is dereferenced
before being initialized.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='ST' category='STYLE' type='ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD'><ShortDescription>Write to static field from instance method</ShortDescription><Details>

  &lt;p&gt; This instance method writes to a static field. This is tricky to get
correct if multiple instances are being manipulated,
and generally bad practice.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='EC' category='CORRECTNESS' type='EC_ARRAY_AND_NONARRAY'><ShortDescription>equals() used to compare array and nonarray</ShortDescription><Details>

&lt;p&gt;
This method invokes the .equals(Object o) to compare an array and a reference that doesn't seem
to be an array. If things being compared are of different types, they are guaranteed to be unequal
and the comparison is almost certainly an error. Even if they are both arrays, the equals method
on arrays only determines of the two arrays are the same object.
To compare the
contents of the arrays, use java.util.Arrays.equals(Object[], Object[]).
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='PZLA' category='STYLE' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS'><ShortDescription>Consider returning a zero length array rather than null</ShortDescription><Details>

&lt;p&gt; It is often a better design to
return a length zero array rather than a null reference to indicate that there
are no results (i.e., an empty list of results).
This way, no explicit check for null is needed by clients of the method.&lt;/p&gt;

&lt;p&gt;On the other hand, using null to indicate
"there is no answer to this question" is probably appropriate.
For example, &lt;code&gt;File.listFiles()&lt;/code&gt; returns an empty list
if given a directory containing no files, and returns null if the file
is not a directory.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NP' category='STYLE' type='NP_LOAD_OF_KNOWN_NULL_VALUE'><ShortDescription>Load of known null value</ShortDescription><Details>

  &lt;p&gt; The variable referenced at this point is known to be null due to an earlier
   check against null. Although this is valid, it might be a mistake (perhaps you
intended to refer to a different variable, or perhaps the earlier check to see if the
variable is null should have been a check to see if it was non-null).
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='SIC' category='PERFORMANCE' type='SIC_INNER_SHOULD_BE_STATIC_ANON'><ShortDescription>Could be refactored into a named static inner class</ShortDescription><Details>

  &lt;p&gt; This class is an inner class, but does not use its embedded reference
  to the object which created it.&amp;nbsp; This reference makes the instances
  of the class larger, and may keep the reference to the creator object
  alive longer than necessary.&amp;nbsp; If possible, the class should be
  made into a &lt;em&gt;static&lt;/em&gt; inner class. Since anonymous inner
classes cannot be marked as static, doing this will require refactoring
the inner class so that it is a named inner class.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='RI' category='STYLE' type='RI_REDUNDANT_INTERFACES'><ShortDescription>Class implements same interface as superclass</ShortDescription><Details>
   
    &lt;p&gt;
    This class declares that it implements an interface that is also implemented by a superclass.
    This is redundant because once a superclass implements an interface, all subclasses by default also
    implement this interface. It may point out that the inheritance hierarchy has changed since
    this class was created, and consideration should be given to the ownership of
    the interface's implementation.
    &lt;/p&gt;
    
     </Details></BugPattern><BugPattern abbrev='ICAST' category='STYLE' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG'><ShortDescription>Result of integer multiplication cast to long</ShortDescription><Details>

&lt;p&gt;
This code performs integer multiply and then converts the result to a long,
as in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;long convertDaysToMilliseconds(int days) { return 1000*3600*24*days; }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
If the multiplication is done using long arithmetic, you can avoid
the possibility that the result will overflow. For example, you
could fix the above code to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;long convertDaysToMilliseconds(int days) { return 1000L*3600*24*days; }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
or
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;static final long MILLISECONDS_PER_DAY = 24L*3600*1000;
long convertDaysToMilliseconds(int days) { return days * MILLISECONDS_PER_DAY; }
&lt;/code&gt;&lt;/pre&gt;

    </Details></BugPattern><BugPattern abbrev='BC' category='STYLE' type='BC_UNCONFIRMED_CAST'><ShortDescription>Unchecked/unconfirmed cast</ShortDescription><Details>

&lt;p&gt;
This cast is unchecked, and not all instances of the type casted from can be cast to
the type it is being cast to. Check that your program logic ensures that this
cast will not fail.
&lt;/p&gt;

    </Details></BugPattern><BugPattern cweid='396' abbrev='REC' category='STYLE' type='REC_CATCH_EXCEPTION'><ShortDescription>Exception is caught when Exception is not thrown</ShortDescription><Details>
  
  &lt;p&gt;
  This method uses a try-catch block that catches Exception objects, but Exception is not
  thrown within the try block, and RuntimeException is not explicitly caught.  It is a common bug pattern to
  say try { ... } catch (Exception e) { something } as a shorthand for catching a number of types of exception
  each of whose catch blocks is identical, but this construct also accidentally catches RuntimeException as well,
  masking potential bugs.
  &lt;/p&gt;
  &lt;p&gt;A better approach is to either explicitly catch the specific exceptions that are thrown,
  or to explicitly catch RuntimeException exception, rethrow it, and then catch all non-Runtime Exceptions, as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;try {
    ...
} catch (RuntimeException e) {
    throw e;
} catch (Exception e) {
    ... deal with all non-runtime exceptions ...
}
&lt;/code&gt;&lt;/pre&gt;
  
     </Details></BugPattern><BugCode abbrev='BC'><Description>Bad casts of object references</Description></BugCode><BugCode abbrev='REC'><Description>RuntimeException capture</Description></BugCode><BugCode abbrev='ST'><Description>Misuse of static fields</Description></BugCode><BugCode cweid='476' abbrev='NP'><Description>Null pointer dereference</Description></BugCode><BugCode abbrev='UwF'><Description>Unwritten field</Description></BugCode><BugCode abbrev='RI'><Description>Redundant Interfaces</Description></BugCode><BugCode abbrev='PZLA'><Description>Prefer zero length arrays to null to indicate no results</Description></BugCode><BugCode abbrev='SIC'><Description>Inner class could be made static</Description></BugCode><BugCode cweid='192' abbrev='ICAST'><Description>Casting from integer values</Description></BugCode><BugCode abbrev='EC'><Description>Comparing incompatible types for equality</Description></BugCode><Errors missingClasses='0' errors='0'></Errors><FindBugsSummary num_packages='6' total_classes='164' priority_3='35' total_size='8916' clock_seconds='13.22' referenced_classes='702' vm_version='25.222-b10' total_bugs='35' java_version='1.8.0_222' gc_seconds='0.51' alloc_mbytes='3641.00' cpu_seconds='56.33' peak_mbytes='1140.83' timestamp='Thu, 12 Sep 2019 10:49:03 +0200'><FileStats path='org/apache/hadoop/hbase/mapred/Driver.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/GroupingTableMap.java' size='49' bugHash='cbcbb5751ba44634475730908531c02c' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/HRegionPartitioner.java' size='37' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/IdentityTableMap.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/IdentityTableReduce.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java' size='17' bugHash='0de927a0787d346bd2cfe53cb4d73188' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/RowCounter.java' size='49' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableInputFormat.java' size='35' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableInputFormatBase.java' size='110' bugHash='80b99b6fd1e5a94c10b934ab9a9dbe51' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableMap.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java' size='126' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableOutputFormat.java' size='41' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableRecordReader.java' size='43' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java' size='124' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableReduce.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableSnapshotInputFormat.java' size='73' bugHash='4adcad76c86f615b7bf0fce08610a8db' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapred/TableSplit.java' size='71' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/CellCounter.java' size='176' bugHash='302254c508b88e71b9623d1111e4b594' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/CellCreator.java' size='20' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/CellSerialization.java' size='40' bugHash='35ac8a3c65387a3c4f81a5f79e44c398' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/CellSortReducer.java' size='20' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/CopyTable.java' size='228' bugHash='ceb9c3af92c1f33df05f152d7753f75a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java' size='72' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/Driver.java' size='18' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/Export.java' size='34' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/ExportUtils.java' size='85' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java' size='53' bugHash='e60280228c7ea896c264a3d27413c1b9' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java' size='76' bugHash='a43765508cddec7de230570524051cfa' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java' size='448' bugHash='2c0028af2d6753c70f0e4a729e6a3657' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java' size='42' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/HashTable.java' size='464' bugHash='3c712d80724dab53b977742a553e4a8d' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/IdentityTableReducer.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/Import.java' size='569' bugHash='00ee601985fa08fd99dcc823f015ad71' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/ImportTsv.java' size='413' bugHash='c2663274db992fe2da994d152eb93e39' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/JarFinder.java' size='98' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/KeyValueSerialization.java' size='39' bugHash='e5fabd289957767d3111600fb2dc5f47' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java' size='20' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java' size='27' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java' size='21' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java' size='126' bugHash='8f18f7b8f4c76ff95729b367e61528da' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java' size='59' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormat.java' size='17' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormatImpl.java' size='97' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java' size='140' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/MutationSerialization.java' size='45' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/PutCombiner.java' size='44' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/PutSortReducer.java' size='61' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/RegionSizeCalculator.java' size='50' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/ResultSerialization.java' size='84' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/RowCounter.java' size='125' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java' size='66' bugHash='e4b2daad427dcebf41c75924f22958b0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/SyncTable.java' size='497' bugHash='85d43d1a7e1399dfe62672636a7688a4' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableInputFormat.java' size='104' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java' size='270' bugHash='b8b321155911c7623912b71102805eba' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java' size='295' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableMapper.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableOutputCommitter.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java' size='79' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableRecordReader.java' size='31' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java' size='154' bugHash='69266f9bdc87111a6b4a107bdfcfdf67' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableReducer.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormat.java' size='82' bugHash='105be3ccf398f99652926655027b1b99' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java' size='253' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TableSplit.java' size='156' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TextSortReducer.java' size='89' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java' size='103' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/TsvImporterTextMapper.java' size='52' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/VisibilityExpressionResolver.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/WALInputFormat.java' size='181' bugHash='40b5318521f7eb7744d49c45ba3747a7' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/WALPlayer.java' size='223' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java' size='484' bugHash='d3166172b709a3a7cb9bdece8e685153' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/regionserver/CompactionTool.java' size='265' bugHash='acdce8313ae98e024b7c40a56d796c12' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hbase/snapshot/ExportSnapshot.java' size='721' bugHash='191e98fd6dbca50ef906655e1b588f48' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hbase/util/MapReduceExtendedCell.java' size='106' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java' size='26' bugCount='0'></FileStats><PackageStats package='org.apache.hadoop.hbase.mapred' total_bugs='5' priority_3='5' total_size='816' total_types='23'><ClassStats bugs='0' size='14' interface='false' sourceFile='Driver.java' class='org.apache.hadoop.hbase.mapred.Driver'></ClassStats><ClassStats bugs='1' size='49' priority_3='1' interface='false' sourceFile='GroupingTableMap.java' class='org.apache.hadoop.hbase.mapred.GroupingTableMap'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='HRegionPartitioner.java' class='org.apache.hadoop.hbase.mapred.HRegionPartitioner'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='IdentityTableMap.java' class='org.apache.hadoop.hbase.mapred.IdentityTableMap'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='IdentityTableReduce.java' class='org.apache.hadoop.hbase.mapred.IdentityTableReduce'></ClassStats><ClassStats bugs='2' size='17' priority_3='2' interface='false' sourceFile='MultiTableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapred.RowCounter'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapred.RowCounter$RowCounterMapper$Counters'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='TableInputFormat.java' class='org.apache.hadoop.hbase.mapred.TableInputFormat'></ClassStats><ClassStats bugs='1' size='88' priority_3='1' interface='false' sourceFile='TableInputFormatBase.java' class='org.apache.hadoop.hbase.mapred.TableInputFormatBase'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='TableInputFormatBase.java' class='org.apache.hadoop.hbase.mapred.TableInputFormatBase$1'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='TableMap.java' class='org.apache.hadoop.hbase.mapred.TableMap'></ClassStats><ClassStats bugs='0' size='126' interface='false' sourceFile='TableMapReduceUtil.java' class='org.apache.hadoop.hbase.mapred.TableMapReduceUtil'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='TableOutputFormat.java' class='org.apache.hadoop.hbase.mapred.TableOutputFormat'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='TableOutputFormat.java' class='org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='TableRecordReader.java' class='org.apache.hadoop.hbase.mapred.TableRecordReader'></ClassStats><ClassStats bugs='0' size='124' interface='false' sourceFile='TableRecordReaderImpl.java' class='org.apache.hadoop.hbase.mapred.TableRecordReaderImpl'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='TableReduce.java' class='org.apache.hadoop.hbase.mapred.TableReduce'></ClassStats><ClassStats bugs='1' size='18' priority_3='1' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRecordReader'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat$TableSnapshotRegionSplit'></ClassStats><ClassStats bugs='0' size='71' interface='false' sourceFile='TableSplit.java' class='org.apache.hadoop.hbase.mapred.TableSplit'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hbase.mapreduce' total_bugs='24' priority_3='24' total_size='6498' total_types='119'><ClassStats bugs='1' size='94' priority_3='1' interface='false' sourceFile='CellCounter.java' class='org.apache.hadoop.hbase.mapreduce.CellCounter'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='CellCounter.java' class='org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='CellCounter.java' class='org.apache.hadoop.hbase.mapreduce.CellCounter$CellCounterMapper$Counters'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='CellCounter.java' class='org.apache.hadoop.hbase.mapreduce.CellCounter$IntSumReducer'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='CellCreator.java' class='org.apache.hadoop.hbase.mapreduce.CellCreator'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='CellSerialization.java' class='org.apache.hadoop.hbase.mapreduce.CellSerialization'></ClassStats><ClassStats bugs='1' size='13' priority_3='1' interface='false' sourceFile='CellSerialization.java' class='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellDeserializer'></ClassStats><ClassStats bugs='2' size='15' priority_3='2' interface='false' sourceFile='CellSerialization.java' class='org.apache.hadoop.hbase.mapreduce.CellSerialization$CellSerializer'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='CellSortReducer.java' class='org.apache.hadoop.hbase.mapreduce.CellSortReducer'></ClassStats><ClassStats bugs='1' size='228' priority_3='1' interface='false' sourceFile='CopyTable.java' class='org.apache.hadoop.hbase.mapreduce.CopyTable'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='DefaultVisibilityExpressionResolver.java' class='org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DefaultVisibilityExpressionResolver.java' class='org.apache.hadoop.hbase.mapreduce.DefaultVisibilityExpressionResolver$1'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='Driver.java' class='org.apache.hadoop.hbase.mapreduce.Driver'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='Export.java' class='org.apache.hadoop.hbase.mapreduce.Export'></ClassStats><ClassStats bugs='0' size='85' interface='false' sourceFile='ExportUtils.java' class='org.apache.hadoop.hbase.mapreduce.ExportUtils'></ClassStats><ClassStats bugs='1' size='53' priority_3='1' interface='false' sourceFile='GroupingTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.GroupingTableMapper'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='HFileInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.HFileInputFormat'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HFileInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$1'></ClassStats><ClassStats bugs='1' size='46' priority_3='1' interface='false' sourceFile='HFileInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.HFileInputFormat$HFileRecordReader'></ClassStats><ClassStats bugs='1' size='265' priority_3='1' interface='false' sourceFile='HFileOutputFormat2.java' class='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2'></ClassStats><ClassStats bugs='0' size='162' interface='false' sourceFile='HFileOutputFormat2.java' class='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='HFileOutputFormat2.java' class='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$TableInfo'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='HFileOutputFormat2.java' class='org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2$WriterLength'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='HRegionPartitioner.java' class='org.apache.hadoop.hbase.mapreduce.HRegionPartitioner'></ClassStats><ClassStats bugs='1' size='164' priority_3='1' interface='false' sourceFile='HashTable.java' class='org.apache.hadoop.hbase.mapreduce.HashTable'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='HashTable.java' class='org.apache.hadoop.hbase.mapreduce.HashTable$HashMapper'></ClassStats><ClassStats bugs='0' size='55' interface='false' sourceFile='HashTable.java' class='org.apache.hadoop.hbase.mapreduce.HashTable$ResultHasher'></ClassStats><ClassStats bugs='0' size='164' interface='false' sourceFile='HashTable.java' class='org.apache.hadoop.hbase.mapreduce.HashTable$TableHash'></ClassStats><ClassStats bugs='0' size='54' interface='false' sourceFile='HashTable.java' class='org.apache.hadoop.hbase.mapreduce.HashTable$TableHash$Reader'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='IdentityTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.IdentityTableMapper'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='IdentityTableReducer.java' class='org.apache.hadoop.hbase.mapreduce.IdentityTableReducer'></ClassStats><ClassStats bugs='0' size='217' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellImporter'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellReducer'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellSortImporter'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellWritableComparable'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellWritableComparable$CellWritableComparator'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$CellWritableComparablePartitioner'></ClassStats><ClassStats bugs='2' size='86' priority_3='2' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$Importer'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueImporter'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueReducer'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueSortImporter'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueWritableComparable'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueWritableComparable$KeyValueWritableComparator'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='Import.java' class='org.apache.hadoop.hbase.mapreduce.Import$KeyValueWritableComparablePartitioner'></ClassStats><ClassStats bugs='1' size='212' priority_3='1' interface='false' sourceFile='ImportTsv.java' class='org.apache.hadoop.hbase.mapreduce.ImportTsv'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='ImportTsv.java' class='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='ImportTsv.java' class='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$BadTsvLineException'></ClassStats><ClassStats bugs='1' size='76' priority_3='1' interface='false' sourceFile='ImportTsv.java' class='org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvParser$ParsedLine'></ClassStats><ClassStats bugs='0' size='98' interface='false' sourceFile='JarFinder.java' class='org.apache.hadoop.hbase.mapreduce.JarFinder'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='KeyValueSerialization.java' class='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization'></ClassStats><ClassStats bugs='1' size='13' priority_3='1' interface='false' sourceFile='KeyValueSerialization.java' class='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueDeserializer'></ClassStats><ClassStats bugs='1' size='14' priority_3='1' interface='false' sourceFile='KeyValueSerialization.java' class='org.apache.hadoop.hbase.mapreduce.KeyValueSerialization$KeyValueSerializer'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='KeyValueSortReducer.java' class='org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='MultiTableHFileOutputFormat.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableHFileOutputFormat'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='MultiTableInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat'></ClassStats><ClassStats bugs='2' size='103' priority_3='2' interface='false' sourceFile='MultiTableInputFormatBase.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='MultiTableInputFormatBase.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='MultiTableOutputFormat.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='MultiTableOutputFormat.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat$MultiTableRecordWriter'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='MultiTableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormat'></ClassStats><ClassStats bugs='0' size='97' interface='false' sourceFile='MultiTableSnapshotInputFormatImpl.java' class='org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$1'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$MapRunner'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordReader'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapRecordWriter'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='MultithreadedTableMapper.java' class='org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper$SubMapStatusReporter'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='MutationSerialization.java' class='org.apache.hadoop.hbase.mapreduce.MutationSerialization'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='MutationSerialization.java' class='org.apache.hadoop.hbase.mapreduce.MutationSerialization$1'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='MutationSerialization.java' class='org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationDeserializer'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='MutationSerialization.java' class='org.apache.hadoop.hbase.mapreduce.MutationSerialization$MutationSerializer'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='PutCombiner.java' class='org.apache.hadoop.hbase.mapreduce.PutCombiner'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='PutSortReducer.java' class='org.apache.hadoop.hbase.mapreduce.PutSortReducer'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='RegionSizeCalculator.java' class='org.apache.hadoop.hbase.mapreduce.RegionSizeCalculator'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='ResultSerialization.java' class='org.apache.hadoop.hbase.mapreduce.ResultSerialization'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='ResultSerialization.java' class='org.apache.hadoop.hbase.mapreduce.ResultSerialization$1'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='ResultSerialization.java' class='org.apache.hadoop.hbase.mapreduce.ResultSerialization$Result94Deserializer'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='ResultSerialization.java' class='org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultDeserializer'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='ResultSerialization.java' class='org.apache.hadoop.hbase.mapreduce.ResultSerialization$ResultSerializer'></ClassStats><ClassStats bugs='0' size='110' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapreduce.RowCounter'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='RowCounter.java' class='org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters'></ClassStats><ClassStats bugs='1' size='66' priority_3='1' interface='false' sourceFile='SimpleTotalOrderPartitioner.java' class='org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner'></ClassStats><ClassStats bugs='0' size='166' interface='false' sourceFile='SyncTable.java' class='org.apache.hadoop.hbase.mapreduce.SyncTable'></ClassStats><ClassStats bugs='0' size='260' interface='false' sourceFile='SyncTable.java' class='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper'></ClassStats><ClassStats bugs='2' size='48' priority_3='2' interface='false' sourceFile='SyncTable.java' class='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$CellScanner'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='SyncTable.java' class='org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper$Counter'></ClassStats><ClassStats bugs='0' size='104' interface='false' sourceFile='TableInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableInputFormat'></ClassStats><ClassStats bugs='1' size='248' priority_3='1' interface='false' sourceFile='TableInputFormatBase.java' class='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='TableInputFormatBase.java' class='org.apache.hadoop.hbase.mapreduce.TableInputFormatBase$1'></ClassStats><ClassStats bugs='0' size='295' interface='false' sourceFile='TableMapReduceUtil.java' class='org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil'></ClassStats><ClassStats bugs='0' size='3' interface='false' sourceFile='TableMapper.java' class='org.apache.hadoop.hbase.mapreduce.TableMapper'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='TableOutputCommitter.java' class='org.apache.hadoop.hbase.mapreduce.TableOutputCommitter'></ClassStats><ClassStats bugs='0' size='54' interface='false' sourceFile='TableOutputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableOutputFormat'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='TableOutputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='TableRecordReader.java' class='org.apache.hadoop.hbase.mapreduce.TableRecordReader'></ClassStats><ClassStats bugs='1' size='154' priority_3='1' interface='false' sourceFile='TableRecordReaderImpl.java' class='org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl'></ClassStats><ClassStats bugs='0' size='3' interface='false' sourceFile='TableReducer.java' class='org.apache.hadoop.hbase.mapreduce.TableReducer'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat'></ClassStats><ClassStats bugs='1' size='33' priority_3='1' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='TableSnapshotInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionSplit'></ClassStats><ClassStats bugs='0' size='143' interface='false' sourceFile='TableSnapshotInputFormatImpl.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl'></ClassStats><ClassStats bugs='0' size='66' interface='false' sourceFile='TableSnapshotInputFormatImpl.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$InputSplit'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='TableSnapshotInputFormatImpl.java' class='org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader'></ClassStats><ClassStats bugs='0' size='129' interface='false' sourceFile='TableSplit.java' class='org.apache.hadoop.hbase.mapreduce.TableSplit'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='TableSplit.java' class='org.apache.hadoop.hbase.mapreduce.TableSplit$Version'></ClassStats><ClassStats bugs='0' size='89' interface='false' sourceFile='TextSortReducer.java' class='org.apache.hadoop.hbase.mapreduce.TextSortReducer'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='TsvImporterMapper.java' class='org.apache.hadoop.hbase.mapreduce.TsvImporterMapper'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='TsvImporterTextMapper.java' class='org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='VisibilityExpressionResolver.java' class='org.apache.hadoop.hbase.mapreduce.VisibilityExpressionResolver'></ClassStats><ClassStats bugs='0' size='62' interface='false' sourceFile='WALInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.WALInputFormat'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='WALInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALKeyRecordReader'></ClassStats><ClassStats bugs='1' size='75' priority_3='1' interface='false' sourceFile='WALInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='WALInputFormat.java' class='org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALSplit'></ClassStats><ClassStats bugs='0' size='117' interface='false' sourceFile='WALPlayer.java' class='org.apache.hadoop.hbase.mapreduce.WALPlayer'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='WALPlayer.java' class='org.apache.hadoop.hbase.mapreduce.WALPlayer$WALCellMapper'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='WALPlayer.java' class='org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='WALPlayer.java' class='org.apache.hadoop.hbase.mapreduce.WALPlayer$WALMapper'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hbase.mapreduce.replication' total_bugs='1' priority_3='1' total_size='484' total_types='4'><ClassStats bugs='1' size='318' priority_3='1' interface='false' sourceFile='VerifyReplication.java' class='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='VerifyReplication.java' class='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$1'></ClassStats><ClassStats bugs='0' size='146' interface='false' sourceFile='VerifyReplication.java' class='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='VerifyReplication.java' class='org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hbase.regionserver' total_bugs='1' priority_3='1' total_size='265' total_types='5'><ClassStats bugs='1' size='111' priority_3='1' interface='false' sourceFile='CompactionTool.java' class='org.apache.hadoop.hbase.regionserver.CompactionTool'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='CompactionTool.java' class='org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionInputFormat'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='CompactionTool.java' class='org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionMapper'></ClassStats><ClassStats bugs='0' size='63' interface='false' sourceFile='CompactionTool.java' class='org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='CompactionTool.java' class='org.apache.hadoop.hbase.regionserver.CompactionTool$CompactionWorker$1'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hbase.snapshot' total_bugs='4' priority_3='4' total_size='721' total_types='11'><ClassStats bugs='2' size='300' priority_3='2' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$1'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$2'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$3'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$Counter'></ClassStats><ClassStats bugs='1' size='240' priority_3='1' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper'></ClassStats><ClassStats bugs='1' size='22' priority_3='1' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotInputSplit'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportSnapshotInputFormat$ExportSnapshotRecordReader'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$Options'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='ExportSnapshot.java' class='org.apache.hadoop.hbase.snapshot.ExportSnapshot$Testing'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hbase.util' total_bugs='0' total_size='132' total_types='2'><ClassStats bugs='0' size='106' interface='false' sourceFile='MapReduceExtendedCell.java' class='org.apache.hadoop.hbase.util.MapReduceExtendedCell'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='MapreduceDependencyClasspathTool.java' class='org.apache.hadoop.hbase.util.MapreduceDependencyClasspathTool'></ClassStats></PackageStats><FindBugsProfile><ClassProfile avgMicrosecondsPerInvocation='523' totalMilliseconds='1820' name='edu.umd.cs.findbugs.classfile.engine.ClassDataAnalysisEngine' maxMicrosecondsPerInvocation='1893' standardDeviationMicrosecondsPerInvocation='263' invocations='3480'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='351' totalMilliseconds='1219' name='edu.umd.cs.findbugs.classfile.engine.ClassInfoAnalysisEngine' maxMicrosecondsPerInvocation='85777' standardDeviationMicrosecondsPerInvocation='1693' invocations='3470'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='890' totalMilliseconds='625' name='edu.umd.cs.findbugs.detect.FieldItemSummary' maxMicrosecondsPerInvocation='68816' standardDeviationMicrosecondsPerInvocation='3518' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='465' totalMilliseconds='584' name='edu.umd.cs.findbugs.classfile.engine.bcel.ValueNumberDataflowFactory' maxMicrosecondsPerInvocation='136913' standardDeviationMicrosecondsPerInvocation='4016' invocations='1255'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='482' totalMilliseconds='559' name='edu.umd.cs.findbugs.classfile.engine.bcel.UnconditionalValueDerefDataflowFactory' maxMicrosecondsPerInvocation='108034' standardDeviationMicrosecondsPerInvocation='3388' invocations='1160'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='757' totalMilliseconds='531' name='edu.umd.cs.findbugs.detect.FindNoSideEffectMethods' maxMicrosecondsPerInvocation='36176' standardDeviationMicrosecondsPerInvocation='1925' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='183' totalMilliseconds='491' name='edu.umd.cs.findbugs.OpcodeStack$JumpInfoFactory' maxMicrosecondsPerInvocation='15846' standardDeviationMicrosecondsPerInvocation='545' invocations='2680'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='411' totalMilliseconds='486' name='edu.umd.cs.findbugs.classfile.engine.bcel.IsNullValueDataflowFactory' maxMicrosecondsPerInvocation='16518' standardDeviationMicrosecondsPerInvocation='1100' invocations='1182'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='426' totalMilliseconds='468' name='edu.umd.cs.findbugs.ba.npe.NullDerefAndRedundantComparisonFinder' maxMicrosecondsPerInvocation='16605' standardDeviationMicrosecondsPerInvocation='1057' invocations='1100'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='360' totalMilliseconds='448' name='edu.umd.cs.findbugs.classfile.engine.bcel.TypeDataflowFactory' maxMicrosecondsPerInvocation='18448' standardDeviationMicrosecondsPerInvocation='1047' invocations='1244'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='258' totalMilliseconds='264' name='edu.umd.cs.findbugs.detect.FindRefComparison$SpecialTypeAnalysis' maxMicrosecondsPerInvocation='6937' standardDeviationMicrosecondsPerInvocation='630' invocations='1023'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='218' totalMilliseconds='258' name='edu.umd.cs.findbugs.classfile.engine.bcel.MethodGenFactory' maxMicrosecondsPerInvocation='193359' standardDeviationMicrosecondsPerInvocation='5616' invocations='1184'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='193' totalMilliseconds='228' name='edu.umd.cs.findbugs.classfile.engine.bcel.CFGFactory' maxMicrosecondsPerInvocation='14486' standardDeviationMicrosecondsPerInvocation='559' invocations='1182'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='52' totalMilliseconds='178' name='edu.umd.cs.findbugs.util.TopologicalSort' maxMicrosecondsPerInvocation='4100' standardDeviationMicrosecondsPerInvocation='112' invocations='3394'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='241' totalMilliseconds='169' name='edu.umd.cs.findbugs.detect.NoteDirectlyRelevantTypeQualifiers' maxMicrosecondsPerInvocation='15018' standardDeviationMicrosecondsPerInvocation='708' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='949' totalMilliseconds='155' name='edu.umd.cs.findbugs.detect.FindOpenStream' maxMicrosecondsPerInvocation='18558' standardDeviationMicrosecondsPerInvocation='2088' invocations='164'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='182' totalMilliseconds='155' name='edu.umd.cs.findbugs.classfile.engine.bcel.JavaClassAnalysisEngine' maxMicrosecondsPerInvocation='31628' standardDeviationMicrosecondsPerInvocation='1280' invocations='852'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='190' totalMilliseconds='134' name='edu.umd.cs.findbugs.detect.BuildObligationPolicyDatabase' maxMicrosecondsPerInvocation='4534' standardDeviationMicrosecondsPerInvocation='363' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='97' totalMilliseconds='115' name='edu.umd.cs.findbugs.classfile.engine.bcel.ConstantDataflowFactory' maxMicrosecondsPerInvocation='5908' standardDeviationMicrosecondsPerInvocation='307' invocations='1182'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='163' totalMilliseconds='114' name='edu.umd.cs.findbugs.detect.FunctionsThatMightBeMistakenForProcedures' maxMicrosecondsPerInvocation='7223' standardDeviationMicrosecondsPerInvocation='406' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='153' totalMilliseconds='107' name='edu.umd.cs.findbugs.detect.OverridingEqualsNotSymmetrical' maxMicrosecondsPerInvocation='16595' standardDeviationMicrosecondsPerInvocation='665' invocations='702'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='150' totalMilliseconds='105' name='edu.umd.cs.findbugs.detect.CalledMethods' maxMicrosecondsPerInvocation='6321' standardDeviationMicrosecondsPerInvocation='363' invocations='702'></ClassProfile></FindBugsProfile></FindBugsSummary><ClassFeatures></ClassFeatures><History></History></BugCollection>