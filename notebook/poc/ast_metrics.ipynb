{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Proof of concept of the AST metrics approach\n",
    "\n",
    "This notebook shows a proof of concept of the solution based on the application of regressiong techniques on a dataset\n",
    "that stores the counts of each different type of node of the Abstract Syntax Tree (AST) of the sources to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "In order to extract numbers out of the code, the basic idea would be to simply count the number of occurrences of each\n",
    "token in the sources, building a bag-of-words.\n",
    "\n",
    "This leads to a very large number of different tokens to build a\n",
    "regression model on, probably raising overfitting problems. It's also worth to notice that in this approach many of the\n",
    "tokens lose their meaning: variable identifiers do not carry information about their type, for example.\n",
    "\n",
    "Building the AST on the sources allows treating the code as what it is and not as plain text, linking each identifier to\n",
    "its type and enabling the distinction between declarations and invocations, casts and argument listings and so on.\n",
    "\n",
    "The process of building the AST and extracting many of what we think are relevant metrics can be achieved with a Java\n",
    "parser such as [ANTLR](https://www.antlr.org/) or [javalang](https://github.com/c2nes/javalang) (we choose the latter\n",
    "because of its speed): once the AST is built, we can simply visit it and increase the counters associated to the type of\n",
    "node encountered.\n",
    "\n",
    "A demonstration of how this approach works is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "// Source code to be analyzed:\npackage greetings;\n\nimport java.util.*;\n\n/*\n * A very simple \"Hello World\" example in Java\n */\npublic class HelloClass<T extends List> extends Class1.SubType<Type, Int3> implements Interface1, Interface2 {\n\n\tpublic static void main(String[] args) {\n\t\t// Print to standard output\n\t\tint j = 1;\n\t\tj++;\n\n\t\tArrayList<String> names = new ArrayList<String>();\n\n\t\tSystem.out.println(\"Hello World!\");\n\t}\n\n}\n\n\n// Analyzer output:\nCounter({'INTERFACES_IMPLEMENTED': 2, 'VARIABLE_DECLARATIONS': 2, 'LITERALS': 2, 'STATEMENTS': 2, 'EXPRESSION_STATEMENTS': 2, 'PACKAGE_DECLARATIONS': 1, 'IMPORT_STATEMENTS': 1, 'CLASS_DECLARATIONS': 1, 'PUBLIC_TYPE_DECLARATIONS': 1, 'PARAMETRIZED_TYPE_DECLARATIONS': 1, 'SUBTYPE_DECLARATIONS': 1, 'METHOD_DECLARATIONS': 1, 'PUBLIC_METHOD_DECLARATIONS': 1, 'STATIC_METHOD_DECLARATIONS': 1, 'BASIC_INT_VARIABLES': 1, 'TYPED_REFERENCES': 1, 'ARRAYLIST_VARIABLES': 1, 'METHOD_INVOCATIONS': 1})\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from src.processing import analyzer\n",
    "\n",
    "source = \"HelloClass.java\"\n",
    "\n",
    "print(\"// Source code to be analyzed:\")\n",
    "with open(f\"./{source}\") as file:\n",
    "    print(file.read())\n",
    "    print()\n",
    "\n",
    "print(\"// Analyzer output:\")\n",
    "print(analyzer.analyze(f\"./{source}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model building\n",
    "\n",
    "The task is about exploring the application of Logistic Regression over the data in order to identify vulnerable files.\n",
    "Building a simple model over the data gathered without any kind of fine tuning yields results to be considered as\n",
    "a benchmark for the models to be built later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\nello\\workspace\\data science & machine learning\\weakness-detector-for-java\\env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
      "c:\\users\\nello\\workspace\\data science & machine learning\\weakness-detector-for-java\\env\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n  Safe files       0.92      1.00      0.95      7343\n  Weak files       0.35      0.03      0.05       686\n\n    accuracy                           0.91      8029\n   macro avg       0.63      0.51      0.50      8029\nweighted avg       0.87      0.91      0.88      8029\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.processing import metrics_extractor\n",
    "\n",
    "features = metrics_extractor.load_features()\n",
    "\n",
    "labels = features[\"IS_WEAK\"]\n",
    "data = features.drop(\"IS_WEAK\", axis=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "report = classification_report(test_labels, predictions, target_names=[\"Safe files\", \"Weak files\"])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This simple classifier doesn't perform well and is unable to generalize due to the imbalance of samples for the two\n",
    "classes, meaning that different weights should be given to the two classes or that the dataset should be artificially\n",
    "cut in order to restore balance, performing undersampling. \n",
    "\n",
    "Following the second approach, the dataset can then be built considering every negative sample and the same amount of\n",
    "positive ones picked randomly. The resulting dataset is then shuffled and split using 80% of samples for training and\n",
    "the remaining 20% for testing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from src.processing.dataset_splitter import __train_data as train_data\n",
    "from src.processing.dataset_splitter import __test_data as test_data\n",
    "from src.processing.dataset_splitter import __train_labels as train_labels\n",
    "from src.processing.dataset_splitter import __test_labels as test_labels\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "report = classification_report(test_labels, predictions, target_names=[\"Safe files\", \"Weak files\"])\n",
    "\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\nello\\workspace\\data science & machine learning\\weakness-detector-for-java\\env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
      "c:\\users\\nello\\workspace\\data science & machine learning\\weakness-detector-for-java\\env\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n  Safe files       0.68      0.85      0.76       689\n  Weak files       0.80      0.60      0.69       689\n\n    accuracy                           0.73      1378\n   macro avg       0.74      0.73      0.72      1378\nweighted avg       0.74      0.73      0.72      1378\n\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are encouraging as the simple model is capable of generalizing although the recall over weak files should be\n",
    "improved in order to avoid leaving out too many vulnerable files.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}