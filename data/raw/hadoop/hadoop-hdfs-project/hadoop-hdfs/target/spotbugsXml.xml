
<BugCollection sequence='0' release='' analysisTimestamp='1568197304870' version='3.1.12' timestamp='1568190930000'><Project projectName='Apache Hadoop HDFS'><Jar>./hadoop/hadoop-hdfs-project/hadoop-hdfs/target/classes</Jar><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-annotations/3.1.1/hadoop-annotations-3.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>/usr/lib/jvm/java-8-openjdk-amd64/jre/../lib/tools.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-auth/3.1.1/hadoop-auth-3.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/net/minidev/json-smart/2.3/json-smart-2.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/zookeeper/zookeeper/3.4.9/zookeeper-3.4.9.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-framework/2.12.0/curator-framework-2.12.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-common/3.1.1/hadoop-common-3.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.19.v20170502/jetty-servlet-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-security/9.3.19.v20170502/jetty-security-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.19.v20170502/jetty-webapp-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-xml/9.3.19.v20170502/jetty-xml-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/re2j/re2j/1.1/re2j-1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-client/2.12.0/curator-client-2.12.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/curator/curator-recipes/2.12.0/curator-recipes-2.12.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/hadoop/hadoop-hdfs-client/3.1.1/hadoop-hdfs-client-3.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.7.8/jackson-annotations-2.7.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-server/9.3.19.v20170502/jetty-server-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-http/9.3.19.v20170502/jetty-http-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-io/9.3.19.v20170502/jetty-io-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-util/9.3.19.v20170502/jetty-util-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/eclipse/jetty/jetty-util-ajax/9.3.19.v20170502/jetty-util-ajax-9.3.19.v20170502.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/io/netty/netty-all/4.0.52.Final/netty-all-4.0.52.Final.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.7.8/jackson-databind-2.7.8.jar</AuxClasspathEntry><AuxClasspathEntry>.m2/repository/com/fasterxml/jackson/core/jackson-core/2.7.8/jackson-core-2.7.8.jar</AuxClasspathEntry><SrcDir>./hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java</SrcDir><WrkDir>./hadoop/hadoop-hdfs-project/hadoop-hdfs/target</WrkDir></Project><BugInstance instanceOccurrenceNum='0' instanceHash='d184bea70fefb987b6955a07ab503017' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_MUTABLE_ARRAY' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable array</ShortMessage><LongMessage>org.apache.hadoop.hdfs.DFSConfigKeys.NNTOP_WINDOWS_MINUTES_DEFAULT is a mutable array</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSConfigKeys' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSConfigKeys' start='37' end='1530' sourcepath='org/apache/hadoop/hdfs/DFSConfigKeys.java' sourcefile='DFSConfigKeys.java'><Message>At DFSConfigKeys.java:[lines 37-1530]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSConfigKeys</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.DFSConfigKeys' signature='[Ljava/lang/String;' name='NNTOP_WINDOWS_MINUTES_DEFAULT' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSConfigKeys' sourcepath='org/apache/hadoop/hdfs/DFSConfigKeys.java' sourcefile='DFSConfigKeys.java'><Message>In DFSConfigKeys.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.DFSConfigKeys.NNTOP_WINDOWS_MINUTES_DEFAULT</Message></Field><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.DFSConfigKeys' start='1130' end='1130' sourcepath='org/apache/hadoop/hdfs/DFSConfigKeys.java' sourcefile='DFSConfigKeys.java' startBytecode='48' primary='true'><Message>At DFSConfigKeys.java:[line 1130]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='518c0c9cd5002c030be87248d75387ae' cweid='476' rank='16' abbrev='NP' category='STYLE' priority='2' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hdfs.DFSUtil.assertAllResultsEqual(Collection)</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil' start='102' end='1682' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 102-1682]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.DFSUtil' signature='(Ljava/util/Collection;)V' name='assertAllResultsEqual' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hdfs.DFSUtil' start='1632' end='1645' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.DFSUtil.assertAllResultsEqual(Collection)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='47' name='currElement' register='3'><Message>Value loaded from currElement</Message></LocalVariable><SourceLine endBytecode='50' classname='org.apache.hadoop.hdfs.DFSUtil' start='1639' end='1639' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='50' primary='true'><Message>At DFSUtil.java:[line 1639]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='98c5b7f3e58f49c3aefd34b70b96d7be' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.DFSUtil.getSuffixIDs(Configuration, InetSocketAddress, String[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil' start='102' end='1682' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 102-1682]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.DFSUtil' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/net/InetSocketAddress;[Ljava/lang/String;)[Ljava/lang/String;' name='getSuffixIDs' primary='true'><SourceLine endBytecode='237' classname='org.apache.hadoop.hdfs.DFSUtil' start='1207' end='1220' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.DFSUtil.getSuffixIDs(Configuration, InetSocketAddress, String[])</Message></Method><SourceLine endBytecode='74' classname='org.apache.hadoop.hdfs.DFSUtil' start='1220' end='1220' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='74' primary='true'><Message>At DFSUtil.java:[line 1220]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c5843649e48ba5e59744c3c5958ed42b' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of ids, which is known to be non-null in org.apache.hadoop.hdfs.DFSUtil.getSuffixIDs(Configuration, InetSocketAddress, String[])</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil' start='102' end='1682' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 102-1682]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.DFSUtil' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/net/InetSocketAddress;[Ljava/lang/String;)[Ljava/lang/String;' name='getSuffixIDs' primary='true'><SourceLine endBytecode='237' classname='org.apache.hadoop.hdfs.DFSUtil' start='1207' end='1220' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.DFSUtil.getSuffixIDs(Configuration, InetSocketAddress, String[])</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='45' name='ids' register='8'><Message>Value loaded from ids</Message></LocalVariable><Method isStatic='true' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.DFSUtil' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/apache/hadoop/hdfs/DFSUtil$AddressMatcher;)[Ljava/lang/String;' name='getSuffixIDs'><SourceLine endBytecode='871' classname='org.apache.hadoop.hdfs.DFSUtil' start='1152' end='1197' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.DFSUtil.getSuffixIDs(Configuration, String, String, String, DFSUtil$AddressMatcher) of type String[]</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='47' classname='org.apache.hadoop.hdfs.DFSUtil' start='1216' end='1216' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='47' primary='true'><Message>Redundant null check at DFSUtil.java:[line 1216]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c8bc3ff1b829a9363d65f646ef269ec2' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress.getNamenodeId() and org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo.getNameNodeID()</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='745' end='771' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 745-771]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' signature='()Ljava/lang/String;' name='getNamenodeId' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='762' end='762' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress.getNamenodeId()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo' start='41' end='131' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.java' sourcefile='RemoteNameNodeInfo.java'><Message>At RemoteNameNodeInfo.java:[lines 41-131]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo' signature='()Ljava/lang/String;' name='getNameNodeID'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo' start='87' end='87' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.java' sourcefile='RemoteNameNodeInfo.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo.getNameNodeID()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='762' end='762' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'><Message>At DFSUtil.java:[line 762]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bf18564e5f0d0629e9d6fb1e23447d4d' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress.getNameserviceId() and org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.getNameServiceId()</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='745' end='771' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 745-771]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' signature='()Ljava/lang/String;' name='getNameserviceId' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='758' end='758' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress.getNameserviceId()</Message></Method><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' signature='()Ljava/lang/String;' name='getNameServiceId'><SourceLine endBytecode='163' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14327' end='14337' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.getNameServiceId()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress' start='758' end='758' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java' startBytecode='0'><Message>At DFSUtil.java:[line 758]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='85e7609a982fcfe6fb4c28f399e78d41' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_COMPARATOR_SHOULD_BE_SERIALIZABLE' instanceOccurrenceMax='0'><ShortMessage>Comparator doesn't implement Serializable</ShortMessage><LongMessage>org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator implements Comparator but not Serializable</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator' start='164' end='189' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 164-189]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator' start='164' end='189' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 164-189]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f35000dd610bc6ec547d86317263ce9f' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_COMPARATOR_SHOULD_BE_SERIALIZABLE' instanceOccurrenceMax='0'><ShortMessage>Comparator doesn't implement Serializable</ShortMessage><LongMessage>org.apache.hadoop.hdfs.DFSUtil$ServiceComparator implements Comparator but not Serializable</LongMessage><Class classname='org.apache.hadoop.hdfs.DFSUtil$ServiceComparator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.DFSUtil$ServiceComparator' start='138' end='154' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 138-154]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.DFSUtil$ServiceComparator</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.DFSUtil$ServiceComparator' start='138' end='154' sourcepath='org/apache/hadoop/hdfs/DFSUtil.java' sourcefile='DFSUtil.java'><Message>At DFSUtil.java:[lines 138-154]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a9824e93dd411e2ec7b333d598934d02' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.HDFSPolicyProvider.getServices() may expose internal representation by returning HDFSPolicyProvider.hdfsServices</LongMessage><Class classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' start='46' end='96' sourcepath='org/apache/hadoop/hdfs/HDFSPolicyProvider.java' sourcefile='HDFSPolicyProvider.java'><Message>At HDFSPolicyProvider.java:[lines 46-96]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.HDFSPolicyProvider</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' signature='()[Lorg/apache/hadoop/security/authorize/Service;' name='getServices' primary='true'><SourceLine endBytecode='45' classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' start='96' end='96' sourcepath='org/apache/hadoop/hdfs/HDFSPolicyProvider.java' sourcefile='HDFSPolicyProvider.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.HDFSPolicyProvider.getServices()</Message></Method><Field isStatic='true' classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' signature='[Lorg/apache/hadoop/security/authorize/Service;' name='hdfsServices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' sourcepath='org/apache/hadoop/hdfs/HDFSPolicyProvider.java' sourcefile='HDFSPolicyProvider.java'><Message>In HDFSPolicyProvider.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.HDFSPolicyProvider.hdfsServices</Message></Field><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.HDFSPolicyProvider' start='96' end='96' sourcepath='org/apache/hadoop/hdfs/HDFSPolicyProvider.java' sourcefile='HDFSPolicyProvider.java' startBytecode='3' primary='true'><Message>At HDFSPolicyProvider.java:[line 96]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b9d795f93fc01af387dea93758613e50' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.BlockListAsLongs.EMPTY isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs' start='45' end='233' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java'><Message>At BlockListAsLongs.java:[lines 45-233]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.BlockListAsLongs</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs' signature='Lorg/apache/hadoop/hdfs/protocol/BlockListAsLongs;' name='EMPTY' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java'><Message>In BlockListAsLongs.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.BlockListAsLongs.EMPTY</Message></Field><SourceLine endBytecode='21' classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs' start='49' end='49' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java' startBytecode='21' primary='true'><Message>At BlockListAsLongs.java:[line 49]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fede253a7883ffc26a3810630f3196ec' rank='19' abbrev='It' category='BAD_PRACTICE' priority='3' type='IT_NO_SUCH_ELEMENT' instanceOccurrenceMax='0'><ShortMessage>Iterator next() method can't throw NoSuchElementException</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next() can't throw NoSuchElementException</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1' start='396' end='431' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java'><Message>At BlockListAsLongs.java:[lines 396-431]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1' signature='()Lorg/apache/hadoop/hdfs/protocol/BlockListAsLongs$BlockReportReplica;' name='next' primary='true'><SourceLine endBytecode='218' classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1' start='414' end='426' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next()</Message></Method><SourceLine synthetic='true' endBytecode='218' classname='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1' start='414' end='426' sourcepath='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' sourcefile='BlockListAsLongs.java' startBytecode='0'><Message>At BlockListAsLongs.java:[lines 414-426]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b51f03a52e6969152154a5f401d0ae55' rank='17' abbrev='BC' category='STYLE' priority='2' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.util.IntrusiveCollection&lt;? extends org.apache.hadoop.util.IntrusiveCollection$Element&gt; to org.apache.hadoop.hdfs.server.namenode.CachePool$DirectiveList in org.apache.hadoop.hdfs.protocol.CacheDirective.insertInternal(IntrusiveCollection, IntrusiveCollection$Element, IntrusiveCollection$Element)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.CacheDirective' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.CacheDirective' start='39' end='266' sourcepath='org/apache/hadoop/hdfs/protocol/CacheDirective.java' sourcefile='CacheDirective.java'><Message>At CacheDirective.java:[lines 39-266]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.CacheDirective</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.CacheDirective' signature='(Lorg/apache/hadoop/util/IntrusiveCollection;Lorg/apache/hadoop/util/IntrusiveCollection$Element;Lorg/apache/hadoop/util/IntrusiveCollection$Element;)V' name='insertInternal' primary='true'><SourceLine endBytecode='20' classname='org.apache.hadoop.hdfs.protocol.CacheDirective' start='222' end='226' sourcepath='org/apache/hadoop/hdfs/protocol/CacheDirective.java' sourcefile='CacheDirective.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.CacheDirective.insertInternal(IntrusiveCollection, IntrusiveCollection$Element, IntrusiveCollection$Element)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/util/IntrusiveCollection;' typeParameters='&lt;? extends org.apache.hadoop.util.IntrusiveCollection$Element&gt;'><SourceLine classname='org.apache.hadoop.util.IntrusiveCollection' start='38' end='373' sourcepath='org/apache/hadoop/util/IntrusiveCollection.java' sourcefile='IntrusiveCollection.java'><Message>At IntrusiveCollection.java:[lines 38-373]</Message></SourceLine><Message>Actual type org.apache.hadoop.util.IntrusiveCollection&lt;? extends org.apache.hadoop.util.IntrusiveCollection$Element&gt;</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/CachePool$DirectiveList;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.CachePool$DirectiveList' start='91' end='100' sourcepath='org/apache/hadoop/hdfs/server/namenode/CachePool.java' sourcefile='CachePool.java'><Message>At CachePool.java:[lines 91-100]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.CachePool$DirectiveList</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='22' name='list' register='1'><Message>Value loaded from list</Message></LocalVariable><SourceLine endBytecode='23' classname='org.apache.hadoop.hdfs.protocol.CacheDirective' start='223' end='223' sourcepath='org/apache/hadoop/hdfs/protocol/CacheDirective.java' sourcefile='CacheDirective.java' startBytecode='23' primary='true'><Message>At CacheDirective.java:[line 223]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a830f7f707615196c859f1b755fa99d1' rank='20' abbrev='CI' category='STYLE' priority='3' type='CI_CONFUSED_INHERITANCE' instanceOccurrenceMax='0'><ShortMessage>Class is final but declares protected field</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException is final but declares protected field org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException.serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException' start='84' end='98' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>At FSLimitException.java:[lines 84-98]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException' signature='J' name='serialVersionUID' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>In FSLimitException.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException.serialVersionUID</Message></Field><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>In FSLimitException.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4b7c38422c31861a9af8c3b85fe96d63' rank='20' abbrev='CI' category='STYLE' priority='3' type='CI_CONFUSED_INHERITANCE' instanceOccurrenceMax='0'><ShortMessage>Class is final but declares protected field</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException is final but declares protected field org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException.serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException' start='52' end='72' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>At FSLimitException.java:[lines 52-72]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException' signature='J' name='serialVersionUID' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>In FSLimitException.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException.serialVersionUID</Message></Field><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException' sourcepath='org/apache/hadoop/hdfs/protocol/FSLimitException.java' sourcefile='FSLimitException.java'><Message>In FSLimitException.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='27fa710d6b74f84fe6e423c5ced74f' rank='19' abbrev='ISC' category='BAD_PRACTICE' priority='3' type='ISC_INSTANTIATE_STATIC_CLASS' instanceOccurrenceMax='0'><ShortMessage>Needless instantiation of class that only supplies static methods</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.LayoutFlags.read(DataInputStream) needlessly instantiates a class that only supplies static methods</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.LayoutFlags' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutFlags' start='47' end='63' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutFlags.java' sourcefile='LayoutFlags.java'><Message>At LayoutFlags.java:[lines 47-63]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.LayoutFlags</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocol.LayoutFlags' signature='(Ljava/io/DataInputStream;)Lorg/apache/hadoop/hdfs/protocol/LayoutFlags;' name='read' primary='true'><SourceLine endBytecode='147' classname='org.apache.hadoop.hdfs.protocol.LayoutFlags' start='47' end='55' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutFlags.java' sourcefile='LayoutFlags.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.LayoutFlags.read(DataInputStream)</Message></Method><SourceLine endBytecode='59' classname='org.apache.hadoop.hdfs.protocol.LayoutFlags' start='55' end='55' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutFlags.java' sourcefile='LayoutFlags.java' startBytecode='59' primary='true'><Message>At LayoutFlags.java:[line 55]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='faafd1b3f508d888578490d65c0e1345' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature defines non-transient non-serializable instance field info</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature' start='77' end='157' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>At LayoutVersion.java:[lines 77-157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature' signature='Lorg/apache/hadoop/hdfs/protocol/LayoutVersion$FeatureInfo;' name='info' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>In LayoutVersion.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature.info</Message></Field><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/protocol/LayoutVersion$FeatureInfo;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' start='172' end='228' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>At LayoutVersion.java:[lines 172-228]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>In LayoutVersion.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='58c090a9490797bdee09703a3273641e' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo.getSpecialFeatures() may expose internal representation by returning LayoutVersion$FeatureInfo.specialFeatures</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' start='172' end='228' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>At LayoutVersion.java:[lines 172-228]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' signature='()[Lorg/apache/hadoop/hdfs/protocol/LayoutVersion$LayoutFeature;' name='getSpecialFeatures' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' start='228' end='228' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo.getSpecialFeatures()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' signature='[Lorg/apache/hadoop/hdfs/protocol/LayoutVersion$LayoutFeature;' name='specialFeatures' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>In LayoutVersion.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo.specialFeatures</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo' start='228' end='228' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java' startBytecode='4' primary='true'><Message>At LayoutVersion.java:[line 228]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8fe48a1d00dd6dd7253f7e6c3cde312f' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_COMPARATOR_SHOULD_BE_SERIALIZABLE' instanceOccurrenceMax='0'><ShortMessage>Comparator doesn't implement Serializable</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator implements Comparator but not Serializable</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator' start='232' end='236' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>At LayoutVersion.java:[lines 232-236]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator' start='232' end='236' sourcepath='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' sourcefile='LayoutVersion.java'><Message>At LayoutVersion.java:[lines 232-236]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4fa2ef92ab78fea6770a8aa838de4f15' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BlackListBasedTrustedChannelResolver.blackListForClient not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.isTrusted()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='30' end='141' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java'><Message>At BlackListBasedTrustedChannelResolver.java:[lines 30-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' signature='Lorg/apache/hadoop/util/CombinedIPList;' name='blackListForClient' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java'><Message>In BlackListBasedTrustedChannelResolver.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.blackListForClient</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' signature='()Z' name='isTrusted' primary='true'><SourceLine endBytecode='115' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='133' end='136' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.isTrusted()</Message></Method><SourceLine endBytecode='10' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='134' end='134' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java' startBytecode='10' primary='true'><Message>At BlackListBasedTrustedChannelResolver.java:[line 134]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='226bdb164f11d1827e1826603b3a8adf' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BlackListBasedTrustedChannelResolver.blackListForServer not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.isTrusted(InetAddress)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='30' end='141' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java'><Message>At BlackListBasedTrustedChannelResolver.java:[lines 30-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' signature='Lorg/apache/hadoop/util/CombinedIPList;' name='blackListForServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java'><Message>In BlackListBasedTrustedChannelResolver.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.blackListForServer</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' signature='(Ljava/net/InetAddress;)Z' name='isTrusted' primary='true'><SourceLine endBytecode='82' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='141' end='141' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver.isTrusted(InetAddress)</Message></Method><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver' start='141' end='141' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' sourcefile='BlackListBasedTrustedChannelResolver.java' startBytecode='8' primary='true'><Message>At BlackListBasedTrustedChannelResolver.java:[line 141]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5d1385d2799932178b15c7359cfbb896' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>WhitelistBasedTrustedChannelResolver.whiteListForServer not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.isTrusted(InetAddress)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='28' end='117' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java'><Message>At WhitelistBasedTrustedChannelResolver.java:[lines 28-117]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' signature='Lorg/apache/hadoop/util/CombinedIPWhiteList;' name='whiteListForServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java'><Message>In WhitelistBasedTrustedChannelResolver.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.whiteListForServer</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' signature='(Ljava/net/InetAddress;)Z' name='isTrusted' primary='true'><SourceLine endBytecode='63' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='117' end='117' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.isTrusted(InetAddress)</Message></Method><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='117' end='117' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java' startBytecode='8' primary='true'><Message>At WhitelistBasedTrustedChannelResolver.java:[line 117]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='56452b0a51cab83c64f550d758ddc764' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>WhitelistBasedTrustedChannelResolver.whitelistForClient not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.isTrusted()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='28' end='117' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java'><Message>At WhitelistBasedTrustedChannelResolver.java:[lines 28-117]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' signature='Lorg/apache/hadoop/util/CombinedIPWhiteList;' name='whitelistForClient' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java'><Message>In WhitelistBasedTrustedChannelResolver.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.whitelistForClient</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' signature='()Z' name='isTrusted' primary='true'><SourceLine endBytecode='96' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='110' end='112' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver.isTrusted()</Message></Method><SourceLine endBytecode='10' classname='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver' start='110' end='110' sourcepath='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' sourcefile='WhitelistBasedTrustedChannelResolver.java' startBytecode='10' primary='true'><Message>At WhitelistBasedTrustedChannelResolver.java:[line 110]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7de573dfb693610abf8b8f26930cc476' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5152' end='5508' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5152-5508]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5354' end='5381' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$BlockPoolRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' start='4298' end='4625' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4298-4625]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='103' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='104' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5376' end='5376' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='104' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5376]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2f37f0e55372a769ec2b2297824086e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5152' end='5508' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5152-5508]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5354' end='5381' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$ListRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' start='2818' end='3350' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2818-3350]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='88' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='89' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5371' end='5371' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='89' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5371]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a563b4abf067e3cba903c4f208822158' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5152' end='5508' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5152-5508]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5354' end='5381' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$ReadRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' start='1700' end='2236' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1700-2236]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='73' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='74' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5366' end='5366' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='74' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5366]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a65c242ca1a72a3fae1227e586a373d7' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5152' end='5508' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5152-5508]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5354' end='5381' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$WriteRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' start='801' end='1337' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 801-1337]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='58' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='59' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService' start='5361' end='5361' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='59' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5361]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8f14eb491fce6cfaa509d908725da99' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5229' end='5299' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5229-5299]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5240' end='5255' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$BlockPoolRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' start='4298' end='4625' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4298-4625]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='106' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5253' end='5253' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='107' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5253]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c381ceb9ddb671b0744d425d3d460b4f' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5229' end='5299' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5229-5299]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5240' end='5255' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$ListRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' start='2818' end='3350' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2818-3350]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='91' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='92' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5251' end='5251' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='92' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5251]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7548b2354ab66b1b98709ee9bb2e337e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5229' end='5299' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5229-5299]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5240' end='5255' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$ReadRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' start='1700' end='2236' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1700-2236]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='76' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='77' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5249' end='5249' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='77' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5249]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7dd2a2fec3399b97464f1410c862a8dc' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5229' end='5299' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 5229-5299]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5240' end='5255' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos$WriteRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' start='801' end='1337' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 801-1337]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='61' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='62' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2' start='5247' end='5247' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='62' primary='true'><Message>At AliasMapProtocolProtos.java:[line 5247]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='20dc35d5ac97ce6881a38dd069126399' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' start='4298' end='4625' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4298-4625]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' start='4369' end='4369' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 4369]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5725625cf35410bd5798894f4a424932' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' start='4298' end='4625' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4298-4625]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9d52850d844b573a55d832119311c117' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder' start='4517' end='4616' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4517-4616]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder' start='4543' end='4545' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder' start='4543' end='4543' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='3' primary='true'><Message>At AliasMapProtocolProtos.java:[line 4543]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='47bde7b2ed05c6c06792417dd5bdd3c0' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4651' end='5142' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4651-5142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4728' end='4728' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 4728]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c2388e156ebbc22c863b9a1348500683' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.getBlockPoolId() and org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.getBlockPoolID()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4651' end='5142' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4651-5142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' signature='()Ljava/lang/String;' name='getBlockPoolId' primary='true'><SourceLine endBytecode='163' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4757' end='4767' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.getBlockPoolId()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6174' end='7650' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 6174-7650]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' signature='()Ljava/lang/String;' name='getBlockPoolID'><SourceLine endBytecode='163' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6411' end='6421' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.getBlockPoolID()</Message></Method><SourceLine synthetic='true' endBytecode='163' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4757' end='4767' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'><Message>At AliasMapProtocolProtos.java:[lines 4757-4767]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3808f08b6b4f40cace34013fbe41a73a' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.getBlockPoolIdBytes() and org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.getBlockPoolIDBytes()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4651' end='5142' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4651-5142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' signature='()Lcom/google/protobuf/ByteString;' name='getBlockPoolIdBytes' primary='true'><SourceLine endBytecode='131' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4775' end='4783' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.getBlockPoolIdBytes()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6174' end='7650' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 6174-7650]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' signature='()Lcom/google/protobuf/ByteString;' name='getBlockPoolIDBytes'><SourceLine endBytecode='131' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6433' end='6441' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.getBlockPoolIDBytes()</Message></Method><SourceLine synthetic='true' endBytecode='131' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4775' end='4783' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'><Message>At AliasMapProtocolProtos.java:[lines 4775-4783]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='319e6b4c7c3cddad42fa3cfd38708611' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.hasBlockPoolId() and org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.hasBlockPoolID()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4651' end='5142' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4651-5142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' signature='()Z' name='hasBlockPoolId' primary='true'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4751' end='4751' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.hasBlockPoolId()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6174' end='7650' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 6174-7650]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' signature='()Z' name='hasBlockPoolID'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6401' end='6401' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.hasBlockPoolID()</Message></Method><SourceLine synthetic='true' endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4751' end='4751' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'><Message>At AliasMapProtocolProtos.java:[line 4751]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='22f3168b9910ad5fdc1ead19ad2473e5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' start='4651' end='5142' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4651-5142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e268071654c96a3f1b00c4eb6bf781f7' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder' start='4941' end='5133' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 4941-5133]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder' start='4967' end='4969' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder' start='4967' end='4967' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='3' primary='true'><Message>At AliasMapProtocolProtos.java:[line 4967]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='453e62cabf0cf6342ff45b3099b0a08e' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' start='45' end='776' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 45-776]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' start='143' end='143' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 143]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8483f2315ec310bc0efb053bdb4cd082' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' start='45' end='776' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 45-776]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b1b39c3f2dd52b8893fe7f60131ec69b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' start='2818' end='3350' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2818-3350]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' start='2903' end='2903' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 2903]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='60266907724f28277d28039f27429523' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' start='2818' end='3350' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2818-3350]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f84152728f6ee219be4ec0b38e9d7d49' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' start='3400' end='4287' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 3400-4287]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' start='3496' end='3496' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 3496]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e693e99b35c11d6848016b95bc5b2ad0' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' start='3400' end='4287' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 3400-4287]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d8fd53f8d0b4ffcf2b38c29d42c640b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' start='1700' end='2236' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1700-2236]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' start='1785' end='1785' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 1785]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='617b9446d00b20815a3fa64b8252557b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' start='1700' end='2236' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1700-2236]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='737a76ac8cadaf26c474f71a9a1e3d93' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' start='2261' end='2793' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2261-2793]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' start='2346' end='2346' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 2346]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='de71363e79fb59645a430b1c82dc040f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' start='2261' end='2793' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 2261-2793]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='66746d38728319fd5134bd0edfe27d31' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' start='801' end='1337' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 801-1337]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' start='886' end='886' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 886]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8c39e69de99cafd417e95bc0a418fcdd' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' start='801' end='1337' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 801-1337]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c94f30b37cf0d78aeb59acf4ef47be57' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' start='1348' end='1675' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1348-1675]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' start='1419' end='1419' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='7' primary='true'><Message>At AliasMapProtocolProtos.java:[line 1419]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ae883743e7b5adbcd1e5d9314d1bd269' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' start='1348' end='1675' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1348-1675]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>In AliasMapProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ef88c13f632da10944986eb08806d33f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder' start='1567' end='1666' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java'><Message>At AliasMapProtocolProtos.java:[lines 1567-1666]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder' start='1593' end='1595' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder' start='1593' end='1593' sourcepath='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' sourcefile='AliasMapProtocolProtos.java' startBytecode='3' primary='true'><Message>At AliasMapProtocolProtos.java:[line 1593]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e93885c41e28ff113d8d8b033f4720b9' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService' start='362' end='556' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>At DatanodeLifelineProtocolProtos.java:[lines 362-556]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService' start='474' end='486' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='46' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='47' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService' start='481' end='481' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='47' primary='true'><Message>At DatanodeLifelineProtocolProtos.java:[line 481]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1de8e794401544d9795c0ad6dcd863fb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2' start='391' end='443' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>At DatanodeLifelineProtocolProtos.java:[lines 391-443]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='23' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2' start='402' end='411' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='49' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='50' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2' start='409' end='409' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='50' primary='true'><Message>At DatanodeLifelineProtocolProtos.java:[line 409]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5a6ddca666a65ed74a0cdfb4de22ab87' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' start='21' end='352' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>At DatanodeLifelineProtocolProtos.java:[lines 21-352]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>In DatanodeLifelineProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' start='92' end='92' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeLifelineProtocolProtos.java:[line 92]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2b42c178e70c9343921cb3ad9429a24c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' start='21' end='352' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>At DatanodeLifelineProtocolProtos.java:[lines 21-352]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>In DatanodeLifelineProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>In DatanodeLifelineProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='286d1d5e4d8623ecb537fc9d00fe41e4' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder' start='244' end='343' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java'><Message>At DatanodeLifelineProtocolProtos.java:[lines 244-343]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder' start='270' end='272' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder' start='270' end='270' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' sourcefile='DatanodeLifelineProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeLifelineProtocolProtos.java:[line 270]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b42aa9a346c508ebf1898727eed6f52e' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' start='3861' end='4312' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 3861-4312]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' start='3938' end='3938' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 3938]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e125d048e8f91bd1ca05a5a8c9833f83' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' start='3861' end='4312' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 3861-4312]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='45afa75eab00821ba15c222d9098131c' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder' start='4138' end='4303' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 4138-4303]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder' start='4164' end='4166' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder' start='4164' end='4164' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 4164]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='30a394b2786594434dc38d7771e38457' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' start='4454' end='6559' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 4454-6559]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' start='4586' end='4586' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 4586]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9efaad35f0e77fba409ec38f757edcf8' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' start='4454' end='6559' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 4454-6559]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='393368e4639abaad9553cbe60747bc57' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' start='9716' end='10406' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 9716-10406]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' start='9799' end='9799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 9799]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='723527f78108c4582ab1e16ef2e1a60f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' start='9716' end='10406' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 9716-10406]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='acec30c22deb03a05f660d5f6f1b7b99' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder' start='10008' end='10397' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 10008-10397]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder' signature='()Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockECReconstructionCommandProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder' start='10075' end='10087' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder' start='10076' end='10076' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='14' primary='true'><Message>At DatanodeProtocolProtos.java:[line 10076]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='91ed2019927eb941f7c18b7cecb2f8d0' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' start='6615' end='7450' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 6615-7450]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' start='6727' end='6727' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 6727]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='be0c1012fd7b7185bf19e52998ad8fb7' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' start='6615' end='7450' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 6615-7450]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a9810124dd3cb411948459852934a954' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder' start='7117' end='7441' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 7117-7441]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder' start='7143' end='7145' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder' start='7143' end='7143' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 7143]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='94104dacbb1d9fe188c80404157dd7da' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' start='24164' end='25220' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 24164-25220]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' start='24265' end='24265' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 24265]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='493dddede089b1de49674492e45b8fbb' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' start='24164' end='25220' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 24164-25220]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5ce918e856c1cda1b81b34a5e85e1a47' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' start='25236' end='25568' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25236-25568]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' start='25307' end='25307' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 25307]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a52cd503fece524e3f38f6745f7f870' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' start='25236' end='25568' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25236-25568]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7f01b28406a74522f68cecee9a14f9e6' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder' start='25460' end='25559' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25460-25559]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder' start='25486' end='25488' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder' start='25486' end='25486' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 25486]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b4839f1c3ba3cfbd8fbef905adfa7fe4' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' start='7491' end='8181' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 7491-8181]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' start='7574' end='7574' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 7574]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2d0a6943bd9671aa46fa96dbfed7327c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' start='7491' end='8181' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 7491-8181]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1e596ed63a22c85a69c0bbb4c7ce0eee' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder' start='7783' end='8172' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 7783-8172]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder' signature='()Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockRecoveryCommandProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder' start='7850' end='7862' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder' start='7851' end='7851' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='14' primary='true'><Message>At DatanodeProtocolProtos.java:[line 7851]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d47d8c8ed78e3bb16ba7256225cd1cd3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' start='17982' end='18865' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 17982-18865]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' start='18079' end='18079' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 18079]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='de7e8ad0f9c7817d9a5fc94bc51ba24d' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' start='17982' end='18865' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 17982-18865]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1b032a05ce121d9224c74fee56394396' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder' start='18447' end='18856' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 18447-18856]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder' start='18473' end='18475' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder' start='18473' end='18473' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 18473]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9183c967ef1e931c828cf763dfe16b93' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' start='16618' end='17879' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 16618-17879]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' start='16732' end='16732' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 16732]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e1b50802d3cc8746524e08f138c450b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' start='16618' end='17879' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 16618-17879]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8af6090bde930d02318113cdf36b7125' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' start='19945' end='20482' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 19945-20482]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' start='20030' end='20030' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 20030]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='23ba848fea003c96cd8038e30a387b96' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' start='19945' end='20482' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 19945-20482]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6648361758ec4f5eb4147b012a77cf3e' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' start='20543' end='21403' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 20543-21403]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' start='20657' end='20657' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 20657]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1658405f9256afe2bd9ed9842000607f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' start='20543' end='21403' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 20543-21403]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4e32d9ed71511d1d5cb66e55b6fe209f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' start='21428' end='21960' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 21428-21960]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' start='21513' end='21513' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 21513]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e15cecbf50e9742d4e9f3a7d3d67e2f3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' start='21428' end='21960' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 21428-21960]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2b6e9424b8ae862e1e716a1193573615' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' start='28228' end='29647' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 28228-29647]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' start='28355' end='28355' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 28355]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3034a3d1ae243d615391a093b5658c70' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' start='28228' end='29647' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 28228-29647]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3da831ef677e1974998fa0b4fa313331' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' start='29663' end='29995' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 29663-29995]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' start='29734' end='29734' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 29734]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8cc82830cba48951ab27affa6160662c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' start='29663' end='29995' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 29663-29995]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ed43aa9c6fbfd58980e079da581bd3d1' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder' start='29887' end='29986' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 29887-29986]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder' start='29913' end='29915' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder' start='29913' end='29913' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 29913]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe91d6ddafbb7eb97a8911707ea9ffbb' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' start='1573' end='3826' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 1573-3826]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' start='1760' end='1760' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 1760]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3421ee40986751cca0b2ab8547e4ea64' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' start='1573' end='3826' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 1573-3826]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bf3fb7bc2c3ba14237ff5a81ed863631' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' start='24164' end='25220' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 24164-25220]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='138' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='139' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31910' end='31910' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='139' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31910]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b9cd93902f57fe13b6aedce62aa85d42' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' start='16618' end='17879' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 16618-17879]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='108' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='109' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31900' end='31900' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='109' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31900]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='690afe09a7ed38c99d7c9326320323bb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$CacheReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' start='20543' end='21403' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 20543-21403]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='123' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='124' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31905' end='31905' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='124' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31905]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='850b310a297ff21905286f95b1985874' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' start='28228' end='29647' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 28228-29647]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='198' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='199' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31930' end='31930' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='199' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31930]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='116ebb2073948e2e9a9eba5b75004633' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$ErrorReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' start='25657' end='26686' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25657-26686]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='153' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='154' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31915' end='31915' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='154' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31915]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cad9e9d875a4d00cd619f1d8dc41e620' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='93' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='94' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31895' end='31895' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='94' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31895]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b00c1ae3b9de05ad96227bc80adb43ce' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$RegisterDatanodeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' start='10448' end='11037' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 10448-11037]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='78' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31890' end='31890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='79' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31890]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4fac0156be81be4615b7cd4696370f68' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$ReportBadBlocksRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' start='27075' end='27765' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27075-27765]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='183' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='184' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31925' end='31925' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='184' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31925]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='985857f7ac0145ca664a9457fb2f7fe7' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31435' end='32157' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31435-32157]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31883' end='31935' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos$VersionRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='168' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='169' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService' start='31920' end='31920' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='169' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31920]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2bfe16ebc36b536f720baaa98b0901b3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto' start='24164' end='25220' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 24164-25220]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='141' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='142' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31666' end='31666' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='142' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31666]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6c06a3261d4b147c78cd169f68bb4241' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$BlockReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto' start='16618' end='17879' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 16618-17879]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='111' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='112' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31662' end='31662' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='112' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31662]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f4875c3526e0a54e151db5569076a851' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$CacheReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto' start='20543' end='21403' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 20543-21403]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='126' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='127' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31664' end='31664' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='127' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31664]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='53268b21d00242ea2beac5d5d5e4d6ee' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto' start='28228' end='29647' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 28228-29647]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='201' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='202' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31674' end='31674' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='202' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31674]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4f065ade605272f2c4e3f9f8771531af' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$ErrorReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' start='25657' end='26686' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25657-26686]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='156' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='157' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31668' end='31668' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='157' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31668]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='88854adf7789baef3d0035697390c363' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='96' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='97' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31660' end='31660' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='97' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31660]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='84bc8d70ace9d8f13c4e77062487a425' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$RegisterDatanodeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' start='10448' end='11037' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 10448-11037]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='81' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='82' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31658' end='31658' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='82' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31658]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='126db8a16e16e36779fd3d8fad16be26' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$ReportBadBlocksRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' start='27075' end='27765' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27075-27765]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='186' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='187' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31672' end='31672' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='187' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31672]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c18dcbf4f3ffaf4399c423c31e30ab95' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31640' end='31740' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31640-31740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31651' end='31676' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos$VersionRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='171' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='172' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2' start='31670' end='31670' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='172' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31670]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='91f864857c37636125c5058668d1cf1a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='127' end='1412' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 127-1412]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='243' end='243' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 243]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c96b20ad2350ed42f48dbda09252a00a' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto.hasDatanodeID() and org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto.hasDataNodeId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='127' end='1412' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 127-1412]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' signature='()Z' name='hasDatanodeID' primary='true'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='270' end='270' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto.hasDatanodeID()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' start='30046' end='30624' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30046-30624]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' signature='()Z' name='hasDataNodeId'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' start='30151' end='30151' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto.hasDataNodeId()</Message></Method><SourceLine synthetic='true' endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='270' end='270' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'><Message>At DatanodeProtocolProtos.java:[line 270]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='be3a95db78294f072c0339c2d9517c9b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' start='127' end='1412' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 127-1412]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='eab4716ce1f3e423c28e72a6b4848256' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' start='25657' end='26686' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25657-26686]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' start='25752' end='25752' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 25752]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b4ec00383f3569876fa3d5fa2555f07' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' start='25657' end='26686' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 25657-26686]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='21bb1a317520956709677dd481acc78f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' start='26702' end='27034' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 26702-27034]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' start='26773' end='26773' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 26773]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7422ded7772cc84e3566e900afa5978e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' start='26702' end='27034' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 26702-27034]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c7c4879c19a6b251d922b575b9e56c08' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder' start='26926' end='27025' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 26926-27025]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder' start='26952' end='26954' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder' start='26952' end='26952' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 26952]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7248f1be6f22261dff44761a0f3d02b8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' start='8224' end='8756' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 8224-8756]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' start='8301' end='8301' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 8301]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b1e9e7513d7fbfca9bd87d9aa56fcc0' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' start='8224' end='8756' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 8224-8756]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='509aefe1499159ae8333e5074eb7fafa' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder' start='8531' end='8747' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 8531-8747]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder' start='8557' end='8559' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder' start='8557' end='8557' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 8557]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='28be615662cf330484eeefbba24c3e47' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12783' end='12783' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 12783]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='34c0eb13ce9150502c5ea54f642cb269' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' start='12622' end='14945' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12622-14945]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='782e5eba4e62475268bc5462704badad' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' start='15059' end='16526' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 15059-16526]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' start='15186' end='15186' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 15186]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='72c6cd7aedf5e2b656987267402372c3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' start='15059' end='16526' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 15059-16526]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cb5daabad3b604ea2d517f15f7dcad3f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' start='8786' end='9327' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 8786-9327]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' start='8871' end='8871' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 8871]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3ca5bceb0b04dd8d92334391d7b341d4' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' start='8786' end='9327' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 8786-9327]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2d752f0a624b63e13ecad8bac18a8bb6' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' start='22016' end='22912' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 22016-22912]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' start='22117' end='22117' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 22117]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4858a42bc7de0fc619d7b355af713baf' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' start='22016' end='22912' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 22016-22912]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='35b0192fe3da5879ab1e68441cfa5dde' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' start='9343' end='9675' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 9343-9675]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' start='9414' end='9414' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 9414]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7a4a5e0220a80d5fd65ad83f8d42c47' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' start='9343' end='9675' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 9343-9675]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e2cbfc66cd66908ed146a2640cc96ffc' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder' start='9567' end='9666' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 9567-9666]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder' start='9593' end='9595' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder' start='9593' end='9593' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 9593]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='916ac40742ecca20ae2aad2e36bcc86b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' start='10448' end='11037' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 10448-11037]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' start='10533' end='10533' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 10533]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e0f257a9a06c9c6ca21442eba879261' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' start='10448' end='11037' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 10448-11037]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c798e3e5dad06afd4e2d3ddd5ce25c49' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' start='11081' end='11672' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 11081-11672]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' start='11166' end='11166' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 11166]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9245c9533c373c7386bc168457bc9cd9' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' start='11081' end='11672' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 11081-11672]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6b23b8821ccba36a285e31164e961b8d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' start='27075' end='27765' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27075-27765]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' start='27158' end='27158' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 27158]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='22ac00a412a6bb7b64f4a501e0645da0' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' start='27075' end='27765' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27075-27765]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a92cf7d91c1d34b1db836c585ba671a2' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder' start='27367' end='27756' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27367-27756]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder' signature='()Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$ReportBadBlocksRequestProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder' start='27434' end='27446' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder' start='27435' end='27435' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='14' primary='true'><Message>At DatanodeProtocolProtos.java:[line 27435]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c278b0bf49e5f268d43478085668b93f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' start='27781' end='28113' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27781-28113]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' start='27852' end='27852' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 27852]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fddc07f18c2134f61a7f62384f1185da' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' start='27781' end='28113' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 27781-28113]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e2c80eec4ac177a99c9a6f503209711c' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder' start='28005' end='28104' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 28005-28104]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder' start='28031' end='28033' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder' start='28031' end='28031' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 28031]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9c04568b49ece4d5a76cff0f60e2977' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' start='30688' end='31419' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30688-31419]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' start='30780' end='30780' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 30780]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='14f56b33306e6f9e4e7fc8011fdb409' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' start='30688' end='31419' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30688-31419]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5594d91edaedb6cc58978a07132bffe1' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder' start='31096' end='31410' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 31096-31410]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder' start='31122' end='31124' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder' start='31122' end='31122' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 31122]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='89d76a7c54a26a0006f53a219e73266a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' start='30046' end='30624' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30046-30624]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' start='30128' end='30128' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 30128]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8e112616dcbe55957329ce426c98afea' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' start='30046' end='30624' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30046-30624]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c299cc735c4a617a88a057d6a4182b63' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder' start='30385' end='30615' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 30385-30615]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder' start='30411' end='30413' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder' start='30411' end='30411' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 30411]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e835f7e07a42a7d4b36d5b0efbd892a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' start='18945' end='19915' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 18945-19915]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' start='19070' end='19070' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 19070]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4e2f7a8484be25e41c1c8524a95496cd' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' start='18945' end='19915' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 18945-19915]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6ad3451d1b7ff8f143296b111d8515ee' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto defines non-transient non-serializable instance field blocksBuffers_</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' start='18945' end='19915' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 18945-19915]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' signature='Ljava/util/List;' name='blocksBuffers_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto.blocksBuffers_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f80befa225c948b03e9b932efed7bb35' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' start='22994' end='24092' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 22994-24092]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' start='23095' end='23095' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 23095]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='790f53a34cce63ea3818de0d7051d206' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' start='22994' end='24092' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 22994-24092]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='743d66ccdd7ee566db890a32e27938ae' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' start='11730' end='12417' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 11730-12417]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' start='11823' end='11823' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At DatanodeProtocolProtos.java:[line 11823]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='53a697e37c8d69bcba7876f0059dbc63' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' start='11730' end='12417' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 11730-12417]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>In DatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='df846b64029cee21a4f304de65dc05' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder' start='12102' end='12408' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java'><Message>At DatanodeProtocolProtos.java:[lines 12102-12408]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder' start='12128' end='12130' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder' start='12128' end='12128' sourcepath='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' sourcefile='DatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At DatanodeProtocolProtos.java:[line 12128]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f939a08578a8f2afc0f78a73b79351b6' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' start='57' end='903' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>At EditLogProtos.java:[lines 57-903]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' start='145' end='145' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java' startBytecode='7' primary='true'><Message>At EditLogProtos.java:[line 145]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='baa5891ed878dfd3a63dde4c7dfba26e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' start='57' end='903' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>At EditLogProtos.java:[lines 57-903]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cf0f0a64aaa47c4a49f06faa835600a5' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' start='954' end='1792' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>At EditLogProtos.java:[lines 954-1792]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' start='1042' end='1042' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java' startBytecode='7' primary='true'><Message>At EditLogProtos.java:[line 1042]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2580dbec7ca7527a8382b4fcc637500b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' start='954' end='1792' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>At EditLogProtos.java:[lines 954-1792]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' sourcefile='EditLogProtos.java'><Message>In EditLogProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3015452c4441041b129949d62555b99d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' start='230' end='899' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 230-899]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' start='317' end='317' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 317]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f26dbfa6da7cac3c6ad710f45fc929c8' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' start='230' end='899' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 230-899]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='922d422f29327b03c062ab4a0d61c7da' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto defines non-transient non-serializable instance field keyBytes_</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' start='230' end='899' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 230-899]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' signature='Lcom/google/protobuf/ByteString;' name='keyBytes_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto.keyBytes_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dace3db27f7311e2e9ce04d38f54e48f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder' start='602' end='890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 602-890]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder' start='628' end='630' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder' start='628' end='628' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='3' primary='true'><Message>At HdfsServerProtos.java:[line 628]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a3a68fbd7143ac4396a97bd9d6121eca' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' start='2302' end='3746' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 2302-3746]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' start='2460' end='2460' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 2460]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4ce5f7f21b488e94eb85db06abae877' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' start='2302' end='3746' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 2302-3746]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='61a9a14e01b7b41736a9cc1467e9290c' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto defines non-transient non-serializable instance field indices_</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' start='2302' end='3746' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 2302-3746]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' signature='Lcom/google/protobuf/ByteString;' name='indices_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto.indices_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2669438ecbba5d5911a11bcc996f42a2' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' start='3787' end='4477' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 3787-4477]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' start='3870' end='3870' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 3870]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e62308c05cb4654cb74d3e3b0a0b75c6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' start='3787' end='4477' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 3787-4477]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='39f6a414a96819d1cd14fb28e6a79c31' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder' start='4079' end='4468' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 4079-4468]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder' signature='()Lorg/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos$BlocksWithLocationsProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder' start='4146' end='4158' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder' start='4147' end='4147' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='14' primary='true'><Message>At HdfsServerProtos.java:[line 4147]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9e20d155112115e17b5e0c94089e3dc9' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' start='10025' end='10729' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 10025-10729]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' start='10115' end='10115' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 10115]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='aeb9961892fcedaab72c11e771c2cab4' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' start='10025' end='10729' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 10025-10729]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2d8f86229db4693010e09695155ef75a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' start='9087' end='9962' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 9087-9962]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' start='9187' end='9187' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 9187]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f013a1a1201ae81619425cc29184cf1d' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' start='9087' end='9962' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 9087-9962]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3be1bff92fdd559958d356108756ef18' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='984' end='2144' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 984-2144]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='1095' end='1095' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 1095]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7eb57fb0a7dc716baf134ea3cdf36d74' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto.getTokenLifeTime() and org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys.getTokenLifetime()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='984' end='2144' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 984-2144]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' signature='()J' name='getTokenLifeTime' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='1156' end='1156' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto.getTokenLifeTime()</Message></Method><Class classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys'><SourceLine classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' start='35' end='115' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java'><Message>At ExportedBlockKeys.java:[lines 35-115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' signature='()J' name='getTokenLifetime'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' start='64' end='64' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys.getTokenLifetime()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='1156' end='1156' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'><Message>At HdfsServerProtos.java:[line 1156]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='64dd3bb08e5386aaeafc1fd1192c66ab' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' start='984' end='2144' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 984-2144]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8d08d741a6574ca5b933666d97204844' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='14815' end='15421' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 14815-15421]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='14903' end='14903' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 14903]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6729c2a8030a76db224344f49b64775a' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.getTxid() and org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.getTxId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='14815' end='15421' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 14815-15421]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' signature='()J' name='getTxid' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='15030' end='15030' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.getTxid()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4577' end='4998' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4577-4998]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' signature='()J' name='getTxId'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4683' end='4683' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.getTxId()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='15030' end='15030' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'><Message>At HdfsServerProtos.java:[line 15030]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cc627d95c7d9a492e091db5b50782be5' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.hasTxid() and org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.hasTxId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='14815' end='15421' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 14815-15421]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' signature='()Z' name='hasTxid' primary='true'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='15024' end='15024' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.hasTxid()</Message></Method><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4577' end='4998' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4577-4998]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' signature='()Z' name='hasTxId'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4677' end='4677' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.hasTxId()</Message></Method><SourceLine synthetic='true' endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='15024' end='15024' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'><Message>At HdfsServerProtos.java:[line 15024]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bb636f5d5682c03b652d65e8ec3d7739' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' start='14815' end='15421' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 14815-15421]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='59ca06961ef8bce94bda51a92c3fbf32' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder' start='15214' end='15412' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 15214-15412]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder' start='15240' end='15242' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder' start='15240' end='15240' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='3' primary='true'><Message>At HdfsServerProtos.java:[line 15240]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e5d041c133f2cf2c3146bfc9908524b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' start='10779' end='11599' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 10779-11599]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' start='10880' end='10880' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 10880]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f1248fe35af2d6c62da00589d0886a81' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' start='10779' end='11599' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 10779-11599]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='33eaa5af044faa2435c38423919f3c57' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' start='13597' end='14778' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 13597-14778]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' start='13703' end='13703' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 13703]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1dd8d682c99b97afa665d33aa12bee8e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' start='13597' end='14778' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 13597-14778]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d0775ae60ce92e47c34db2efd1e08dc1' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6174' end='7650' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 6174-7650]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6295' end='6295' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 6295]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fa518c1795a4690baa2c615f6a33777' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' start='6174' end='7650' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 6174-7650]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='afc0c9b18273b31171064a394e032626' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' start='7768' end='9022' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 7768-9022]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' start='7889' end='7889' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 7889]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8bb3b7f8a152b29212005057ace6704b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' start='7768' end='9022' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 7768-9022]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='955fd136c4720a0304429db661de59b6' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto defines non-transient non-serializable instance field blockIndices_</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' start='7768' end='9022' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 7768-9022]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' signature='Lcom/google/protobuf/ByteString;' name='blockIndices_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto.blockIndices_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7c5d85756a6dc35fa90ebade71ef2b36' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' start='5232' end='6005' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 5232-6005]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' start='5320' end='5320' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 5320]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c46a712c363fdb049ae106c0ae875bf6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' start='5232' end='6005' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 5232-6005]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9fac1852e32ae4164982e5469976cba5' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' start='4539' end='5181' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 4539-5181]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' start='4626' end='4626' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 4626]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2fb986d665af76d4932182f0270348' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' start='4539' end='5181' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 4539-5181]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f867c4b136a8a0bd3a1e5c5835c08392' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder' start='4903' end='5172' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 4903-5172]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder' start='4929' end='4931' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder' start='4929' end='4929' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='3' primary='true'><Message>At HdfsServerProtos.java:[line 4929]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7bc2be9dfa44703089ca8feeb675455b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12615' end='13483' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 12615-13483]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12707' end='12707' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 12707]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5238c540628882df75de5365e8d9887a' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto.getClusterID() and org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.getClusterId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12615' end='13483' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 12615-13483]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' signature='()Ljava/lang/String;' name='getClusterID' primary='true'><SourceLine endBytecode='163' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12792' end='12802' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto.getClusterID()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='()Ljava/lang/String;' name='getClusterId'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='197' end='197' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.getClusterId()</Message></Method><SourceLine synthetic='true' endBytecode='163' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12792' end='12802' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'><Message>At HdfsServerProtos.java:[lines 12792-12802]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a5fd01e60843bd2447a03cba61141bcf' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' start='12615' end='13483' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 12615-13483]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d3d095b94fca889481c0416ca2173f25' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder' start='13072' end='13474' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 13072-13474]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder' start='13098' end='13100' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder' start='13098' end='13098' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='3' primary='true'><Message>At HdfsServerProtos.java:[line 13098]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='411f54db14f3f8657c5f99d6ab4eb2db' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11686' end='11686' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 11686]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4ea45640431d4147f50073093f82bbb8' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3a22a9c748ac1feb3e0edaeecc08e5b' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder' start='11839' end='11938' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11839-11938]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder' start='11865' end='11867' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder' start='11865' end='11865' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='3' primary='true'><Message>At HdfsServerProtos.java:[line 11865]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='677d5ec10d9a3660b122e4197e397140' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' start='11977' end='12518' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11977-12518]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' start='12062' end='12062' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java' startBytecode='7' primary='true'><Message>At HdfsServerProtos.java:[line 12062]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='790577b15bb92c718adcf6317870f964' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' start='11977' end='12518' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11977-12518]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>In HdfsServerProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9e3ddad95d6ea185e6e96b0ecd8bce98' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' start='37' end='579' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 37-579]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' start='122' end='122' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 122]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='20462ed21319caeb9f27dd783d57daac' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' start='37' end='579' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 37-579]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a720b9d88f1493ab0e0a12358134fc18' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' start='649' end='1435' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 649-1435]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' start='750' end='750' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 750]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c8a6b663ddc77e2c9fe27f7ef67941f2' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' start='649' end='1435' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 649-1435]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d75946a1ab36b69981c1f98362c89e48' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto in org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3032' end='3300' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 3032-3300]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='32' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3194' end='3211' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' start='37' end='579' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 37-579]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='54' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='55' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3201' end='3201' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='55' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 3201]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d344833490b3c8f87a5db7eacbebe05b' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto in org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3032' end='3300' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 3032-3300]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='32' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3194' end='3211' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1531' end='2448' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 1531-2448]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='69' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService' start='3206' end='3206' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='70' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 3206]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e460dfc46eff4108e7db30c305733d8c' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto in org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3087' end='3145' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 3087-3145]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='30' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3098' end='3109' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto' start='37' end='579' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 37-579]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='57' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='58' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3105' end='3105' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='58' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 3105]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1dd778bf2168990cdeb326161edaef93' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto in org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3087' end='3145' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 3087-3145]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='30' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3098' end='3109' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1531' end='2448' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 1531-2448]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='72' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2' start='3107' end='3107' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='73' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 3107]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c5c52eb8f65a7d2bd1bbfbbadf841bfe' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1531' end='2448' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 1531-2448]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1631' end='1631' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 1631]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='36fcc0b5a21658029a2b78da750671dd' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto.getRecoveryId() and org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica.getRecoveryID()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1531' end='2448' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 1531-2448]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' signature='()J' name='getRecoveryId' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1702' end='1702' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto.getRecoveryId()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica' start='45' end='119' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedProvidedReplica.java' sourcefile='FinalizedProvidedReplica.java'><Message>At FinalizedProvidedReplica.java:[lines 45-119]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica' signature='()J' name='getRecoveryID'><SourceLine endBytecode='76' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica' start='107' end='107' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedProvidedReplica.java' sourcefile='FinalizedProvidedReplica.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica.getRecoveryID()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1702' end='1702' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'><Message>At InterDatanodeProtocolProtos.java:[line 1702]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a6efb7d4ccf4a66ebdde21b70da7ea97' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' start='1531' end='2448' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 1531-2448]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c0dc86b36f94cedab6baebbb800e8410' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' start='2491' end='3015' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 2491-3015]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' start='2568' end='2568' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 2568]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7744a3ba23ecd6e5e448e69143526434' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' start='2491' end='3015' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 2491-3015]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>In InterDatanodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3945bccb98d2c9ef227812a45be313b9' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder' start='2794' end='3006' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java'><Message>At InterDatanodeProtocolProtos.java:[lines 2794-3006]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder' start='2820' end='2822' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder' start='2820' end='2820' sourcepath='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' sourcefile='InterDatanodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At InterDatanodeProtocolProtos.java:[line 2820]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a211dc68a3aed9ea7376f7bbe812ce28' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' start='3444' end='4332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3444-4332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' start='3539' end='3539' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 3539]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b21bd026934fa357d12828af0d223f5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' start='3444' end='4332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3444-4332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='410e575601a7574a50a7e34faf75a3aa' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' start='4380' end='4960' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4380-4960]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' start='4467' end='4467' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 4467]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f457eda950b0d80ca4337845f0bf1b0f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' start='4380' end='4960' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4380-4960]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5a5beac77ce11bd68abffb63bb274d1' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder' start='4722' end='4951' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4722-4951]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder' start='4748' end='4750' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder' start='4748' end='4748' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At JournalProtocolProtos.java:[line 4748]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f408824b98102c98b53673c00941b874' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='85' end='825' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 85-825]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='172' end='172' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 172]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='af3e4f20bcd222d22bf0fb71cd217c2c' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.getNamespaceID() and org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.getNamespaceId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='85' end='825' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 85-825]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' signature='()I' name='getNamespaceID' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='288' end='288' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.getNamespaceID()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1842' end='2767' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 1842-2767]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' signature='()I' name='getNamespaceId'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1983' end='1983' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.getNamespaceId()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='288' end='288' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'><Message>At JournalProtocolProtos.java:[line 288]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bb67ac52256947a1368dad90594d5c63' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.hasNamespaceID() and org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.hasNamespaceId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='85' end='825' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 85-825]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' signature='()Z' name='hasNamespaceID' primary='true'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='278' end='278' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.hasNamespaceID()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1842' end='2767' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 1842-2767]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' signature='()Z' name='hasNamespaceId'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1977' end='1977' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.hasNamespaceId()</Message></Method><SourceLine synthetic='true' endBytecode='68' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='278' end='278' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'><Message>At JournalProtocolProtos.java:[line 278]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7991037c85048d3300601d0b7abc4ca7' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' start='85' end='825' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 85-825]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b4b1572c3ea6ac18a3f3c8f9e75289ac' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder' start='484' end='816' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 484-816]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder' start='510' end='512' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder' start='510' end='510' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At JournalProtocolProtos.java:[line 510]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9adec66896f04adc8b6f32af63472f7e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='4978' end='5316' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4978-5316]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5186' end='5208' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$FenceRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' start='3444' end='4332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3444-4332]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='84' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='85' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5203' end='5203' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='85' primary='true'><Message>At JournalProtocolProtos.java:[line 5203]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='872099821868f965a4cd4eacfdea1e2f' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='4978' end='5316' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4978-5316]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5186' end='5208' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$JournalRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='899' end='1799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 899-1799]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='54' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='55' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5193' end='5193' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='55' primary='true'><Message>At JournalProtocolProtos.java:[line 5193]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c63cbf2014bc764ad993d62c2d64b207' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='4978' end='5316' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 4978-5316]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5186' end='5208' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$StartLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' start='2218' end='3008' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 2218-3008]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='69' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService' start='5198' end='5198' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='70' primary='true'><Message>At JournalProtocolProtos.java:[line 5198]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='840f10d674075671ef3523470499fba1' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5057' end='5121' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 5057-5121]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5068' end='5081' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$FenceRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto' start='3444' end='4332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3444-4332]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='87' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5079' end='5079' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='88' primary='true'><Message>At JournalProtocolProtos.java:[line 5079]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='45fd3efa14c69d9bcb0075bd2147c0b6' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5057' end='5121' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 5057-5121]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5068' end='5081' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$JournalRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='899' end='1799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 899-1799]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='57' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='58' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5075' end='5075' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='58' primary='true'><Message>At JournalProtocolProtos.java:[line 5075]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='891738c562357bc33e234037d1b4f6f7' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5057' end='5121' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 5057-5121]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5068' end='5081' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos$StartLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' start='2218' end='3008' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 2218-3008]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='72' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2' start='5077' end='5077' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='73' primary='true'><Message>At JournalProtocolProtos.java:[line 5077]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5518c531314c0fef0c01aee24f3b6b02' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='899' end='1799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 899-1799]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='1004' end='1004' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 1004]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2dabb1781c3c84ecb0cd8589686fc6b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='899' end='1799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 899-1799]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ed54c6c561344cb08e1151abdbf8692a' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto defines non-transient non-serializable instance field records_</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' start='899' end='1799' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 899-1799]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/ByteString;' name='records_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto.records_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c775984169a41cc110ddf8010c94edfc' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' start='1815' end='2147' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 1815-2147]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' start='1886' end='1886' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 1886]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='636819e5e7d2e78c4ab0a58cceba1406' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' start='1815' end='2147' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 1815-2147]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f6af0d3813c3e2098776339f1e839b2d' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder' start='2039' end='2138' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 2039-2138]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder' start='2065' end='2067' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder' start='2065' end='2065' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At JournalProtocolProtos.java:[line 2065]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4e8c858f378a8b236df4a0b747ec31d0' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' start='2218' end='3008' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 2218-3008]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' start='2313' end='2313' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 2313]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='59d071daeedd5b37081137ae424f3e74' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' start='2218' end='3008' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 2218-3008]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a82a6d045fc09b4e0a26c2fb71b77c10' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' start='3024' end='3356' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3024-3356]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' start='3095' end='3095' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At JournalProtocolProtos.java:[line 3095]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fc7fe6b0e47d0f928b1ca36f3c34c0a9' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' start='3024' end='3356' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3024-3356]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>In JournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='40ecf320e68e3d8a92a329209f4e2fd4' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='3248' end='3347' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java'><Message>At JournalProtocolProtos.java:[lines 3248-3347]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='3274' end='3276' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='3274' end='3274' sourcepath='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' sourcefile='JournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At JournalProtocolProtos.java:[line 3274]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3ea039c958883de581c3178bcd6bdfd6' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' start='8861' end='9656' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8861-9656]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' start='8959' end='8959' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 8959]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='13ca58274d438639f218eda54ec0ba4a' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' start='8861' end='9656' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8861-9656]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d81a1228a229188723154f56d50c2d8d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' start='9672' end='10004' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 9672-10004]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' start='9743' end='9743' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 9743]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e6b8c1d08185e3c990d34277d495a329' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' start='9672' end='10004' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 9672-10004]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a112333f2bfc74e3a0caad817f8808df' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder' start='9896' end='9995' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 9896-9995]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder' start='9922' end='9924' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder' start='9922' end='9922' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 9922]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bd97aba00a113d78f28f2d54a3986b8a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' start='5087' end='5984' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 5087-5984]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' start='5182' end='5182' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 5182]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4daea200fc0382173780e12e35eb0483' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' start='5087' end='5984' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 5087-5984]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a4d4c4fc64e325d221edf8c4d0cd01fe' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' start='6000' end='6332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6000-6332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' start='6071' end='6071' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 6071]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='384c2f46431b5ed584fcf0f1173fc6de' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' start='6000' end='6332' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6000-6332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a613f3f3d2547d87be50490fb49c15df' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder' start='6224' end='6323' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6224-6323]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder' start='6250' end='6252' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder' start='6250' end='6250' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 6250]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe4a6c8b16ff5ccd894b4b7520df0f38' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' start='1558' end='1890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1558-1890]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' start='1629' end='1629' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 1629]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='153c6969a8cd6956565e1746fa7aa77b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' start='1558' end='1890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1558-1890]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8c4ed25448c520a91a070fac1c8d4620' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder' start='1782' end='1881' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1782-1881]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder' start='1808' end='1810' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder' start='1808' end='1808' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 1808]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f5aa0ee8e1213ab1cdfb61a8ff0b721c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' start='1920' end='2457' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1920-2457]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' start='2005' end='2005' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 2005]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='da3fdaf94fe93624bc7f46e5086406c3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' start='1920' end='2457' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1920-2457]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c08d05ffc080343c5d8a5b97d00e2700' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' start='91' end='911' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 91-911]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' start='186' end='186' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 186]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='79c68ec1c09f11772615d3f4b919c9b3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' start='91' end='911' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 91-911]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f65b2442dfefa944eeb724a9cae8c039' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' start='953' end='1542' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 953-1542]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' start='1038' end='1038' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 1038]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='753feb04463d23df102fdc75b9ba92a4' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' start='953' end='1542' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 953-1542]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2187c76b831a4756bfc3ea14d83bb96f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' start='10038' end='10488' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10038-10488]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' start='10115' end='10115' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 10115]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8d61fd8a5fbac974a0040d416ef9d1c8' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' start='10038' end='10488' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10038-10488]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7f01b88f748be1c1deb58485431d93ed' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder' start='10314' end='10479' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10314-10479]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder' start='10340' end='10342' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder' start='10340' end='10340' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 10340]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='636dcf7887c3df719657065337cc6c6' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' start='10519' end='11061' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10519-11061]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' start='10604' end='10604' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 10604]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='424f3fceb9a76a5b090b8be5ce4a78e5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' start='10519' end='11061' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10519-11061]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b9af383334c9c4bd610d1858ae82f88d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' start='4224' end='4556' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4224-4556]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' start='4295' end='4295' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 4295]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6a682c0622bd1758dd9b69b7a44a966' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' start='4224' end='4556' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4224-4556]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1edcbdd299b1bb49ee1953754d1bf17c' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder' start='4448' end='4547' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4448-4547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder' start='4474' end='4476' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder' start='4474' end='4474' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 4474]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='94198fb4621ba13577cb9f5856034ade' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4577' end='4998' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4577-4998]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4654' end='4654' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 4654]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e007aa43cc31d0e4d91d84786751c604' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' start='4577' end='4998' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4577-4998]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a2bce06cc2ae9ac2cc676bdcd79ef6dc' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder' start='4840' end='4989' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4840-4989]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder' start='4866' end='4868' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder' start='4866' end='4866' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 4866]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='33ba034345c9a1b2c3429325283530e4' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' start='2473' end='2805' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2473-2805]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' start='2544' end='2544' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 2544]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='70ea8445c73cfbb07538e98f08b60415' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' start='2473' end='2805' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2473-2805]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='23aa40c9d24a385d9cf201a71fac344e' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder' start='2697' end='2796' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2697-2796]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder' start='2723' end='2725' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder' start='2723' end='2723' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 2723]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='62cf5046f965c04348211720bf0f3654' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' start='2839' end='3289' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2839-3289]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' start='2916' end='2916' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 2916]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='92d5366154713c5eef40b0e93f1d2ffd' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' start='2839' end='3289' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2839-3289]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='20c83a9f7192e2d24732b9513608ead7' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder' start='3115' end='3280' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3115-3280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder' start='3141' end='3143' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder' start='3141' end='3141' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 3141]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cca7ef04a211fa6f0d283ade00d78ae7' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' start='11867' end='12199' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11867-12199]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' start='11938' end='11938' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 11938]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2ff7080375e12a63a9e70d74040e7d23' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' start='11867' end='12199' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11867-12199]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7050103f740e814ce4a7b8b93049aae2' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder' start='12091' end='12190' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12091-12190]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder' start='12117' end='12119' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder' start='12117' end='12117' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12117]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4c4c6e6871e6d88a30a468387a0080ea' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' start='12220' end='12641' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12220-12641]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' start='12297' end='12297' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12297]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='71bcba65b0e8ea3123c7955a840c3be6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' start='12220' end='12641' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12220-12641]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='74292e55247c8e98fdcac6ae8528e424' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder' start='12483' end='12632' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12483-12632]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder' start='12509' end='12511' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder' start='12509' end='12509' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12509]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ebcb7cce8a91dfb197b9b915d6ca8bcc' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' start='11077' end='11409' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11077-11409]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' start='11148' end='11148' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 11148]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='250dcbfebf8693adb04ddb978a55409d' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' start='11077' end='11409' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11077-11409]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2e80beb71620a141f618ffa84a9f0d65' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder' start='11301' end='11400' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11301-11400]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder' start='11327' end='11329' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder' start='11327' end='11327' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 11327]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c167a465cff3e6fcdd745d32827a0e02' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' start='11430' end='11851' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11430-11851]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' start='11507' end='11507' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 11507]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f7392c7d7f5d940a0d43577b9ac235d5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' start='11430' end='11851' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11430-11851]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='519144fa83ff2e74b34109b4401fc15d' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder' start='11693' end='11842' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11693-11842]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder' start='11719' end='11721' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder' start='11719' end='11719' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 11719]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bbb61270de20115d65a582bdcc9b2e6b' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos$VersionRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='169' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='170' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13299' end='13299' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='170' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13299]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a1273e167f2324a8f4d5023c8fda8b1b' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$EndCheckpointRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' start='8861' end='9656' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8861-9656]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='229' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='230' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13319' end='13319' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='230' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13319]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a293cd4990a95e046100a4912f52d2db' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$ErrorReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' start='5087' end='5984' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 5087-5984]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='184' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='185' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13304' end='13304' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='185' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13304]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8ab24feea6cdd2563559f286c886bbe5' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetBlockKeysRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' start='1558' end='1890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1558-1890]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='109' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='110' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13279' end='13279' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='110' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13279]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='da2d37f4665ba608dd7625ee0dd7f761' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetBlocksRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' start='91' end='911' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 91-911]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='94' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='95' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13274' end='13274' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='95' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13274]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2ab5552f36e7f20396bca527f9440c3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' start='10038' end='10488' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10038-10488]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='244' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='245' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13324' end='13324' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='245' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13324]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='31ebd49ec66dbe960c0f5a7ae68b8fb9' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' start='4224' end='4556' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4224-4556]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='139' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='140' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13289' end='13289' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='140' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13289]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='25be998366bd6bd4819f73d1687ff389' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetTransactionIdRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' start='2473' end='2805' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2473-2805]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='124' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='125' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13284' end='13284' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='125' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13284]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bf5ffd27f3140d467936245fa2c39feb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$IsRollingUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' start='11867' end='12199' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11867-12199]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='274' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='275' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13334' end='13334' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='275' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13334]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a2a799927e204076e2c31d2222c114bd' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' start='11077' end='11409' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11077-11409]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='259' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='260' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13329' end='13329' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='260' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13329]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e39702c774a23b8a6cc924863cb2c198' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$RegisterRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' start='6374' end='6963' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6374-6963]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='199' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='200' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13309' end='13309' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='200' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13309]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bada2edc48fa4d9fd0585c0d32a9a89e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$RollEditLogRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' start='3305' end='3637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3305-3637]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='154' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='155' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13294' end='13294' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='155' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13294]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2d38d3a71563e9ae34155a62bf31d080' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='12659' end='13637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12659-13637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13267' end='13339' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$StartCheckpointRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' start='7639' end='8229' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7639-8229]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='214' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='215' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService' start='13314' end='13314' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='215' primary='true'><Message>At NamenodeProtocolProtos.java:[line 13314]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='db076a60931b5afdb4f256e6727fd446' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos$VersionRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto' start='11615' end='11947' sourcepath='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' sourcefile='HdfsServerProtos.java'><Message>At HdfsServerProtos.java:[lines 11615-11947]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='172' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='173' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12976' end='12976' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='173' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12976]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='28118ea313ab2f24182c665502917d91' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$EndCheckpointRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto' start='8861' end='9656' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8861-9656]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='232' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='233' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12984' end='12984' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='233' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12984]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d94d2bc60c19f312cc85c4c0b757dbcb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$ErrorReportRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto' start='5087' end='5984' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 5087-5984]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='187' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='188' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12978' end='12978' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='188' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12978]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='107dbcf7d6af8c8ab9dbd2e141fb84e4' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetBlockKeysRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto' start='1558' end='1890' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 1558-1890]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='112' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='113' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12968' end='12968' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='113' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12968]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7491ce38535456c0025a0b6669f25230' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetBlocksRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto' start='91' end='911' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 91-911]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='97' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='98' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12966' end='12966' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='98' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12966]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2090705f7ffa28f6fb099d12d9d4fcd3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto' start='10038' end='10488' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 10038-10488]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='247' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='248' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12986' end='12986' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='248' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12986]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='82c17d69248a761a9161c8016032e4bb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto' start='4224' end='4556' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 4224-4556]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='142' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='143' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12972' end='12972' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='143' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12972]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ba115b158dfb6b5441ffc2d17e2a8acb' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$GetTransactionIdRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto' start='2473' end='2805' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 2473-2805]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='127' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='128' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12970' end='12970' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='128' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12970]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e20bf95cc13c458daff891606c4243e8' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$IsRollingUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto' start='11867' end='12199' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11867-12199]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='277' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='278' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12990' end='12990' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='278' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12990]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ca95555dc5102540cbebe5906eec638f' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto' start='11077' end='11409' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 11077-11409]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='262' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='263' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12988' end='12988' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='263' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12988]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ff0e806df8e2199e0b038efc6c884448' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$RegisterRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' start='6374' end='6963' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6374-6963]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='202' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='203' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12980' end='12980' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='203' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12980]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d3d1b98e5ec9593e50df1bd9f846afa9' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$RollEditLogRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' start='3305' end='3637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3305-3637]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='157' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='158' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12974' end='12974' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='158' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12974]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ac2d83f4f67fae4af618e2038ecf8344' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12948' end='13072' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 12948-13072]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12959' end='12992' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos$StartCheckpointRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' start='7639' end='8229' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7639-8229]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='217' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='218' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2' start='12982' end='12982' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='218' primary='true'><Message>At NamenodeProtocolProtos.java:[line 12982]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5addc6d8bc8b8ebc2ea70e3e6fc7a154' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' start='6374' end='6963' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6374-6963]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' start='6459' end='6459' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 6459]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ddcaf5f62e6a7b89be04a740f8b31275' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' start='6374' end='6963' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 6374-6963]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b7add19f88861999949c02962b59440' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' start='7006' end='7596' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7006-7596]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' start='7091' end='7091' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 7091]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='70805cc12b8463a94f71048422a90924' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' start='7006' end='7596' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7006-7596]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9c88fa4b0a214073b3373a2b34b0d464' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' start='3305' end='3637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3305-3637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' start='3376' end='3376' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 3376]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b88ed20f63ae498314ff5754e9fd50f8' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' start='3305' end='3637' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3305-3637]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='10db69aa2612ebb11857e50c476e1cda' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder' start='3529' end='3628' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3529-3628]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder' start='3555' end='3557' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder' start='3555' end='3555' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='3' primary='true'><Message>At NamenodeProtocolProtos.java:[line 3555]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2023867acd338993b7ba6a6420bbd66b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' start='3667' end='4208' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3667-4208]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' start='3752' end='3752' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 3752]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e498b96a52828db53855678d76b9877b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' start='3667' end='4208' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 3667-4208]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d998f51aeb05e8e87dde63023dd97a3b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' start='7639' end='8229' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7639-8229]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' start='7724' end='7724' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 7724]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='324c65a5f1b954d0ec2c2804614e0326' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' start='7639' end='8229' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 7639-8229]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fb46525f8904e1368807465d5f9182f3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' start='8260' end='8802' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8260-8802]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' start='8345' end='8345' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java' startBytecode='7' primary='true'><Message>At NamenodeProtocolProtos.java:[line 8345]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='835c3c76b4015322fc6f63aa69e2b3a7' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' start='8260' end='8802' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>At NamenodeProtocolProtos.java:[lines 8260-8802]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto' sourcepath='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' sourcefile='NamenodeProtocolProtos.java'><Message>In NamenodeProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='db09ba77d853c1a8040ee4bc145f80dd' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID() and org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId()</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB' start='71' end='263' sourcepath='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolTranslatorPB.java' sourcefile='NamenodeProtocolTranslatorPB.java'><Message>At NamenodeProtocolTranslatorPB.java:[lines 71-263]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB' signature='()J' name='getTransactionID' primary='true'><SourceLine endBytecode='112' classname='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB' start='130' end='133' sourcepath='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolTranslatorPB.java' sourcefile='NamenodeProtocolTranslatorPB.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' signature='()J' name='getTransactionId'><SourceLine endBytecode='80' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='238' end='239' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getTransactionId()</Message></Method><SourceLine synthetic='true' endBytecode='112' classname='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB' start='130' end='133' sourcepath='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolTranslatorPB.java' sourcefile='NamenodeProtocolTranslatorPB.java' startBytecode='0'><Message>At NamenodeProtocolTranslatorPB.java:[lines 130-133]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a6369b8f764136d5ddc8763e9e1d549e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.BalancerBandwidthCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BalancerBandwidthCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BalancerBandwidthCommand' start='45' end='64' sourcepath='org/apache/hadoop/hdfs/server/protocol/BalancerBandwidthCommand.java' sourcefile='BalancerBandwidthCommand.java'><Message>At BalancerBandwidthCommand.java:[lines 45-64]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BalancerBandwidthCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='91' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='92' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='568' end='568' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='92' primary='true'><Message>At PBHelper.java:[line 568]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e1e8cf8ff878192b446dcd9182ba3e44' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.BlockCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='192' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='193' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='593' end='593' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='193' primary='true'><Message>At PBHelper.java:[line 593]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b6552c511f16764da18d2c940adfb90d' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand' start='58' end='146' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 58-146]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='234' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='235' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='602' end='602' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='235' primary='true'><Message>At PBHelper.java:[line 602]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a979d7069747fd5ab8d2dcefc7fd0793' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.BlockIdCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockIdCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='37' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java'><Message>At BlockIdCommand.java:[lines 37-47]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BlockIdCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='213' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='214' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='598' end='598' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='214' primary='true'><Message>At PBHelper.java:[line 598]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ea1b8aea1e7430d21a680970fb351bee' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand' start='132' end='168' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java'><Message>At BlockRecoveryCommand.java:[lines 132-168]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='133' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='134' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='578' end='578' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='134' primary='true'><Message>At PBHelper.java:[line 578]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8834038459560f8658eed9d1e5cf7180' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.FinalizeCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/FinalizeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.FinalizeCommand' start='31' end='40' sourcepath='org/apache/hadoop/hdfs/server/protocol/FinalizeCommand.java' sourcefile='FinalizeCommand.java'><Message>At FinalizeCommand.java:[lines 31-40]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.FinalizeCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='154' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='155' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='583' end='583' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='155' primary='true'><Message>At PBHelper.java:[line 583]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='532d92e9f66c64fbe755a9891d77021c' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.DatanodeCommand to org.apache.hadoop.hdfs.server.protocol.KeyUpdateCommand in org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</LongMessage><Class classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='135' end='1125' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java'><Message>At PBHelper.java:[lines 135-1125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.protocolPB.PBHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;)Lorg/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos$DatanodeCommandProto;' name='convert' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='561' end='610' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(DatanodeCommand)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand' start='32' end='33' sourcepath='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' sourcefile='DatanodeCommand.java'><Message>At DatanodeCommand.java:[lines 32-33]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.DatanodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/KeyUpdateCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.KeyUpdateCommand' start='30' end='39' sourcepath='org/apache/hadoop/hdfs/server/protocol/KeyUpdateCommand.java' sourcefile='KeyUpdateCommand.java'><Message>At KeyUpdateCommand.java:[lines 30-39]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.KeyUpdateCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='112' name='datanodeCommand' register='0'><Message>Value loaded from datanodeCommand</Message></LocalVariable><SourceLine endBytecode='113' classname='org.apache.hadoop.hdfs.protocolPB.PBHelper' start='574' end='574' sourcepath='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' sourcefile='PBHelper.java' startBytecode='113' primary='true'><Message>At PBHelper.java:[line 574]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcd71225b8a8f264e3dcf0392888952f' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$3 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$3'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$3' start='328' end='331' sourcepath='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java' sourcefile='IPCLoggerChannel.java'><Message>At IPCLoggerChannel.java:[lines 328-331]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$3</Message></Class><Class classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel' start='76' end='753' sourcepath='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java' sourcefile='IPCLoggerChannel.java'><Message>At IPCLoggerChannel.java:[lines 76-753]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel' signature='()V' name='waitForAllPendingCalls' primary='true'><SourceLine endBytecode='129' classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel' start='328' end='337' sourcepath='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java' sourcefile='IPCLoggerChannel.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.waitForAllPendingCalls()</Message></Method><SourceLine endBytecode='9' classname='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel' start='328' end='328' sourcepath='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java' sourcefile='IPCLoggerChannel.java' startBytecode='9' primary='true'><Message>At IPCLoggerChannel.java:[line 328]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7748cddff76e3aad635bd109c8c23ffe' rank='20' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_COMPARATOR_SHOULD_BE_SERIALIZABLE' instanceOccurrenceMax='0'><ShortMessage>Comparator doesn't implement Serializable</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator implements Comparator but not Serializable</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator' start='34' end='89' sourcepath='org/apache/hadoop/hdfs/qjournal/client/SegmentRecoveryComparator.java' sourcefile='SegmentRecoveryComparator.java'><Message>At SegmentRecoveryComparator.java:[lines 34-89]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator' start='34' end='89' sourcepath='org/apache/hadoop/hdfs/qjournal/client/SegmentRecoveryComparator.java' sourcefile='SegmentRecoveryComparator.java'><Message>At SegmentRecoveryComparator.java:[lines 34-89]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2bc3432e0d1abd5ad159ea1ec1c39466' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService' start='16' end='210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java'><Message>At InterQJournalProtocolProtos.java:[lines 16-210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService' start='128' end='140' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='46' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='47' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService' start='135' end='135' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java' startBytecode='47' primary='true'><Message>At InterQJournalProtocolProtos.java:[line 135]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='454e4158ee150f900cfd0b950b2bcf64' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2' start='45' end='97' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java'><Message>At InterQJournalProtocolProtos.java:[lines 45-97]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='23' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2' start='56' end='65' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='49' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='50' classname='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2' start='63' end='63' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' sourcefile='InterQJournalProtocolProtos.java' startBytecode='50' primary='true'><Message>At InterQJournalProtocolProtos.java:[line 63]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='11e17f9589e51baabd6a513dc5d71a22' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' start='25704' end='26690' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 25704-26690]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' start='25807' end='25807' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 25807]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcb970499cb7282c8ecca62151610c8a' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' start='25704' end='26690' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 25704-26690]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7faae1940f82ba090dfe6e1e6a7f2884' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' start='26701' end='27028' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 26701-27028]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' start='26772' end='26772' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 26772]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9e8ac77e764ee32bbf72575cc109809e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' start='26701' end='27028' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 26701-27028]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3d00c39522fe90e8b122bf8d03a3a755' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder' start='26920' end='27019' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 26920-27019]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder' start='26946' end='26948' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder' start='26946' end='26946' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 26946]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8740d691b259af8246d57756bb6c350a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14216' end='14216' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 14216]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e3998d5c562aa494e620a42f00601653' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.getNameServiceIdBytes() and org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.getNameserviceIdBytes()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' signature='()Lcom/google/protobuf/ByteString;' name='getNameServiceIdBytes' primary='true'><SourceLine endBytecode='131' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14345' end='14353' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.getNameServiceIdBytes()</Message></Method><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' signature='()Lcom/google/protobuf/ByteString;' name='getNameserviceIdBytes'><SourceLine endBytecode='131' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15926' end='15934' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.getNameserviceIdBytes()</Message></Method><SourceLine synthetic='true' endBytecode='131' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14345' end='14353' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'><Message>At QJournalProtocolProtos.java:[lines 14345-14353]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='241a6b51ce92d10ba45326cb12fe619' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.hasNameServiceId() and org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.hasNameserviceId()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' signature='()Z' name='hasNameServiceId' primary='true'><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14321' end='14321' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.hasNameServiceId()</Message></Method><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' signature='()Z' name='hasNameserviceId'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15902' end='15902' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.hasNameserviceId()</Message></Method><SourceLine synthetic='true' endBytecode='70' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14321' end='14321' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'><Message>At QJournalProtocolProtos.java:[line 14321]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='698f66db9101e320fdcdb5eb5c3fc58f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3eb17bebfdf61f9911d2006acacf8cc' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' start='15301' end='15722' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15301-15722]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' start='15378' end='15378' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 15378]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ce12794b6613c6f44cf4924aca9a774a' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' start='15301' end='15722' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15301-15722]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2c03a87ed36167b5f68cac194b1aa69e' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder' start='15564' end='15713' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15564-15713]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder' start='15590' end='15592' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder' start='15590' end='15590' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 15590]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4516d096358f824d1a8a20ca9e79c4db' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' start='16851' end='17630' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16851-17630]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' start='16946' end='16946' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 16946]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7f52c586846782d82f0af230c097196c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' start='16851' end='17630' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16851-17630]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='81f4bb73b00acc4113b506a3b7183feb' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' start='17641' end='17968' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 17641-17968]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' start='17712' end='17712' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 17712]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2bf84ccba69ede806d560537c13f77cc' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' start='17641' end='17968' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 17641-17968]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6be16af1c03a45c917193f2f4cd27b7' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder' start='17860' end='17959' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 17860-17959]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder' start='17886' end='17888' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder' start='17886' end='17886' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 17886]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8076a859d697e06e7844e151a0102f05' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' start='12983' end='13674' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12983-13674]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' start='13073' end='13073' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 13073]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c542bf2f567b03b418d1648b70a8b58' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' start='12983' end='13674' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12983-13674]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='226d0d7f93e23a55ea0544226a08aaf4' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' start='13685' end='14012' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 13685-14012]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' start='13756' end='13756' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 13756]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f2081a4b22e9c283df410f5e2f731284' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' start='13685' end='14012' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 13685-14012]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a4087a818d1b3da7fbf51227714cbb5a' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder' start='13904' end='14003' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 13904-14003]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder' start='13930' end='13932' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder' start='13930' end='13930' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 13930]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fdb7d76ba842253bf776ec85821052aa' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' start='10933' end='11474' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10933-11474]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' start='11018' end='11018' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 11018]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='98cd8c357756033bfb1843f5b79f52a3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' start='10933' end='11474' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10933-11474]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d13bb79d8286429efacc1957b2523dbc' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' start='11485' end='11812' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11485-11812]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' start='11556' end='11556' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 11556]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dd7100f98edc460bd15df02d59fef043' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' start='11485' end='11812' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11485-11812]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ac4509bf14381665a52b1276da92e975' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder' start='11704' end='11803' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11704-11803]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder' start='11730' end='11732' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder' start='11730' end='11730' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 11730]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fb525e37a8474e4705a986cd9dc6c09b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15857' end='15857' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 15857]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bac15f5288b9a61940d9c4fa5c0dba3c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2f9615d3f1473dbf0057dfbd60a0903c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' start='16469' end='16796' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16469-16796]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' start='16540' end='16540' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 16540]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bbb50172d7bf937622898819baa08b58' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' start='16469' end='16796' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16469-16796]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='eb4c8ade4cc9b05cc36a7ebc451d064f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder' start='16688' end='16787' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16688-16787]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder' start='16714' end='16716' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder' start='16714' end='16714' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 16714]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='62fcc1d8a42cd1a0677fdc8c0ba1b763' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' start='11856' end='12600' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11856-12600]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' start='11954' end='11954' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 11954]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='967e229d08c88ed2bcf3bea4a06d2a9f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' start='11856' end='12600' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11856-12600]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7cf114f8440f3dd713806c23aa84ae71' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' start='12611' end='12938' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12611-12938]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' start='12682' end='12682' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 12682]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c03fef771f41bc968958b447055476a3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' start='12611' end='12938' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12611-12938]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='49fb1f2fc12713ef727ec8197827bbc5' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder' start='12830' end='12929' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12830-12929]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder' start='12856' end='12858' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder' start='12856' end='12856' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 12856]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='214325b51776d5f35bc182782867a6d8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' start='6485' end='7202' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6485-7202]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' start='6580' end='6580' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 6580]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e1593ffeadf12a9b69911178cac9394e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' start='6485' end='7202' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6485-7202]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='46e4213f2d25a5343a63bef2bfcf363b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' start='7213' end='7540' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7213-7540]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' start='7284' end='7284' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 7284]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b854c8df2cdc31cfc07926aa2ad782fc' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' start='7213' end='7540' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7213-7540]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b889501a5c1e486cabb1ef736ade2777' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder' start='7432' end='7531' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7432-7531]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder' start='7458' end='7460' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder' start='7458' end='7458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 7458]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5a0603da75deb6b4f99d348ea2a4391e' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' start='19500' end='20394' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 19500-20394]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' start='19603' end='19603' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 19603]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cfb489f491178266056b81a695956e13' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' start='19500' end='20394' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 19500-20394]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1570af8f6de12179b23f1becdae74f6d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' start='20405' end='20732' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20405-20732]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' start='20476' end='20476' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 20476]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fa0ca021f221ae3edda47af42e91d9b6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' start='20405' end='20732' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20405-20732]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e6ae11da093b1cded15f427912eb41f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder' start='20624' end='20723' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20624-20723]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder' start='20650' end='20652' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder' start='20650' end='20650' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 20650]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='423d29dd0cc8518e5eaf05a9eae2e2af' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22400' end='22400' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 22400]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ba31bafa6c262c7ee24e359c1e2f147a' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ea9c140f279a4ac42e628a13c0b45cfc' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' start='23271' end='24069' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 23271-24069]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' start='23366' end='23366' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 23366]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e43da8eee7e638b612429258a5df91ae' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' start='23271' end='24069' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 23271-24069]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='791b540118061f8c70912844528ad63c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' start='9770' end='10461' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9770-10461]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' start='9860' end='9860' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 9860]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2eabc55a4155b66d1014194a62ede7b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' start='9770' end='10461' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9770-10461]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='767a62b85ade880134216617ea38d29f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' start='10482' end='10903' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10482-10903]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' start='10559' end='10559' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 10559]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e428b102ed3497638f498c8adde2bc7e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' start='10482' end='10903' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10482-10903]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2b7049496c2c8d5884238fcf9bff8fd5' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder' start='10745' end='10894' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10745-10894]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder' start='10771' end='10773' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder' start='10771' end='10771' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 10771]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='465f5fc45f96b9cd54138ef3e68e0d23' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' start='18013' end='18704' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18013-18704]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' start='18103' end='18103' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 18103]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='885a5459bedbc1412226b40dae3ec28c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' start='18013' end='18704' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18013-18704]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c8b7eebb15c7e8a58664029e05eb9da3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' start='18758' end='19441' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18758-19441]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' start='18845' end='18845' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 18845]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b2f9925eaa13fe00526790b46bac0d6b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' start='18758' end='19441' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18758-19441]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='197d1ed124db6241ef54b6d9b67ecf69' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder' start='19136' end='19432' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 19136-19432]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder' start='19162' end='19164' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder' start='19162' end='19162' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 19162]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1a50025621105b022bcb8d61e50ffdb0' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' start='4392' end='4928' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4392-4928]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' start='4477' end='4477' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 4477]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a755ae3cd29ee4d9f65ae5df8a614769' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' start='4392' end='4928' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4392-4928]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5432d4760585b46f545782e73ba5753d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' start='4943' end='5274' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4943-5274]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' start='5014' end='5014' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 5014]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe7aed2645dfaf4a8b909a95e1cd02eb' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' start='4943' end='5274' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4943-5274]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8da94ca2b28deb9676f5a20b9096303' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder' start='5166' end='5265' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 5166-5265]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder' start='5192' end='5194' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder' start='5192' end='5192' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 5192]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='65f479d40d5579a88b83a405087e05d3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' start='8592' end='9283' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8592-9283]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' start='8682' end='8682' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 8682]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='934cffca2809bcf90e86ba6031d188f0' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' start='8592' end='9283' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8592-9283]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7b981aea424efe9fad1d161f1b7384f8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' start='9304' end='9725' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9304-9725]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' start='9381' end='9381' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 9381]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='24491dd68d60d3b86c8c8abf407a78f9' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' start='9304' end='9725' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9304-9725]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='526071357beb862581fabbe8ed6dff55' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder' start='9567' end='9716' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9567-9716]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder' start='9593' end='9595' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder' start='9593' end='9593' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 9593]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f03e5ce4435b51230f22a5f0800cc23a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' start='32' end='523' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 32-523]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' start='109' end='109' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 109]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='186801970c63b20a434599005bec2d09' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' start='32' end='523' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 32-523]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f0f755ef370d413e35cb30d27ff92c0f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder' start='322' end='514' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 322-514]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder' start='348' end='350' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder' start='348' end='348' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 348]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d530d297034b4ea401b8f18ec5da55fd' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='2988' end='4029' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2988-4029]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='3098' end='3098' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 3098]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9ad9ba84aa828e86befeb2e8f3b36126' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='2988' end='4029' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2988-4029]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2dd512719a84b19708056e2c17669dfb' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto defines non-transient non-serializable instance field records_</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='2988' end='4029' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2988-4029]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' signature='Lcom/google/protobuf/ByteString;' name='records_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto.records_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f0fdc2e2498d5f3f8df463f83a96d40c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' start='4040' end='4367' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4040-4367]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' start='4111' end='4111' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 4111]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0d841b54b66aa8679ac20fc883d3031' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' start='4040' end='4367' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4040-4367]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e98e6fdd7c6f027d86ff206b5b64a995' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder' start='4259' end='4358' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4259-4358]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder' start='4285' end='4287' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder' start='4285' end='4285' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 4285]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='deddec3b4e8106c3e26bc0daba730c3c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' start='20801' end='21783' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20801-21783]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' start='20909' end='20909' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 20909]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e75bd1605803ad946d949f2f9226063e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' start='20801' end='21783' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20801-21783]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='53bffa7fb142c44773b6e21bebd40afd' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' start='21804' end='22217' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 21804-22217]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' start='21881' end='21881' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 21881]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='25789c0f7e4605f22bde56e203646fea' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' start='21804' end='22217' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 21804-22217]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4a1c517638540fefb131758dc65f0dd' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder' start='22063' end='22208' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22063-22208]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder' start='22089' end='22091' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder' start='22089' end='22089' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 22089]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='651f9f64bb9f81cb9f8c7eeedce55cc8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' start='2278' end='2908' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2278-2908]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' start='2368' end='2368' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 2368]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e7ff63f21bab6c1c822e4501f8e0750f' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' start='2278' end='2908' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2278-2908]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ce9236eb8039d812d368c443541b1cb8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' start='24109' end='24738' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24109-24738]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' start='24199' end='24199' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 24199]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1dec20cc7b0fa7703997c8e45a2258fd' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' start='24109' end='24738' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24109-24738]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcf3e7d1a3925cc9c58b1de0ac75991a' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' start='24805' end='25621' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24805-25621]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' start='24905' end='24905' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 24905]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fc3d5263b9ee301f68f328a488f8e768' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' start='24805' end='25621' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24805-25621]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e8c2082b220a0b51bd5d85bb7457421' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' start='7580' end='8209' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7580-8209]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' start='7670' end='7670' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 7670]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='681968c8992a64c4ca2c4479ac9afb7' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' start='7580' end='8209' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7580-8209]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='de4ad8b4e60cee586fa6cc4f9767bd85' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' start='8220' end='8547' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8220-8547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' start='8291' end='8291' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 8291]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='316fa9e9fcf521a60c6e0a1ee7ce2bda' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' start='8220' end='8547' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8220-8547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2db4750628fbe6d39586b9cfce14bf2a' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder' start='8439' end='8538' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8439-8538]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder' start='8465' end='8467' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder' start='8465' end='8465' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 8465]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4643cd10cc9cf67ccd807c546d55f353' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$AcceptRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' start='25704' end='26690' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 25704-26690]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='388' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='389' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27793' end='27793' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='389' primary='true'><Message>At QJournalProtocolProtos.java:[line 27793]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='826d83614e46a2b14d0d58fed059d5be' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$CanRollBackRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='193' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='194' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27728' end='27728' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='194' primary='true'><Message>At QJournalProtocolProtos.java:[line 27728]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ba08096b2d9d8726635dd45365fcf222' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DiscardSegmentsRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' start='16851' end='17630' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16851-17630]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='223' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='224' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27738' end='27738' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='224' primary='true'><Message>At QJournalProtocolProtos.java:[line 27738]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a2fd764f8e264c41300915fb2fbb6569' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoFinalizeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' start='12983' end='13674' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12983-13674]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='178' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='179' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27723' end='27723' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='179' primary='true'><Message>At QJournalProtocolProtos.java:[line 27723]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8c7268b8236bdb1c30a1183af663e9c3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoPreUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' start='10933' end='11474' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10933-11474]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='148' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27713' end='27713' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='149' primary='true'><Message>At QJournalProtocolProtos.java:[line 27713]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3c22636067d34a7046d9ce5a330cefa6' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoRollbackRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='208' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='209' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27733' end='27733' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='209' primary='true'><Message>At QJournalProtocolProtos.java:[line 27733]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b420a88c0818436abbcff4841e30de08' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' start='11856' end='12600' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11856-12600]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='163' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='164' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27718' end='27718' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='164' primary='true'><Message>At QJournalProtocolProtos.java:[line 27718]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f811336d19695a1968d1a7502f8001ca' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$FinalizeLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' start='6485' end='7202' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6485-7202]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='328' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='329' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27773' end='27773' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='329' primary='true'><Message>At QJournalProtocolProtos.java:[line 27773]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0fda136911ffed55905ee6bbd00b526' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$FormatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' start='19500' end='20394' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 19500-20394]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='268' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='269' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27753' end='27753' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='269' primary='true'><Message>At QJournalProtocolProtos.java:[line 27753]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b4f6e01a93f9bc7e5ade97974d0082b' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='358' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='359' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27783' end='27783' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='359' primary='true'><Message>At QJournalProtocolProtos.java:[line 27783]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fcf61554ab19d92a14dbad76a993f746' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetJournalCTimeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' start='9770' end='10461' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9770-10461]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='133' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='134' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27708' end='27708' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='134' primary='true'><Message>At QJournalProtocolProtos.java:[line 27708]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5f2722417de4f2a782bc5a2e9822a20' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetJournalStateRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' start='18013' end='18704' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18013-18704]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='238' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='239' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27743' end='27743' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='239' primary='true'><Message>At QJournalProtocolProtos.java:[line 27743]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='151542124e989050d1ec894574ea1e16' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' start='4392' end='4928' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4392-4928]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='298' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='299' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27763' end='27763' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='299' primary='true'><Message>At QJournalProtocolProtos.java:[line 27763]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ea8f2e71d2a7cb462450bbc62ca21412' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$IsFormattedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' start='8592' end='9283' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8592-9283]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='118' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='119' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27703' end='27703' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='119' primary='true'><Message>At QJournalProtocolProtos.java:[line 27703]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1efb161753f36fedee4cc9cf727760a4' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$JournalRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='2988' end='4029' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2988-4029]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='283' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='284' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27758' end='27758' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='284' primary='true'><Message>At QJournalProtocolProtos.java:[line 27758]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9bbd0cfb5f72a84a743a38c21d6473ca' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$NewEpochRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' start='20801' end='21783' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20801-21783]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='253' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='254' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27748' end='27748' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='254' primary='true'><Message>At QJournalProtocolProtos.java:[line 27748]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3c3a520f1fb1d239152787546f14029' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$PrepareRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' start='24109' end='24738' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24109-24738]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='373' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='374' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27788' end='27788' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='374' primary='true'><Message>At QJournalProtocolProtos.java:[line 27788]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1583c8f4d15134327559dcc5908b7ca6' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$PurgeLogsRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' start='7580' end='8209' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7580-8209]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='343' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='344' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27778' end='27778' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='344' primary='true'><Message>At QJournalProtocolProtos.java:[line 27778]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fd3e26cd25858e867cdcc3c48db33ff6' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27044' end='28210' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27044-28210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;Lcom/google/protobuf/RpcCallback;)V' name='callMethod' primary='true'><SourceLine endBytecode='168' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27696' end='27798' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService.callMethod(Descriptors$MethodDescriptor, RpcController, Message, RpcCallback)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$StartLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' start='5340' end='6097' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 5340-6097]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='313' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='314' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService' start='27768' end='27768' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='314' primary='true'><Message>At QJournalProtocolProtos.java:[line 27768]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='200bd046a701dcb2aa58cf5992124334' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$AcceptRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto' start='25704' end='26690' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 25704-26690]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='391' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='392' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27415' end='27415' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='392' primary='true'><Message>At QJournalProtocolProtos.java:[line 27415]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1e843a7fa92725ecf4cb39a170ac9bb6' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$CanRollBackRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto' start='14095' end='15280' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 14095-15280]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='196' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='197' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27389' end='27389' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='197' primary='true'><Message>At QJournalProtocolProtos.java:[line 27389]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f2030eba771f91ee1ad2d1337e2aed42' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DiscardSegmentsRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto' start='16851' end='17630' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 16851-17630]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='226' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='227' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27393' end='27393' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='227' primary='true'><Message>At QJournalProtocolProtos.java:[line 27393]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4e0b66d9079a60026088d7e3688ac355' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoFinalizeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto' start='12983' end='13674' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 12983-13674]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='181' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='182' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27387' end='27387' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='182' primary='true'><Message>At QJournalProtocolProtos.java:[line 27387]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5619686ad0861d475cf5f1e81f44f3d2' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoPreUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto' start='10933' end='11474' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 10933-11474]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='151' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='152' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27383' end='27383' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='152' primary='true'><Message>At QJournalProtocolProtos.java:[line 27383]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='52919466a0257752140880ca8e4f97fd' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoRollbackRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto' start='15767' end='16458' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 15767-16458]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='211' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='212' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27391' end='27391' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='212' primary='true'><Message>At QJournalProtocolProtos.java:[line 27391]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5bef4f85797ac547ce6343b55319662' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$DoUpgradeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto' start='11856' end='12600' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 11856-12600]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='166' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='167' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27385' end='27385' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='167' primary='true'><Message>At QJournalProtocolProtos.java:[line 27385]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d207149d3c2270d0bcd8185c2a7306de' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$FinalizeLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto' start='6485' end='7202' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6485-7202]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='331' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='332' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27407' end='27407' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='332' primary='true'><Message>At QJournalProtocolProtos.java:[line 27407]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2b08c9dbaea05b0ef86e7e824a2d6b56' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$FormatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto' start='19500' end='20394' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 19500-20394]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='271' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='272' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27399' end='27399' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='272' primary='true'><Message>At QJournalProtocolProtos.java:[line 27399]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3b76c1ba5503018e7ac4a7b46251b1f8' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetEditLogManifestRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto' start='22300' end='23213' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 22300-23213]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='361' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='362' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27411' end='27411' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='362' primary='true'><Message>At QJournalProtocolProtos.java:[line 27411]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cea832f9191088c30521c07ad7abe82f' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetJournalCTimeRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto' start='9770' end='10461' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 9770-10461]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='136' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='137' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27381' end='27381' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='137' primary='true'><Message>At QJournalProtocolProtos.java:[line 27381]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b43dbb051eca8aa1ce9cfae05ef21cad' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetJournalStateRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto' start='18013' end='18704' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 18013-18704]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='241' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='242' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27395' end='27395' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='242' primary='true'><Message>At QJournalProtocolProtos.java:[line 27395]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f67b9d6e97cc52455bab880506d7d932' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$HeartbeatRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto' start='4392' end='4928' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 4392-4928]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='301' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='302' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27403' end='27403' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='302' primary='true'><Message>At QJournalProtocolProtos.java:[line 27403]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='57719905bae25451ce905c36ccba3253' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$IsFormattedRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto' start='8592' end='9283' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 8592-9283]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='121' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='122' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27379' end='27379' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='122' primary='true'><Message>At QJournalProtocolProtos.java:[line 27379]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='de0ff7cd47f33f77474e23fb29b6422' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$JournalRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto' start='2988' end='4029' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 2988-4029]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='286' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='287' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27401' end='27401' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='287' primary='true'><Message>At QJournalProtocolProtos.java:[line 27401]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dad3c5cca7aab8bcd7acd4b8572d73e0' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$NewEpochRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto' start='20801' end='21783' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 20801-21783]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='256' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='257' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27397' end='27397' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='257' primary='true'><Message>At QJournalProtocolProtos.java:[line 27397]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9f73e333a51317460e267c1aea68f8c4' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$PrepareRecoveryRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto' start='24109' end='24738' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 24109-24738]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='376' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='377' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27413' end='27413' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='377' primary='true'><Message>At QJournalProtocolProtos.java:[line 27413]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b46934b0ddcdfb6db7d9fac0f2abd17' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$PurgeLogsRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto' start='7580' end='8209' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 7580-8209]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='346' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='347' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27409' end='27409' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='347' primary='true'><Message>At QJournalProtocolProtos.java:[line 27409]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3e69a86d65a0e5ce9574e69b01698df' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from com.google.protobuf.Message to org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27361' end='27521' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 27361-27521]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' signature='(Lcom/google/protobuf/Descriptors$MethodDescriptor;Lcom/google/protobuf/RpcController;Lcom/google/protobuf/Message;)Lcom/google/protobuf/Message;' name='callBlockingMethod' primary='true'><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27372' end='27417' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message)</Message></Method><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/Message;'><SourceLine classname='com.google.protobuf.Message' sourcepath='com/google/protobuf/Message.java' sourcefile='Message.java'><Message>In Message.java</Message></SourceLine><Message>Actual type com.google.protobuf.Message</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$StartLogSegmentRequestProto;'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' start='5340' end='6097' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 5340-6097]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='316' name='request' register='3'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='317' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2' start='27405' end='27405' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='317' primary='true'><Message>At QJournalProtocolProtos.java:[line 27405]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='920e4564032d6368fe393d7517a7c773' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' start='609' end='1599' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 609-1599]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' start='714' end='714' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 714]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='72227a3867b744b3423a07a96902ac2b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' start='609' end='1599' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 609-1599]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='14366bb7f3a4a421c1b5d10882eac6c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' start='1640' end='2237' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 1640-2237]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' start='1727' end='1727' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 1727]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d3d5d722ac9efcbfabcf78d844fd6deb' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' start='1640' end='2237' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 1640-2237]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5f5bfc76bc72d758b95c6c66ef161e1' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder' start='1987' end='2228' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 1987-2228]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder' start='2013' end='2015' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder' start='2013' end='2013' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 2013]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c8b54bbe25e579e8b9ce3df554dc161d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' start='5340' end='6097' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 5340-6097]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' start='5435' end='5435' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 5435]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f83a6d70d0fba43d4510316e806c5123' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' start='5340' end='6097' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 5340-6097]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='63c2b2049b3764db4524ab795f4c632d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' start='6108' end='6435' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6108-6435]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' start='6179' end='6179' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='7' primary='true'><Message>At QJournalProtocolProtos.java:[line 6179]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='543273ccfbc9cf147ac2918d7a893488' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' start='6108' end='6435' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6108-6435]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>In QJournalProtocolProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6a2e3d3297c6fefd24246015893554ce' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='6327' end='6426' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java'><Message>At QJournalProtocolProtos.java:[lines 6327-6426]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='6353' end='6355' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder' start='6353' end='6353' sourcepath='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' sourcefile='QJournalProtocolProtos.java' startBytecode='3' primary='true'><Message>At QJournalProtocolProtos.java:[line 6353]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='73205e9a128021c44e1de1d42acc287d' rank='16' abbrev='OS' category='BAD_PRACTICE' priority='2' type='OS_OPEN_STREAM' instanceOccurrenceMax='0'><ShortMessage>Method may fail to close stream</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.server.Journal.persistPaxosData(long, QJournalProtocolProtos$PersistedRecoveryPaxosData) may fail to close stream</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.server.Journal' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.Journal' start='81' end='1151' sourcepath='org/apache/hadoop/hdfs/qjournal/server/Journal.java' sourcefile='Journal.java'><Message>At Journal.java:[lines 81-1151]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.Journal</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.server.Journal' signature='(JLorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$PersistedRecoveryPaxosData;)V' name='persistPaxosData' primary='true'><SourceLine endBytecode='55' classname='org.apache.hadoop.hdfs.qjournal.server.Journal' start='1020' end='1043' sourcepath='org/apache/hadoop/hdfs/qjournal/server/Journal.java' sourcefile='Journal.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.server.Journal.persistPaxosData(long, QJournalProtocolProtos$PersistedRecoveryPaxosData)</Message></Method><Type role='TYPE_CLOSEIT' descriptor='Ljava/io/Writer;'><SourceLine classname='java.io.Writer' start='50' end='294' sourcepath='java/io/Writer.java' sourcefile='Writer.java'><Message>At Writer.java:[lines 50-294]</Message></SourceLine><Message>Need to close java.io.Writer </Message></Type><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.qjournal.server.Journal' start='1028' end='1028' sourcepath='org/apache/hadoop/hdfs/qjournal/server/Journal.java' sourcefile='Journal.java' startBytecode='37' primary='true'><Message>At Journal.java:[line 1028]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='33550ef915c9899009f08fe79c09c1fd' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector.instance isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector' start='32' end='40' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector.java' sourcefile='JournalFaultInjector.java'><Message>At JournalFaultInjector.java:[lines 32-40]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector' signature='Lorg/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector;' name='instance' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector.java' sourcefile='JournalFaultInjector.java'><Message>In JournalFaultInjector.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector.instance</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector' start='33' end='33' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector.java' sourcefile='JournalFaultInjector.java' startBytecode='7' primary='true'><Message>At JournalFaultInjector.java:[line 33]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c52050e0a2ff49a21661fd917c968f08' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.qjournal.server.JournalNode$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode$1'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode$1' start='374' end='377' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNode.java' sourcefile='JournalNode.java'><Message>At JournalNode.java:[lines 374-377]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.qjournal.server.JournalNode$1</Message></Class><Class classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode' start='69' end='513' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNode.java' sourcefile='JournalNode.java'><Message>At JournalNode.java:[lines 69-513]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.JournalNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode' signature='()Ljava/lang/String;' name='getJournalsStatus' primary='true'><SourceLine endBytecode='593' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode' start='355' end='393' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNode.java' sourcefile='JournalNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.server.JournalNode.getJournalsStatus()</Message></Method><SourceLine endBytecode='146' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNode' start='374' end='374' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNode.java' sourcefile='JournalNode.java' startBytecode='146' primary='true'><Message>At JournalNode.java:[line 374]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='75dfa1dbc293ffaa3822a4f9f6117973' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of otherJournalEditLogs, which is known to be non-null in org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getMissingLogSegments(List, QJournalProtocolProtos$GetEditLogManifestResponseProto, JournalNodeSyncer$JournalNodeProxy)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='62' end='483' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java'><Message>At JournalNodeSyncer.java:[lines 62-483]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' signature='(Ljava/util/List;Lorg/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos$GetEditLogManifestResponseProto;Lorg/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer$JournalNodeProxy;)V' name='getMissingLogSegments' primary='true'><SourceLine endBytecode='777' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='324' end='369' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getMissingLogSegments(List, QJournalProtocolProtos$GetEditLogManifestResponseProto, JournalNodeSyncer$JournalNodeProxy)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='12' name='otherJournalEditLogs' register='4'><Message>Value loaded from otherJournalEditLogs</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest' signature='()Ljava/util/List;' name='getLogs'><SourceLine endBytecode='49' classname='org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest' start='72' end='72' sourcepath='org/apache/hadoop/hdfs/server/protocol/RemoteEditLogManifest.java' sourcefile='RemoteEditLogManifest.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest.getLogs() of type java.util.List</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='14' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='326' end='326' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java' startBytecode='14' primary='true'><Message>Redundant null check at JournalNodeSyncer.java:[line 326]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='be777d0d88011b3cbddbfcbb7ecf1af1' rank='11' abbrev='RpC' category='CORRECTNESS' priority='3' type='RpC_REPEATED_CONDITIONAL_TEST' instanceOccurrenceMax='0'><ShortMessage>Repeated conditional tests</ShortMessage><LongMessage>Repeated conditional test in org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getMissingLogList(List, List)</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='62' end='483' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java'><Message>At JournalNodeSyncer.java:[lines 62-483]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' signature='(Ljava/util/List;Ljava/util/List;)Ljava/util/List;' name='getMissingLogList' primary='true'><SourceLine endBytecode='477' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='378' end='411' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getMissingLogList(List, List)</Message></Method><SourceLine endBytecode='138' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='405' end='405' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java' startBytecode='135' primary='true'><Message>At JournalNodeSyncer.java:[line 405]</Message></SourceLine><SourceLine endBytecode='145' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer' start='406' end='406' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java' startBytecode='142'><Message>At JournalNodeSyncer.java:[line 406]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c0a5753cebf91a48fbaa353272d558bd' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_NEEDS_THIS' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy could be refactored into a _static_ inner class</LongMessage><Class classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy' start='486' end='512' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java'><Message>At JournalNodeSyncer.java:[lines 486-512]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy' start='486' end='512' sourcepath='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' sourcefile='JournalNodeSyncer.java'><Message>At JournalNodeSyncer.java:[lines 486-512]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='77749dba768c75ff73dba288ba6085da' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.setBlockPoolId(String) and org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.setBlockpoolID(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager' start='58' end='517' sourcepath='org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java' sourcefile='BlockTokenSecretManager.java'><Message>At BlockTokenSecretManager.java:[lines 58-517]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager' signature='(Ljava/lang/String;)V' name='setBlockPoolId' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager' start='151' end='152' sourcepath='org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java' sourcefile='BlockTokenSecretManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.setBlockPoolId(String)</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.CheckpointSignature'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.CheckpointSignature' start='31' end='175' sourcepath='org/apache/hadoop/hdfs/server/namenode/CheckpointSignature.java' sourcefile='CheckpointSignature.java'><Message>At CheckpointSignature.java:[lines 31-175]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.CheckpointSignature</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.CheckpointSignature' signature='(Ljava/lang/String;)V' name='setBlockpoolID'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.CheckpointSignature' start='103' end='104' sourcepath='org/apache/hadoop/hdfs/server/namenode/CheckpointSignature.java' sourcefile='CheckpointSignature.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.setBlockpoolID(String)</Message></Method><SourceLine synthetic='true' endBytecode='61' classname='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager' start='151' end='152' sourcepath='org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java' sourcefile='BlockTokenSecretManager.java' startBytecode='0'><Message>At BlockTokenSecretManager.java:[lines 151-152]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5c2371e1ba23198b2e12044c40200d2' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys.getAllKeys() may expose internal representation by returning ExportedBlockKeys.allKeys</LongMessage><Class classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' start='35' end='115' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java'><Message>At ExportedBlockKeys.java:[lines 35-115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' signature='()[Lorg/apache/hadoop/hdfs/security/token/block/BlockKey;' name='getAllKeys' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' start='72' end='72' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys.getAllKeys()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' signature='[Lorg/apache/hadoop/hdfs/security/token/block/BlockKey;' name='allKeys' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java'><Message>In ExportedBlockKeys.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys.allKeys</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys' start='72' end='72' sourcepath='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' sourcefile='ExportedBlockKeys.java' startBytecode='4' primary='true'><Message>At ExportedBlockKeys.java:[line 72]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cc3760af13899e13b66415c7c7d56829' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup to org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source of return value in org.apache.hadoop.hdfs.server.balancer.Balancer.choose4One(Dispatcher$DDatanode$StorageGroup, Collection, Matcher)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.balancer.Balancer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Balancer' start='174' end='933' sourcepath='org/apache/hadoop/hdfs/server/balancer/Balancer.java' sourcefile='Balancer.java'><Message>At Balancer.java:[lines 174-933]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Balancer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Balancer' signature='(Lorg/apache/hadoop/hdfs/server/balancer/Dispatcher$DDatanode$StorageGroup;Ljava/util/Collection;Lorg/apache/hadoop/hdfs/server/balancer/Matcher;)Z' name='choose4One' primary='true'><SourceLine endBytecode='33' classname='org.apache.hadoop.hdfs.server.balancer.Balancer' start='514' end='528' sourcepath='org/apache/hadoop/hdfs/server/balancer/Balancer.java' sourcefile='Balancer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.balancer.Balancer.choose4One(Dispatcher$DDatanode$StorageGroup, Collection, Matcher)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/balancer/Dispatcher$DDatanode$StorageGroup;'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup' start='541' end='628' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 541-628]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/balancer/Dispatcher$Source;'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' start='751' end='1014' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 751-1014]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='46' name='chosen' register='5'><Message>Value loaded from chosen</Message></LocalVariable><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.server.balancer.Balancer' start='523' end='523' sourcepath='org/apache/hadoop/hdfs/server/balancer/Balancer.java' sourcefile='Balancer.java' startBytecode='48' primary='true'><Message>At Balancer.java:[line 523]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2ddb447b8785c203b441afb6f02f2127' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.balancer.Dispatcher$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$1'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$1' start='1155' end='1159' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 1155-1159]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.balancer.Dispatcher$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='89' end='1391' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 89-1391]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Dispatcher</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' signature='(Lorg/apache/hadoop/hdfs/server/balancer/Dispatcher$PendingMove;)V' name='executePendingMove' primary='true'><SourceLine endBytecode='248' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='1141' end='1161' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.balancer.Dispatcher.executePendingMove(Dispatcher$PendingMove)</Message></Method><SourceLine endBytecode='92' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='1155' end='1155' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='92' primary='true'><Message>At Dispatcher.java:[line 1155]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='96eee77d929e91b157223e0053119e20' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.balancer.Dispatcher$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$2'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$2' start='1218' end='1222' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 1218-1222]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.balancer.Dispatcher$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='89' end='1391' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 89-1391]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Dispatcher</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' signature='()J' name='dispatchBlockMoves' primary='true'><SourceLine endBytecode='916' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='1182' end='1244' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves()</Message></Method><SourceLine endBytecode='354' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher' start='1218' end='1218' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='354' primary='true'><Message>At Dispatcher.java:[line 1218]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f999080d7d5d5cb8b8fd000440c55c08' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator.lotSize; locked 50% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' start='145' end='176' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 145-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' signature='I' name='lotSize' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>In Dispatcher.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator.lotSize</Message></Field><Int role='INT_SYNC_PERCENT' value='50'><Message>Synchronized 50% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='2' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' start='166' end='166' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='2' primary='true'><Message>Unsynchronized access at Dispatcher.java:[line 166]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='2' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator' start='175' end='175' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='2'><Message>Synchronized access at Dispatcher.java:[line 175]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe1c0f73a7b01ba54dab258f3565a750' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped(Block, byte[], short, int) may expose internal representation by storing an externally mutable object into Dispatcher$DBlockStriped.indices</LongMessage><Class classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' start='493' end='518' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 493-518]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' signature='(Lorg/apache/hadoop/hdfs/protocol/Block;[BSI)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='119' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' start='493' end='497' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped(Block, byte[], short, int)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' signature='[B' name='indices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>In Dispatcher.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped.indices</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='7' name='indices' register='2'><Message>Local variable named indices</Message></LocalVariable><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped' start='494' end='494' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='7' primary='true'><Message>At Dispatcher.java:[line 494]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='87af2d7d401354c110e715a803ea1274' rank='17' abbrev='UW' category='MT_CORRECTNESS' priority='3' type='UW_UNCOND_WAIT' instanceOccurrenceMax='0'><ShortMessage>Unconditional wait</ShortMessage><LongMessage>Unconditional wait in org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source.dispatchBlocks(long)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' start='751' end='1014' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java'><Message>At Dispatcher.java:[lines 751-1014]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' signature='(J)V' name='dispatchBlocks' primary='true'><SourceLine endBytecode='925' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' start='934' end='1005' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source.dispatchBlocks(long)</Message></Method><SourceLine endBytecode='361' classname='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source' start='992' end='992' sourcepath='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' sourcefile='Dispatcher.java' startBytecode='361' primary='true'><Message>At Dispatcher.java:[line 992]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c3510fe8c80ed84607c7b522db7a7b3c' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped in org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getDefaultStorageNum(BlockInfo)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='157' end='4973' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 157-4973]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;)I' name='getDefaultStorageNum' primary='true'><SourceLine endBytecode='25' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='882' end='888' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getDefaultStorageNum(BlockInfo)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo' start='41' end='326' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java' sourcefile='BlockInfo.java'><Message>At BlockInfo.java:[lines 41-326]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped' start='43' end='266' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java' sourcefile='BlockInfoStriped.java'><Message>At BlockInfoStriped.java:[lines 43-266]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='36' name='block' register='1'><Message>Value loaded from block</Message></LocalVariable><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='883' end='883' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='37' primary='true'><Message>At BlockManager.java:[line 883]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='75b18b386a656d3c5c26d92b2ae898ff' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped in org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getExpectedRedundancyNum(BlockInfo)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='157' end='4973' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 157-4973]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;)S' name='getExpectedRedundancyNum' primary='true'><SourceLine endBytecode='9' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4416' end='4416' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getExpectedRedundancyNum(BlockInfo)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo' start='41' end='326' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java' sourcefile='BlockInfo.java'><Message>At BlockInfo.java:[lines 41-326]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped' start='43' end='266' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java' sourcefile='BlockInfoStriped.java'><Message>At BlockInfoStriped.java:[lines 43-266]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='7' name='block' register='1'><Message>Value loaded from block</Message></LocalVariable><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4416' end='4416' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='8' primary='true'><Message>At BlockManager.java:[line 4416]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4363e150277fa2174c86f0e9be5e755' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped in org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getMinStorageNum(BlockInfo)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='157' end='4973' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 157-4973]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;)S' name='getMinStorageNum' primary='true'><SourceLine endBytecode='25' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='897' end='903' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.getMinStorageNum(BlockInfo)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo' start='41' end='326' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java' sourcefile='BlockInfo.java'><Message>At BlockInfo.java:[lines 41-326]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped' start='43' end='266' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java' sourcefile='BlockInfoStriped.java'><Message>At BlockInfoStriped.java:[lines 43-266]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='36' name='block' register='1'><Message>Value loaded from block</Message></LocalVariable><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='898' end='898' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='37' primary='true'><Message>At BlockManager.java:[line 898]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5a1443a0b239a0e5f45f2290f549187' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.shutdown() and org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker.shutDown()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='157' end='4973' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 157-4973]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' signature='()V' name='shutdown' primary='true'><SourceLine endBytecode='81' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4727' end='4731' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.shutdown()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' start='46' end='171' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' sourcefile='ErasureCodingWorker.java'><Message>At ErasureCodingWorker.java:[lines 46-171]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' signature='()V' name='shutDown'><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' start='169' end='171' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' sourcefile='ErasureCodingWorker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker.shutDown()</Message></Method><SourceLine synthetic='true' endBytecode='81' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4727' end='4731' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'><Message>At BlockManager.java:[lines 4727-4731]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcf4da1239259ccabc2593222bed5de9' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2' start='4835' end='4838' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 4835-4838]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='157' end='4973' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java'><Message>At BlockManager.java:[lines 157-4973]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' signature='()V' name='flushBlockOps' primary='true'><SourceLine endBytecode='59' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4835' end='4841' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.flushBlockOps()</Message></Method><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager' start='4835' end='4835' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' sourcefile='BlockManager.java' startBytecode='6' primary='true'><Message>At BlockManager.java:[line 4835]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d6501e3b72c2db75b0a7369f1ae8c841' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector.instance isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector' start='29' end='51' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.java' sourcefile='BlockManagerFaultInjector.java'><Message>At BlockManagerFaultInjector.java:[lines 29-51]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector' signature='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector;' name='instance' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.java' sourcefile='BlockManagerFaultInjector.java'><Message>In BlockManagerFaultInjector.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector.instance</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector' start='31' end='31' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.java' sourcefile='BlockManagerFaultInjector.java' startBytecode='7' primary='true'><Message>At BlockManagerFaultInjector.java:[line 31]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='78aa4e8b6972ed109fb8ee5c20796ffd' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.net.NetworkTopology to org.apache.hadoop.hdfs.net.DFSNetworkTopology in org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode(String, Collection, StorageType)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' start='49' end='1211' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java'><Message>At BlockPlacementPolicyDefault.java:[lines 49-1211]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' signature='(Ljava/lang/String;Ljava/util/Collection;Lorg/apache/hadoop/fs/StorageType;)Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor;' name='chooseDataNode' primary='true'><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' start='851' end='851' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode(String, Collection, StorageType)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/net/NetworkTopology;'><SourceLine classname='org.apache.hadoop.net.NetworkTopology' start='48' end='950' sourcepath='org/apache/hadoop/net/NetworkTopology.java' sourcefile='NetworkTopology.java'><Message>At NetworkTopology.java:[lines 48-950]</Message></SourceLine><Message>Actual type org.apache.hadoop.net.NetworkTopology</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/net/DFSNetworkTopology;'><SourceLine classname='org.apache.hadoop.hdfs.net.DFSNetworkTopology' start='44' end='369' sourcepath='org/apache/hadoop/hdfs/net/DFSNetworkTopology.java' sourcefile='DFSNetworkTopology.java'><Message>At DFSNetworkTopology.java:[lines 44-369]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.net.DFSNetworkTopology</Message></Type><Field isStatic='false' role='FIELD_VALUE_OF' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' signature='Lorg/apache/hadoop/net/NetworkTopology;' name='clusterMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java'><Message>In BlockPlacementPolicyDefault.java</Message></SourceLine><Message>Value loaded from field org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.clusterMap</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' start='851' end='851' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java' startBytecode='4' primary='true'><Message>At BlockPlacementPolicyDefault.java:[line 851]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b16d7bd53f4b702978a990124fcf611' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.net.NetworkTopology to org.apache.hadoop.net.NetworkTopologyWithNodeGroup in org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup.chooseFavouredNodes(String, int, List, Set, long, int, List, boolean, EnumMap)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='42' end='436' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java'><Message>At BlockPlacementPolicyWithNodeGroup.java:[lines 42-436]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' signature='(Ljava/lang/String;ILjava/util/List;Ljava/util/Set;JILjava/util/List;ZLjava/util/EnumMap;)V' name='chooseFavouredNodes' primary='true'><SourceLine endBytecode='86' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='65' end='100' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup.chooseFavouredNodes(String, int, List, Set, long, int, List, boolean, EnumMap)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/net/NetworkTopology;'><SourceLine classname='org.apache.hadoop.net.NetworkTopology' start='48' end='950' sourcepath='org/apache/hadoop/net/NetworkTopology.java' sourcefile='NetworkTopology.java'><Message>At NetworkTopology.java:[lines 48-950]</Message></SourceLine><Message>Actual type org.apache.hadoop.net.NetworkTopology</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/net/NetworkTopologyWithNodeGroup;'><SourceLine classname='org.apache.hadoop.net.NetworkTopologyWithNodeGroup' start='38' end='300' sourcepath='org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java' sourcefile='NetworkTopologyWithNodeGroup.java'><Message>At NetworkTopologyWithNodeGroup.java:[lines 38-300]</Message></SourceLine><Message>Expected org.apache.hadoop.net.NetworkTopologyWithNodeGroup</Message></Type><Field isStatic='false' role='FIELD_VALUE_OF' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' signature='Lorg/apache/hadoop/net/NetworkTopology;' name='clusterMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java'><Message>In BlockPlacementPolicyDefault.java</Message></SourceLine><Message>Value loaded from field org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.clusterMap</Message></Field><SourceLine endBytecode='90' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='78' end='78' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java' startBytecode='90' primary='true'><Message>At BlockPlacementPolicyWithNodeGroup.java:[line 78]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0369fae9303bba6f89927e8190d7e1c' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.net.NetworkTopology to org.apache.hadoop.net.NetworkTopologyWithNodeGroup in org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup.chooseLocalStorage(Node, Set, long, int, List, boolean, EnumMap, boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='42' end='436' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java'><Message>At BlockPlacementPolicyWithNodeGroup.java:[lines 42-436]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' signature='(Lorg/apache/hadoop/net/Node;Ljava/util/Set;JILjava/util/List;ZLjava/util/EnumMap;Z)Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo;' name='chooseLocalStorage' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='126' end='144' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup.chooseLocalStorage(Node, Set, long, int, List, boolean, EnumMap, boolean)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/net/NetworkTopology;'><SourceLine classname='org.apache.hadoop.net.NetworkTopology' start='48' end='950' sourcepath='org/apache/hadoop/net/NetworkTopology.java' sourcefile='NetworkTopology.java'><Message>At NetworkTopology.java:[lines 48-950]</Message></SourceLine><Message>Actual type org.apache.hadoop.net.NetworkTopology</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/net/NetworkTopologyWithNodeGroup;'><SourceLine classname='org.apache.hadoop.net.NetworkTopologyWithNodeGroup' start='38' end='300' sourcepath='org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java' sourcefile='NetworkTopologyWithNodeGroup.java'><Message>At NetworkTopologyWithNodeGroup.java:[lines 38-300]</Message></SourceLine><Message>Expected org.apache.hadoop.net.NetworkTopologyWithNodeGroup</Message></Type><Field isStatic='false' role='FIELD_VALUE_OF' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' signature='Lorg/apache/hadoop/net/NetworkTopology;' name='clusterMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' sourcefile='BlockPlacementPolicyDefault.java'><Message>In BlockPlacementPolicyDefault.java</Message></SourceLine><Message>Value loaded from field org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.clusterMap</Message></Field><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup' start='137' end='137' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' sourcefile='BlockPlacementPolicyWithNodeGroup.java' startBytecode='37' primary='true'><Message>At BlockPlacementPolicyWithNodeGroup.java:[line 137]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4dde471e20754249e36429ad90676ffe' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_METHOD_NAMING_CONVENTION' instanceOccurrenceMax='0'><ShortMessage>Method names should start with a lower case letter</ShortMessage><LongMessage>The method name org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData.ListHead(String) doesn't start with a lower case letter</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' start='85' end='122' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java'><Message>At BlockReportLeaseManager.java:[lines 85-122]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' signature='(Ljava/lang/String;)Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager$NodeData;' name='ListHead' primary='true'><SourceLine endBytecode='84' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' start='85' end='88' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData.ListHead(String)</Message></Method><SourceLine synthetic='true' endBytecode='84' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' start='85' end='88' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='0'><Message>At BlockReportLeaseManager.java:[lines 85-88]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcd166dec55f5bdf3a866fecdf76ffdf' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BlockReportLeaseManager$NodeData.next not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.pruneExpiredPending(long)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' start='85' end='122' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java'><Message>At BlockReportLeaseManager.java:[lines 85-122]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' signature='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager$NodeData;' name='next' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java'><Message>In BlockReportLeaseManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData.next</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' signature='(J)V' name='pruneExpiredPending' primary='true'><SourceLine endBytecode='175' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' start='286' end='295' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.pruneExpiredPending(long)</Message></Method><SourceLine endBytecode='17' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' start='288' end='288' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='17' primary='true'><Message>At BlockReportLeaseManager.java:[line 288]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2b24980fe7a2636652f95e592cb2de3b' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BlockReportLeaseManager$NodeData.next not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.requestLease(DatanodeDescriptor)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' start='85' end='122' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java'><Message>At BlockReportLeaseManager.java:[lines 85-122]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' signature='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager$NodeData;' name='next' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java'><Message>In BlockReportLeaseManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData.next</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor;)J' name='requestLease' primary='true'><SourceLine endBytecode='591' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' start='228' end='269' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.requestLease(DatanodeDescriptor)</Message></Method><SourceLine endBytecode='149' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager' start='252' end='252' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' sourcefile='BlockReportLeaseManager.java' startBytecode='149' primary='true'><Message>At BlockReportLeaseManager.java:[line 252]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4051f402332978b4b64bc0625c34ef75' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite(byte, BlockStoragePolicy[]) may expose internal representation by storing an externally mutable object into BlockStoragePolicySuite.policies</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' start='37' end='150' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' sourcefile='BlockStoragePolicySuite.java'><Message>At BlockStoragePolicySuite.java:[lines 37-150]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' signature='(B[Lorg/apache/hadoop/hdfs/protocol/BlockStoragePolicy;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' start='98' end='101' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' sourcefile='BlockStoragePolicySuite.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite(byte, BlockStoragePolicy[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' signature='[Lorg/apache/hadoop/hdfs/protocol/BlockStoragePolicy;' name='policies' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' sourcefile='BlockStoragePolicySuite.java'><Message>In BlockStoragePolicySuite.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite.policies</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='policies' register='2'><Message>Local variable named policies</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' start='100' end='100' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' sourcefile='BlockStoragePolicySuite.java' startBytecode='11' primary='true'><Message>At BlockStoragePolicySuite.java:[line 100]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='563bed57abc5bf1cecc2058de7ddc4f6' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1' start='47' end='58' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java'><Message>At BlocksMap.java:[lines 47-58]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='34' end='235' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java'><Message>At BlocksMap.java:[lines 34-235]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' signature='(I)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='116' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='44' end='61' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap(int)</Message></Method><SourceLine endBytecode='38' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='47' end='47' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java' startBytecode='38' primary='true'><Message>At BlocksMap.java:[line 47]</Message></SourceLine><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' signature='Lorg/apache/hadoop/util/GSet;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java'><Message>In BlocksMap.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.blocks</Message></Field></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4d5d5688cc78a1c8b4ec6796e257e093' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$2'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$2' start='147' end='150' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java'><Message>At BlocksMap.java:[lines 147-150]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='34' end='235' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java'><Message>At BlocksMap.java:[lines 34-235]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;)Ljava/lang/Iterable;' name='getStorages' primary='true'><SourceLine endBytecode='86' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='144' end='147' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getStorages(BlockInfo)</Message></Method><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap' start='147' end='147' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' sourcefile='BlocksMap.java' startBytecode='14' primary='true'><Message>At BlocksMap.java:[line 147]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='40dc3140586f74fe41e42652700eae10' cweid='253' rank='16' abbrev='RV' category='BAD_PRACTICE' priority='2' type='RV_RETURN_VALUE_IGNORED_BAD_PRACTICE' instanceOccurrenceMax='0'><ShortMessage>Method ignores exceptional return value</ShortMessage><LongMessage>Exceptional return value of java.util.concurrent.locks.Condition.await(long, TimeUnit) ignored in org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor' start='66' end='822' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java' sourcefile='CacheReplicationMonitor.java'><Message>At CacheReplicationMonitor.java:[lines 66-822]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor' signature='()V' name='run' primary='true'><SourceLine endBytecode='784' classname='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor' start='157' end='210' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java' sourcefile='CacheReplicationMonitor.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run()</Message></Method><Method isStatic='false' role='METHOD_CALLED' classname='java.util.concurrent.locks.Condition' signature='(JLjava/util/concurrent/TimeUnit;)Z' name='await'><SourceLine classname='java.util.concurrent.locks.Condition' sourcepath='java/util/concurrent/locks/Condition.java' sourcefile='Condition.java'></SourceLine><Message>Called method java.util.concurrent.locks.Condition.await(long, TimeUnit)</Message></Method><SourceLine endBytecode='176' classname='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor' start='181' end='181' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java' sourcefile='CacheReplicationMonitor.java' startBytecode='176' primary='true'><Message>At CacheReplicationMonitor.java:[line 181]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a52669670600eb6b23c6810fea25bfac' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.hostProperties; locked 57% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='62' end='271' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 62-271]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' signature='Lorg/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager$HostProperties;' name='hostProperties' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>In CombinedHostFileManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.hostProperties</Message></Field><Int role='INT_SYNC_PERCENT' value='57'><Message>Synchronized 57% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='190' end='190' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1' primary='true'><Message>Unsynchronized access at CombinedHostFileManager.java:[line 190]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='195' end='195' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1'><Message>Unsynchronized access at CombinedHostFileManager.java:[line 195]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='258' end='258' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1'><Message>Unsynchronized access at CombinedHostFileManager.java:[line 258]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='248' end='248' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1'><Message>Synchronized access at CombinedHostFileManager.java:[line 248]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='253' end='253' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1'><Message>Synchronized access at CombinedHostFileManager.java:[line 253]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='6' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='269' end='269' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='6'><Message>Synchronized access at CombinedHostFileManager.java:[line 269]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager' start='239' end='239' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='1'><Message>Synchronized access at CombinedHostFileManager.java:[line 239]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c77dba9ef3f35e3bc7c730ecaef6a564' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' start='89' end='92' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 89-92]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='68' end='165' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 68-165]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' signature='(Ljava/net/InetSocketAddress;)Z' name='isIncluded' primary='true'><SourceLine endBytecode='112' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='87' end='87' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties.isIncluded(InetSocketAddress)</Message></Method><SourceLine endBytecode='26' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='88' end='88' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='26' primary='true'><Message>At CombinedHostFileManager.java:[line 88]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='388e7cbd7fa06a10aa4d34e3f8f5422d' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' start='99' end='104' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 99-104]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='68' end='165' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 68-165]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' signature='(Ljava/net/InetSocketAddress;)Z' name='isExcluded' primary='true'><SourceLine endBytecode='77' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='98' end='98' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties.isExcluded(InetSocketAddress)</Message></Method><SourceLine endBytecode='19' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='98' end='98' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='19' primary='true'><Message>At CombinedHostFileManager.java:[line 98]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='619e1ed43053101c6b8f54e571bf788b' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' start='112' end='115' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 112-115]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='68' end='165' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 68-165]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' signature='(Ljava/net/InetSocketAddress;)Ljava/lang/String;' name='getUpgradeDomain' primary='true'><SourceLine endBytecode='179' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='110' end='118' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties.getUpgradeDomain(InetSocketAddress)</Message></Method><SourceLine endBytecode='19' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='111' end='111' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='19' primary='true'><Message>At CombinedHostFileManager.java:[line 111]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b846a9d36ade95a66fc15535b3559ce9' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' start='154' end='159' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 154-159]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='68' end='165' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 68-165]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' signature='(Ljava/net/InetSocketAddress;)J' name='getMaintenanceExpireTimeInMS' primary='true'><SourceLine endBytecode='177' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='152' end='164' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties.getMaintenanceExpireTimeInMS(InetSocketAddress)</Message></Method><SourceLine endBytecode='19' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties' start='153' end='153' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='19' primary='true'><Message>At CombinedHostFileManager.java:[line 153]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7ab912f4e8f778f1586478bed4cbb1ef' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>input must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' start='89' end='92' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 89-92]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' signature='(Lorg/apache/hadoop/hdfs/protocol/DatanodeAdminProperties;)Z' name='apply' primary='true'><SourceLine endBytecode='98' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' start='91' end='91' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1.apply(DatanodeAdminProperties)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='input' register='1'><Message>Parameter input</Message></LocalVariable><SourceLine synthetic='true' endBytecode='98' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1' start='91' end='91' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'><Message>At CombinedHostFileManager.java:[line 91]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2398c2fc63a4f25011907a73a9947e' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>input must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' start='99' end='104' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 99-104]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' signature='(Lorg/apache/hadoop/hdfs/protocol/DatanodeAdminProperties;)Z' name='apply' primary='true'><SourceLine endBytecode='115' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' start='101' end='101' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2.apply(DatanodeAdminProperties)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='input' register='1'><Message>Parameter input</Message></LocalVariable><SourceLine synthetic='true' endBytecode='115' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2' start='101' end='101' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'><Message>At CombinedHostFileManager.java:[line 101]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cf0aafd0a025ef613b8994baf618a467' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>input must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' start='112' end='115' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 112-115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' signature='(Lorg/apache/hadoop/hdfs/protocol/DatanodeAdminProperties;)Z' name='apply' primary='true'><SourceLine endBytecode='98' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' start='114' end='114' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3.apply(DatanodeAdminProperties)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='input' register='1'><Message>Parameter input</Message></LocalVariable><SourceLine synthetic='true' endBytecode='98' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3' start='114' end='114' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'><Message>At CombinedHostFileManager.java:[line 114]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3cc71f485118a1855317dc9dd020021' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>entry must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1' start='138' end='141' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 138-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1' signature='(Ljava/util/Map$Entry;)Z' name='apply' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1' start='141' end='141' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1.apply(Map$Entry)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='entry' register='1'><Message>Parameter entry</Message></LocalVariable><SourceLine synthetic='true' endBytecode='88' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1' start='141' end='141' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'><Message>At CombinedHostFileManager.java:[line 141]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e48a3891428ab8ed85646252c9eededb' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>input must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' start='154' end='159' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java'><Message>At CombinedHostFileManager.java:[lines 154-159]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' signature='(Lorg/apache/hadoop/hdfs/protocol/DatanodeAdminProperties;)Z' name='apply' primary='true'><SourceLine endBytecode='115' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' start='156' end='156' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6.apply(DatanodeAdminProperties)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='input' register='1'><Message>Parameter input</Message></LocalVariable><SourceLine synthetic='true' endBytecode='115' classname='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6' start='156' end='156' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' sourcefile='CombinedHostFileManager.java' startBytecode='0'><Message>At CombinedHostFileManager.java:[line 156]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='269611bb39b679375c5ab33890dd4aba' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap.getCorruptBlockIdsForTesting(BlockIdManager, BlockType, int, Long) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap' start='45' end='276' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java' sourcefile='CorruptReplicasMap.java'><Message>At CorruptReplicasMap.java:[lines 45-276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager;Lorg/apache/hadoop/hdfs/protocol/BlockType;ILjava/lang/Long;)[J' name='getCorruptBlockIdsForTesting' primary='true'><SourceLine endBytecode='234' classname='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap' start='220' end='225' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java' sourcefile='CorruptReplicasMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap.getCorruptBlockIdsForTesting(BlockIdManager, BlockType, int, Long)</Message></Method><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap' start='221' end='221' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java' sourcefile='CorruptReplicasMap.java' startBytecode='11' primary='true'><Message>At CorruptReplicasMap.java:[line 221]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5420ae33a93194b2715a2300071efe11' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.getLeaseRecoveryCommand(int) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' start='64' end='1066' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' sourcefile='DatanodeDescriptor.java'><Message>At DatanodeDescriptor.java:[lines 64-1066]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' signature='(I)[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='getLeaseRecoveryCommand' primary='true'><SourceLine endBytecode='139' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' start='715' end='718' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' sourcefile='DatanodeDescriptor.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.getLeaseRecoveryCommand(int)</Message></Method><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' start='717' end='717' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' sourcefile='DatanodeDescriptor.java' startBytecode='14' primary='true'><Message>At DatanodeDescriptor.java:[line 717]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a1cc3306ea40a1018de553ee6d8a75d' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.protocol.LocatedBlock to org.apache.hadoop.hdfs.protocol.LocatedStripedBlock in org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedStripedBlock(LocatedBlock, Comparator)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='71' end='1967' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java'><Message>At DatanodeManager.java:[lines 71-1967]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' signature='(Lorg/apache/hadoop/hdfs/protocol/LocatedBlock;Ljava/util/Comparator;)V' name='sortLocatedStripedBlock' primary='true'><SourceLine endBytecode='77' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='466' end='486' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedStripedBlock(LocatedBlock, Comparator)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/protocol/LocatedBlock;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LocatedBlock' start='81' end='279' sourcepath='org/apache/hadoop/hdfs/protocol/LocatedBlock.java' sourcefile='LocatedBlock.java'><Message>At LocatedBlock.java:[lines 81-279]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.protocol.LocatedBlock</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/LocatedStripedBlock;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LocatedStripedBlock' start='36' end='86' sourcepath='org/apache/hadoop/hdfs/protocol/LocatedStripedBlock.java' sourcefile='LocatedStripedBlock.java'><Message>At LocatedStripedBlock.java:[lines 36-86]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.LocatedStripedBlock</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='23' name='lb' register='1'><Message>Value loaded from lb</Message></LocalVariable><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='470' end='470' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java' startBytecode='24' primary='true'><Message>At DatanodeManager.java:[line 470]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='da5eb11f7241467ef79c829d7c5cee5f' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeID[], String[], String, Object[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='71' end='1967' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java'><Message>At DatanodeManager.java:[lines 71-1967]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' signature='([Lorg/apache/hadoop/hdfs/protocol/DatanodeID;[Ljava/lang/String;Ljava/lang/String;[Ljava/lang/Object;)[Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo;' name='getDatanodeStorageInfos' primary='true'><SourceLine endBytecode='445' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='652' end='678' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeID[], String[], String, Object[])</Message></Method><SourceLine endBytecode='123' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager' start='665' end='665' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' sourcefile='DatanodeManager.java' startBytecode='123' primary='true'><Message>At DatanodeManager.java:[line 665]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f885487a99d01b2bcddb388915fb04c5' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageIDs(DatanodeStorageInfo[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='40' end='408' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java'><Message>At DatanodeStorageInfo.java:[lines 40-408]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' signature='([Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo;)[Ljava/lang/String;' name='toStorageIDs' primary='true'><SourceLine endBytecode='141' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='64' end='71' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageIDs(DatanodeStorageInfo[])</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='65' end='65' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java' startBytecode='5' primary='true'><Message>At DatanodeStorageInfo.java:[line 65]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5efab4170d92852a7802bbd295bf2e98' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes(DatanodeStorageInfo[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='40' end='408' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java'><Message>At DatanodeStorageInfo.java:[lines 40-408]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' signature='([Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo;)[Lorg/apache/hadoop/fs/StorageType;' name='toStorageTypes' primary='true'><SourceLine endBytecode='141' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='75' end='82' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.toStorageTypes(DatanodeStorageInfo[])</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo' start='76' end='76' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' sourcefile='DatanodeStorageInfo.java' startBytecode='5' primary='true'><Message>At DatanodeStorageInfo.java:[line 76]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='82a42d738fbb202674101d5f4d3f091f' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor' start='430' end='466' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java' sourcefile='HeartbeatManager.java'><Message>At HeartbeatManager.java:[lines 430-466]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor' signature='()V' name='run' primary='true'><SourceLine endBytecode='439' classname='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor' start='436' end='466' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java' sourcefile='HeartbeatManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run()</Message></Method><SourceLine endBytecode='146' classname='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor' start='452' end='452' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java' sourcefile='HeartbeatManager.java' startBytecode='146' primary='true'><Message>At HeartbeatManager.java:[line 452]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b1b5125f9430c92e955ad71cbcc9a0d9' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.HostSet$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet$2'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet$2' start='105' end='109' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java' sourcefile='HostSet.java'><Message>At HostSet.java:[lines 105-109]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.HostSet$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet' start='43' end='112' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java' sourcefile='HostSet.java'><Message>At HostSet.java:[lines 43-112]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.HostSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet' signature='()Ljava/lang/String;' name='toString' primary='true'><SourceLine endBytecode='104' classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet' start='103' end='112' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java' sourcefile='HostSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.HostSet.toString()</Message></Method><SourceLine endBytecode='25' classname='org.apache.hadoop.hdfs.server.blockmanagement.HostSet' start='104' end='104' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java' sourcefile='HostSet.java' startBytecode='25' primary='true'><Message>At HostSet.java:[line 104]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a5b8d85b78d44ff183f44edd65a133ec' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks$1'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks$1' start='503' end='522' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java' sourcefile='LowRedundancyBlocks.java'><Message>At LowRedundancyBlocks.java:[lines 503-522]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks' start='67' end='503' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java' sourcefile='LowRedundancyBlocks.java'><Message>At LowRedundancyBlocks.java:[lines 67-503]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks' signature='()Ljava/util/Iterator;' name='iterator' primary='true'><SourceLine endBytecode='93' classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks' start='502' end='503' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java' sourcefile='LowRedundancyBlocks.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks.iterator()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks' start='503' end='503' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java' sourcefile='LowRedundancyBlocks.java' startBytecode='16' primary='true'><Message>At LowRedundancyBlocks.java:[line 503]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='aabbeade3b85c9f726f6eb6baeae4123' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor to org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor of return value in org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo.removeBlock(BlockInfo)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='459' end='490' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java'><Message>At ProvidedStorageMap.java:[lines 459-490]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' signature='(Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;)Z' name='removeBlock' primary='true'><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='464' end='468' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo.removeBlock(BlockInfo)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' start='64' end='1066' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' sourcefile='DatanodeDescriptor.java'><Message>At DatanodeDescriptor.java:[lines 64-1066]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap$ProvidedDescriptor;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor' start='312' end='449' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java'><Message>At ProvidedStorageMap.java:[lines 312-449]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor</Message></Type><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='464' end='464' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java' startBytecode='4' primary='true'><Message>At ProvidedStorageMap.java:[line 464]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8f662a60ba03e0630d96c7e11550bf00' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor to org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor of return value in org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo.setState(DatanodeStorage$State)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='459' end='490' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java'><Message>At ProvidedStorageMap.java:[lines 459-490]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeStorage$State;)V' name='setState' primary='true'><SourceLine endBytecode='22' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='474' end='486' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo.setState(DatanodeStorage$State)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor' start='64' end='1066' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' sourcefile='DatanodeDescriptor.java'><Message>At DatanodeDescriptor.java:[lines 64-1066]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap$ProvidedDescriptor;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor' start='312' end='449' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java'><Message>At ProvidedStorageMap.java:[lines 312-449]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor</Message></Type><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo' start='477' end='477' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' sourcefile='ProvidedStorageMap.java' startBytecode='11' primary='true'><Message>At ProvidedStorageMap.java:[line 477]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7278dff1c31bcab53f01f1f0ba9bc2d' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$2'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$2' start='227' end='231' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java' sourcefile='SlowDiskTracker.java'><Message>At SlowDiskTracker.java:[lines 227-231]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker' start='54' end='298' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java' sourcefile='SlowDiskTracker.java'><Message>At SlowDiskTracker.java:[lines 54-298]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker' signature='(Ljava/util/Map;IJ)Ljava/util/ArrayList;' name='getSlowDisks' primary='true'><SourceLine endBytecode='454' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker' start='221' end='254' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java' sourcefile='SlowDiskTracker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker.getSlowDisks(Map, int, long)</Message></Method><SourceLine endBytecode='35' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker' start='226' end='226' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java' sourcefile='SlowDiskTracker.java' startBytecode='35' primary='true'><Message>At SlowDiskTracker.java:[line 226]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d59355b15363fb50fd5bdce4b9b879e5' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$1'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$1' start='245' end='249' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java' sourcefile='SlowPeerTracker.java'><Message>At SlowPeerTracker.java:[lines 245-249]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker' start='57' end='275' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java' sourcefile='SlowPeerTracker.java'><Message>At SlowPeerTracker.java:[lines 57-275]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker' signature='(I)Ljava/util/Collection;' name='getJsonReports' primary='true'><SourceLine endBytecode='439' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker' start='239' end='270' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java' sourcefile='SlowPeerTracker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker.getJsonReports(int)</Message></Method><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker' start='244' end='244' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java' sourcefile='SlowPeerTracker.java' startBytecode='34' primary='true'><Message>At SlowPeerTracker.java:[line 244]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cb37691db8bddfb9286c8e0048bbca85' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_MUTABLE_ARRAY' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable array</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants.RESERVED_PATH_COMPONENTS is a mutable array</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' start='61' end='79' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 61-79]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' signature='[Ljava/lang/String;' name='RESERVED_PATH_COMPONENTS' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants.RESERVED_PATH_COMPONENTS</Message></Field><SourceLine endBytecode='26' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' start='74' end='74' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='26' primary='true'><Message>At HdfsServerConstants.java:[line 74]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3ee3347f5b7b3ede769567dd1f169057' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_MUTABLE_ARRAY' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable array</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants.DOT_SNAPSHOT_DIR_BYTES is a mutable array</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' start='61' end='79' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 61-79]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' signature='[B' name='DOT_SNAPSHOT_DIR_BYTES' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants.DOT_SNAPSHOT_DIR_BYTES</Message></Field><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants' start='79' end='79' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='34' primary='true'><Message>At HdfsServerConstants.java:[line 79]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cd7de9d38986c1b766dbb7acc0e5a2ee' rank='20' abbrev='SA' category='STYLE' priority='3' type='SA_FIELD_DOUBLE_ASSIGNMENT' instanceOccurrenceMax='0'><ShortMessage>Double assignment of field</ShortMessage><LongMessage>Double assignment of field HdfsServerConstants$NamenodeRole.description in new org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole(String, int, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' start='265' end='275' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 265-275]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' signature='(Ljava/lang/String;ILjava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='76' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' start='271' end='271' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole(String, int, String)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' signature='Ljava/lang/String;' name='description' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole.description</Message></Field><SourceLine endBytecode='13' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole' start='271' end='271' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='13' primary='true'><Message>At HdfsServerConstants.java:[line 271]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a8d92ad910ff78ff323419e89f1e0335' rank='16' abbrev='ME' category='BAD_PRACTICE' priority='2' type='ME_ENUM_FIELD_SETTER' instanceOccurrenceMax='0'><ShortMessage>Public enum method unconditionally sets its field</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setClusterId(String) unconditionally sets the field clusterId</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='(Ljava/lang/String;)V' name='setClusterId' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='193' end='194' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setClusterId(String)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='Ljava/lang/String;' name='clusterId' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.clusterId</Message></Field><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='193' end='193' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='2' primary='true'><Message>At HdfsServerConstants.java:[line 193]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8df4cf15f7351db5a4415ab4da0af1c2' rank='16' abbrev='ME' category='BAD_PRACTICE' priority='2' type='ME_ENUM_FIELD_SETTER' instanceOccurrenceMax='0'><ShortMessage>Public enum method unconditionally sets its field</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setForce(int) unconditionally sets the field force</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='(I)V' name='setForce' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='217' end='218' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setForce(int)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='I' name='force' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.force</Message></Field><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='217' end='217' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='2' primary='true'><Message>At HdfsServerConstants.java:[line 217]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4be340589278f4f4f56cb765c9dd84b3' rank='16' abbrev='ME' category='BAD_PRACTICE' priority='2' type='ME_ENUM_FIELD_SETTER' instanceOccurrenceMax='0'><ShortMessage>Public enum method unconditionally sets its field</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setForceFormat(boolean) unconditionally sets the field isForceFormat</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='(Z)V' name='setForceFormat' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='229' end='230' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setForceFormat(boolean)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='Z' name='isForceFormat' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.isForceFormat</Message></Field><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='229' end='229' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='2' primary='true'><Message>At HdfsServerConstants.java:[line 229]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cd5ad8da481005ce94196b05a9afcd97' rank='16' abbrev='ME' category='BAD_PRACTICE' priority='2' type='ME_ENUM_FIELD_SETTER' instanceOccurrenceMax='0'><ShortMessage>Public enum method unconditionally sets its field</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setInteractiveFormat(boolean) unconditionally sets the field isInteractiveFormat</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='(Z)V' name='setInteractiveFormat' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='237' end='238' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setInteractiveFormat(boolean)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='Z' name='isInteractiveFormat' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>In HdfsServerConstants.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.isInteractiveFormat</Message></Field><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='237' end='237' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='2' primary='true'><Message>At HdfsServerConstants.java:[line 237]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b49f1741b32e22e7ec138c8d06e131c2' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setClusterId(String) and org.apache.hadoop.hdfs.server.namenode.NNStorage.setClusterID(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='135' end='257' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java'><Message>At HdfsServerConstants.java:[lines 135-257]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' signature='(Ljava/lang/String;)V' name='setClusterId' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='193' end='194' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption.setClusterId(String)</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.NNStorage'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNStorage' start='66' end='1168' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorage.java' sourcefile='NNStorage.java'><Message>At NNStorage.java:[lines 66-1168]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NNStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NNStorage' signature='(Ljava/lang/String;)V' name='setClusterID'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.NNStorage' start='971' end='972' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorage.java' sourcefile='NNStorage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NNStorage.setClusterID(String)</Message></Method><SourceLine synthetic='true' endBytecode='61' classname='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption' start='193' end='194' sourcepath='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' sourcefile='HdfsServerConstants.java' startBytecode='0'><Message>At HdfsServerConstants.java:[lines 193-194]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='13429a2481ec47618c73b2d4f3a9442e' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.common.Storage.LAYOUT_VERSIONS_203 should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.Storage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.Storage' start='77' end='1389' sourcepath='org/apache/hadoop/hdfs/server/common/Storage.java' sourcefile='Storage.java'><Message>At Storage.java:[lines 77-1389]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.Storage</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.common.Storage' signature='[I' name='LAYOUT_VERSIONS_203' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.Storage' sourcepath='org/apache/hadoop/hdfs/server/common/Storage.java' sourcefile='Storage.java'><Message>In Storage.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.Storage.LAYOUT_VERSIONS_203</Message></Field><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.server.common.Storage' start='90' end='90' sourcepath='org/apache/hadoop/hdfs/server/common/Storage.java' sourcefile='Storage.java' startBytecode='24' primary='true'><Message>At Storage.java:[line 90]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d8b1b57dad9567ccb38e11492b4977b6' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>InMemoryLevelDBAliasMapClient.aliasMap not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.close()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='44' end='176' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>At InMemoryLevelDBAliasMapClient.java:[lines 44-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='Lorg/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB;' name='aliasMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>In InMemoryLevelDBAliasMapClient.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.aliasMap</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='()V' name='close' primary='true'><SourceLine endBytecode='53' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='53' end='54' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.close()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='53' end='53' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='4' primary='true'><Message>At InMemoryLevelDBAliasMapClient.java:[line 53]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5d50cf03c3e8877f027854747512b7ba' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>InMemoryLevelDBAliasMapClient.aliasMap not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.getReader(BlockAliasMap$Reader$Options, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='44' end='176' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>At InMemoryLevelDBAliasMapClient.java:[lines 44-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='Lorg/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB;' name='aliasMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>In InMemoryLevelDBAliasMapClient.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.aliasMap</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='(Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap$Reader$Options;Ljava/lang/String;)Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap$Reader;' name='getReader' primary='true'><SourceLine endBytecode='142' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='139' end='148' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.getReader(BlockAliasMap$Reader$Options, String)</Message></Method><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='140' end='140' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='12' primary='true'><Message>At InMemoryLevelDBAliasMapClient.java:[line 140]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5c8c1430bdfecbb06690ab7d67c8bf08' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>InMemoryLevelDBAliasMapClient.aliasMap not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.getWriter(BlockAliasMap$Writer$Options, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='44' end='176' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>At InMemoryLevelDBAliasMapClient.java:[lines 44-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='Lorg/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB;' name='aliasMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java'><Message>In InMemoryLevelDBAliasMapClient.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.aliasMap</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' signature='(Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap$Writer$Options;Ljava/lang/String;)Lorg/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap$Writer;' name='getWriter' primary='true'><SourceLine endBytecode='131' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='154' end='160' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient.getWriter(BlockAliasMap$Writer$Options, String)</Message></Method><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient' start='155' end='155' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' sourcefile='InMemoryLevelDBAliasMapClient.java' startBytecode='12' primary='true'><Message>At InMemoryLevelDBAliasMapClient.java:[line 155]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4e54a263c32eaf7d03e7f72c113ed08' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader.blockPoolID</LongMessage><Class classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader' start='259' end='412' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java' sourcefile='TextFileRegionAliasMap.java'><Message>At TextFileRegionAliasMap.java:[lines 259-412]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader' signature='Ljava/lang/String;' name='blockPoolID' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java' sourcefile='TextFileRegionAliasMap.java'><Message>In TextFileRegionAliasMap.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader.blockPoolID</Message></Field><SourceLine endBytecode='39' classname='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader' start='293' end='293' sourcepath='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java' sourcefile='TextFileRegionAliasMap.java' startBytecode='39' primary='true'><Message>At TextFileRegionAliasMap.java:[line 293]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ad575148216d4a5b49903c2473ea3483' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.datanode.BPOfferService.getDataNode() and org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer.getDatanode()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BPOfferService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BPOfferService' start='56' end='846' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPOfferService.java' sourcefile='BPOfferService.java'><Message>At BPOfferService.java:[lines 56-846]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BPOfferService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BPOfferService' signature='()Lorg/apache/hadoop/hdfs/server/datanode/DataNode;' name='getDataNode' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.BPOfferService' start='362' end='362' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPOfferService.java' sourcefile='BPOfferService.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BPOfferService.getDataNode()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer' start='82' end='151' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java'><Message>At BlockChecksumHelper.java:[lines 82-151]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer' signature='()Lorg/apache/hadoop/hdfs/server/datanode/DataNode;' name='getDatanode'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer' start='102' end='102' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer.getDatanode()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.BPOfferService' start='362' end='362' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPOfferService.java' sourcefile='BPOfferService.java' startBytecode='0'><Message>At BPOfferService.java:[line 362]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='591f16070032c10c9b2130182bbee8e1' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BPServiceActor$LifelineSender.lifelineNamenode not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender.sendLifeline()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' start='956' end='1082' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java' sourcefile='BPServiceActor.java'><Message>At BPServiceActor.java:[lines 956-1082]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' signature='Lorg/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolClientSideTranslatorPB;' name='lifelineNamenode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java' sourcefile='BPServiceActor.java'><Message>In BPServiceActor.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender.lifelineNamenode</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' signature='()V' name='sendLifeline' primary='true'><SourceLine endBytecode='324' classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' start='1064' end='1082' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java' sourcefile='BPServiceActor.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender.sendLifeline()</Message></Method><SourceLine endBytecode='171' classname='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender' start='1074' end='1074' sourcepath='org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java' sourcefile='BPServiceActor.java' startBytecode='171' primary='true'><Message>At BPServiceActor.java:[line 1074]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7d0c60a7fae7c6a9c35f2d6cd12b97a2' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer.crcPartialBlock() return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='175' end='277' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java'><Message>At BlockChecksumHelper.java:[lines 175-277]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' signature='()[B' name='crcPartialBlock' primary='true'><SourceLine endBytecode='275' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='260' end='277' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer.crcPartialBlock()</Message></Method><SourceLine endBytecode='93' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='277' end='277' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='93' primary='true'><Message>At BlockChecksumHelper.java:[line 277]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='17e7a054bf07fd62f8118818f9bbce19' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>BlockChecksumHelper$BlockChecksumComputer.checksum not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer.crcPartialBlock()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='175' end='277' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java'><Message>At BlockChecksumHelper.java:[lines 175-277]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' signature='Lorg/apache/hadoop/util/DataChecksum;' name='checksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java'><Message>In BlockChecksumHelper.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer.checksum</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' signature='()[B' name='crcPartialBlock' primary='true'><SourceLine endBytecode='275' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='260' end='277' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer.crcPartialBlock()</Message></Method><SourceLine endBytecode='66' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='271' end='271' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='66' primary='true'><Message>At BlockChecksumHelper.java:[line 271]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='85' classname='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer' start='273' end='273' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' sourcefile='BlockChecksumHelper.java' startBytecode='85'><Message>Another occurrence at BlockChecksumHelper.java:[line 273]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fb97393815ec2751b4be4b02b7b92736' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2' start='270' end='274' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java'><Message>At BlockPoolManager.java:[lines 270-274]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='44' end='293' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java'><Message>At BlockPoolManager.java:[lines 44-293]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockPoolManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' signature='(Ljava/util/Map;Ljava/util/Map;)V' name='doRefreshNamenodes' primary='true'><SourceLine endBytecode='2233' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='178' end='284' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(Map, Map)</Message></Method><SourceLine endBytecode='873' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='269' end='269' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java' startBytecode='873' primary='true'><Message>At BlockPoolManager.java:[line 269]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a5bd72d4a0dda776c39746d9f1cfd6ba' rank='18' abbrev='WMI' category='PERFORMANCE' priority='2' type='WMI_WRONG_MAP_ITERATOR' instanceOccurrenceMax='0'><ShortMessage>Inefficient use of keySet iterator instead of entrySet iterator</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(Map, Map) makes inefficient use of keySet iterator instead of entrySet iterator</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='44' end='293' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java'><Message>At BlockPoolManager.java:[lines 44-293]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockPoolManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' signature='(Ljava/util/Map;Ljava/util/Map;)V' name='doRefreshNamenodes' primary='true'><SourceLine endBytecode='2233' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='178' end='284' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.doRefreshNamenodes(Map, Map)</Message></Method><SourceLine endBytecode='424' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='222' end='222' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java' startBytecode='424' primary='true'><Message>At BlockPoolManager.java:[line 222]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='820' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager' start='264' end='264' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' sourcefile='BlockPoolManager.java' startBytecode='820'><Message>Another occurrence at BlockPoolManager.java:[line 264]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ae99c318acc91efb15e3ca923188170e' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$2' start='662' end='675' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>At BlockPoolSliceStorage.java:[lines 662-675]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='69' end='896' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>At BlockPoolSliceStorage.java:[lines 69-896]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' signature='(Ljava/io/File;)V' name='doFinalize' primary='true'><SourceLine endBytecode='344' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='641' end='678' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doFinalize(File)</Message></Method><SourceLine endBytecode='138' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='662' end='662' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='138' primary='true'><Message>At BlockPoolSliceStorage.java:[line 662]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e53d14a1c915e215cb3628d11dd66f1b' rank='20' abbrev='ST' category='STYLE' priority='3' type='ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD' instanceOccurrenceMax='0'><ShortMessage>Write to static field from instance method</ShortMessage><LongMessage>Write to static field org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.storagesWithRollingUpgradeMarker from instance method new org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage(int, String, long, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='69' end='896' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>At BlockPoolSliceStorage.java:[lines 69-896]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' signature='(ILjava/lang/String;JLjava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='174' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='116' end='125' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage(int, String, long, String)</Message></Method><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' signature='Ljava/util/Set;' name='storagesWithRollingUpgradeMarker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>In BlockPoolSliceStorage.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.storagesWithRollingUpgradeMarker</Message></Field><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='121' end='121' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='44' primary='true'><Message>At BlockPoolSliceStorage.java:[line 121]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8f7564dbe5e2114ee1321de166f552c0' rank='20' abbrev='ST' category='STYLE' priority='3' type='ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD' instanceOccurrenceMax='0'><ShortMessage>Write to static field from instance method</ShortMessage><LongMessage>Write to static field org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.storagesWithoutRollingUpgradeMarker from instance method new org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage(int, String, long, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='69' end='896' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>At BlockPoolSliceStorage.java:[lines 69-896]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' signature='(ILjava/lang/String;JLjava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='174' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='116' end='125' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage(int, String, long, String)</Message></Method><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' signature='Ljava/util/Set;' name='storagesWithoutRollingUpgradeMarker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java'><Message>In BlockPoolSliceStorage.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.storagesWithoutRollingUpgradeMarker</Message></Field><SourceLine endBytecode='57' classname='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage' start='123' end='123' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' sourcefile='BlockPoolSliceStorage.java' startBytecode='57' primary='true'><Message>At BlockPoolSliceStorage.java:[line 123]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b96746be66f36be7c4803c6aeb2907d4' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(PipelineAck, long, long, long, int)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1213' end='1668' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java'><Message>At BlockReceiver.java:[lines 1213-1668]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' signature='(Lorg/apache/hadoop/hdfs/protocol/datatransfer/PipelineAck;JJJI)V' name='sendAckUpstream' primary='true'><SourceLine endBytecode='600' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1559' end='1581' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(PipelineAck, long, long, long, int)</Message></Method><SourceLine endBytecode='130' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1573' end='1573' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='130' primary='true'><Message>At BlockReceiver.java:[line 1573]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='61db93200d109a002edcb9938263a316' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendOOBResponse(DataTransferProtos$Status)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1213' end='1668' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java'><Message>At BlockReceiver.java:[lines 1213-1668]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' signature='(Lorg/apache/hadoop/hdfs/protocol/proto/DataTransferProtos$Status;)V' name='sendOOBResponse' primary='true'><SourceLine endBytecode='528' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1289' end='1320' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendOOBResponse(DataTransferProtos$Status)</Message></Method><SourceLine endBytecode='207' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1317' end='1317' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='207' primary='true'><Message>At BlockReceiver.java:[line 1317]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e1eba832de244b76c3e004454a62bf7' rank='17' abbrev='Wa' category='MT_CORRECTNESS' priority='3' type='WA_NOT_IN_LOOP' instanceOccurrenceMax='0'><ShortMessage>Wait not in loop </ShortMessage><LongMessage>Wait not in loop in org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendOOBResponse(DataTransferProtos$Status)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1213' end='1668' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java'><Message>At BlockReceiver.java:[lines 1213-1668]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' signature='(Lorg/apache/hadoop/hdfs/protocol/proto/DataTransferProtos$Status;)V' name='sendOOBResponse' primary='true'><SourceLine endBytecode='528' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1289' end='1320' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendOOBResponse(DataTransferProtos$Status)</Message></Method><SourceLine endBytecode='63' classname='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder' start='1297' end='1297' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' sourcefile='BlockReceiver.java' startBytecode='63' primary='true'><Message>At BlockReceiver.java:[line 1297]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ed39571124fa8128dfdd83cebdf89bff' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock to org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock of return value in org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1' start='593' end='608' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java' sourcefile='BlockRecoveryWorker.java'><Message>At BlockRecoveryWorker.java:[lines 593-608]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1' signature='()V' name='run' primary='true'><SourceLine endBytecode='48' classname='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1' start='596' end='608' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java' sourcefile='BlockRecoveryWorker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run()</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand$RecoveringBlock;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock' start='65' end='99' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java'><Message>At BlockRecoveryCommand.java:[lines 65-99]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand$RecoveringStripedBlock;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' start='109' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java'><Message>At BlockRecoveryCommand.java:[lines 109-124]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='52' name='b' register='2'><Message>Value loaded from b</Message></LocalVariable><SourceLine endBytecode='53' classname='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1' start='600' end='600' sourcepath='org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java' sourcefile='BlockRecoveryWorker.java' startBytecode='53' primary='true'><Message>At BlockRecoveryWorker.java:[line 600]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3db1c36939b9874c18b68d1874721fe' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.ChunkChecksum.getChecksum() may expose internal representation by returning ChunkChecksum.checksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='34' end='44' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java'><Message>At ChunkChecksum.java:[lines 34-44]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.ChunkChecksum</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' signature='()[B' name='getChecksum' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='44' end='44' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.ChunkChecksum.getChecksum()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' signature='[B' name='checksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java'><Message>In ChunkChecksum.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.ChunkChecksum.checksum</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='44' end='44' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java' startBytecode='4' primary='true'><Message>At ChunkChecksum.java:[line 44]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c3fe8efd9465668c1157e8d0375f00d5' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.datanode.ChunkChecksum(long, byte[]) may expose internal representation by storing an externally mutable object into ChunkChecksum.checksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='34' end='44' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java'><Message>At ChunkChecksum.java:[lines 34-44]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.ChunkChecksum</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' signature='(J[B)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='34' end='37' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.ChunkChecksum(long, byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' signature='[B' name='checksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java'><Message>In ChunkChecksum.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.ChunkChecksum.checksum</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='checksum' register='3'><Message>Local variable named checksum</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum' start='36' end='36' sourcepath='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' sourcefile='ChunkChecksum.java' startBytecode='11' primary='true'><Message>At ChunkChecksum.java:[line 36]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='eff6cb601505c8749efea7a0784c5f8e' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.datanode.DataNode.directoryScanner; locked 83% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='262' end='3607' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 262-3607]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNode</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' signature='Lorg/apache/hadoop/hdfs/server/datanode/DirectoryScanner;' name='directoryScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>In DataNode.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.DataNode.directoryScanner</Message></Field><Int role='INT_SYNC_PERCENT' value='83'><Message>Synchronized 83% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='2851' end='2851' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='1' primary='true'><Message>Unsynchronized access at DataNode.java:[line 2851]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1096' end='1096' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='1'><Message>Synchronized access at DataNode.java:[line 1096]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='8' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1097' end='1097' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='8'><Message>Synchronized access at DataNode.java:[line 1097]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1075' end='1075' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='1'><Message>Synchronized access at DataNode.java:[line 1075]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='76' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1086' end='1086' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='76'><Message>Synchronized access at DataNode.java:[line 1086]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='80' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1087' end='1087' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='80'><Message>Synchronized access at DataNode.java:[line 1087]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='49c108db445b4ea6a69700c51bc9a5c6' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.datanode.DataNode.id; locked 50% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='262' end='3607' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 262-3607]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNode</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' signature='Lorg/apache/hadoop/hdfs/protocol/DatanodeID;' name='id' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>In DataNode.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.DataNode.id</Message></Field><Int role='INT_SYNC_PERCENT' value='50'><Message>Synchronized 50% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='3293' end='3293' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='1' primary='true'><Message>Unsynchronized access at DataNode.java:[line 3293]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='2' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='1567' end='1567' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='2'><Message>Synchronized access at DataNode.java:[line 1567]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='175e80505aa1ca51f200b2e7104b5807' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.datanode.DataNode.getDatanodeUuid() and org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode.getDataNodeUUID()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='262' end='3607' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 262-3607]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' signature='()Ljava/lang/String;' name='getDatanodeUuid' primary='true'><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='3313' end='3313' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DataNode.getDatanodeUuid()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' start='30' end='266' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java' sourcefile='DiskBalancerDataNode.java'><Message>At DiskBalancerDataNode.java:[lines 30-266]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' signature='()Ljava/lang/String;' name='getDataNodeUUID'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' start='122' end='122' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java' sourcefile='DiskBalancerDataNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode.getDataNodeUUID()</Message></Method><SourceLine synthetic='true' endBytecode='73' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='3313' end='3313' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='0'><Message>At DataNode.java:[line 3313]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3e0c1036d82df29b87875a702a752296' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.DataNode$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.DataNode$1'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode$1' start='508' end='513' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 508-513]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.DataNode$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='262' end='3607' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 262-3607]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/util/List;Lorg/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker;Lorg/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter$SecureResources;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='897' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='444' end='519' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.DataNode(Configuration, List, StorageLocationChecker, SecureDataNodeStarter$SecureResources)</Message></Method><SourceLine endBytecode='446' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='507' end='507' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='446' primary='true'><Message>At DataNode.java:[line 507]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='80ba8f2fc5a6facf8b6cf0123f70d1e' rank='17' abbrev='UW' category='MT_CORRECTNESS' priority='3' type='UW_UNCOND_WAIT' instanceOccurrenceMax='0'><ShortMessage>Unconditional wait</ShortMessage><LongMessage>Unconditional wait in org.apache.hadoop.hdfs.server.datanode.DataNode.join()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='262' end='3607' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java'><Message>At DataNode.java:[lines 262-3607]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' signature='()V' name='join' primary='true'><SourceLine endBytecode='231' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='2727' end='2742' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DataNode.join()</Message></Method><SourceLine endBytecode='42' classname='org.apache.hadoop.hdfs.server.datanode.DataNode' start='2736' end='2736' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNode.java' sourcefile='DataNode.java' startBytecode='42' primary='true'><Message>At DataNode.java:[line 2736]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6c118d8f2d77e093fd6e8151c336c90d' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_MUTABLE_COLLECTION' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable collection</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion.FEATURES is a mutable collection</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion' start='30' end='48' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.java' sourcefile='DataNodeLayoutVersion.java'><Message>At DataNodeLayoutVersion.java:[lines 30-48]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion' signature='Ljava/util/Map;' name='FEATURES' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.java' sourcefile='DataNodeLayoutVersion.java'><Message>In DataNodeLayoutVersion.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion.FEATURES</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion' start='32' end='32' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.java' sourcefile='DataNodeLayoutVersion.java' startBytecode='7' primary='true'><Message>At DataNodeLayoutVersion.java:[line 32]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='baf7959a3cb34535286e5542e0fef6ea' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.datanode.DataStorage.setDatanodeUuid(String) and org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode.setDataNodeUUID(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='75' end='1389' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java'><Message>At DataStorage.java:[lines 75-1389]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' signature='(Ljava/lang/String;)V' name='setDatanodeUuid' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='130' end='131' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DataStorage.setDatanodeUuid(String)</Message></Method><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' start='30' end='266' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java' sourcefile='DiskBalancerDataNode.java'><Message>At DiskBalancerDataNode.java:[lines 30-266]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' signature='(Ljava/lang/String;)V' name='setDataNodeUUID'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode' start='131' end='132' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java' sourcefile='DiskBalancerDataNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode.setDataNodeUUID(String)</Message></Method><SourceLine synthetic='true' endBytecode='61' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='130' end='131' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java' startBytecode='0'><Message>At DataStorage.java:[lines 130-131]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ff4b731d1263629b3e2465a1539d4858' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.DataStorage$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataStorage$2' start='974' end='988' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java'><Message>At DataStorage.java:[lines 974-988]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.DataStorage$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='75' end='1389' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java'><Message>At DataStorage.java:[lines 75-1389]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DataStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' signature='(Lorg/apache/hadoop/hdfs/server/common/Storage$StorageDirectory;)V' name='doFinalize' primary='true'><SourceLine endBytecode='309' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='957' end='990' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DataStorage.doFinalize(Storage$StorageDirectory)</Message></Method><SourceLine endBytecode='128' classname='org.apache.hadoop.hdfs.server.datanode.DataStorage' start='974' end='974' sourcepath='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' sourcefile='DataStorage.java' startBytecode='128' primary='true'><Message>At DataStorage.java:[line 974]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5507edc43baa0c5a62f801ec586d8af' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from Throwable to java.io.IOException of return value in org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.getCauseIfDiskError(IOException)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil' start='34' end='143' sourcepath='org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java' sourcefile='DatanodeUtil.java'><Message>At DatanodeUtil.java:[lines 34-143]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DatanodeUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil' signature='(Ljava/io/IOException;)Ljava/io/IOException;' name='getCauseIfDiskError' primary='true'><SourceLine endBytecode='13' classname='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil' start='47' end='50' sourcepath='org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java' sourcefile='DatanodeUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.getCauseIfDiskError(IOException)</Message></Method><Type role='TYPE_FOUND' descriptor='Ljava/lang/Throwable;'><SourceLine classname='java.lang.Throwable' start='115' end='1108' sourcepath='java/lang/Throwable.java' sourcefile='Throwable.java'><Message>At Throwable.java:[lines 115-1108]</Message></SourceLine><Message>Actual type Throwable</Message></Type><Type role='TYPE_EXPECTED' descriptor='Ljava/io/IOException;'><SourceLine classname='java.io.IOException' start='47' end='100' sourcepath='java/io/IOException.java' sourcefile='IOException.java'><Message>At IOException.java:[lines 47-100]</Message></SourceLine><Message>Expected java.io.IOException</Message></Type><SourceLine endBytecode='23' classname='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil' start='48' end='48' sourcepath='org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java' sourcefile='DatanodeUtil.java' startBytecode='23' primary='true'><Message>At DatanodeUtil.java:[line 48]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a193e7a493d88d73e16d4fb94d5c2caf' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in new org.apache.hadoop.hdfs.server.datanode.DirectoryScanner(DataNode, FsDatasetSpi, Configuration)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='62' end='574' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java'><Message>At DirectoryScanner.java:[lines 62-574]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DirectoryScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' signature='(Lorg/apache/hadoop/hdfs/server/datanode/DataNode;Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi;Lorg/apache/hadoop/conf/Configuration;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='456' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='220' end='261' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.DirectoryScanner(DataNode, FsDatasetSpi, Configuration)</Message></Method><SourceLine endBytecode='92' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='227' end='227' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java' startBytecode='92' primary='true'><Message>At DirectoryScanner.java:[line 227]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='552fbde2f6fff010cdbcc5735155a7fd' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.start()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='62' end='574' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java'><Message>At DirectoryScanner.java:[lines 62-574]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DirectoryScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' signature='()V' name='start' primary='true'><SourceLine endBytecode='299' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='268' end='286' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.start()</Message></Method><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner' start='269' end='269' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java' startBytecode='24' primary='true'><Message>At DirectoryScanner.java:[line 269]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='25422d27a43acecf28c3c697a00e37f8' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.datanode</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler' start='586' end='668' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java'><Message>At DirectoryScanner.java:[lines 586-668]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler' signature='Lorg/apache/hadoop/hdfs/server/datanode/DataNode;' name='datanode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java'><Message>In DirectoryScanner.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.datanode</Message></Field><SourceLine endBytecode='33' classname='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler' start='602' end='602' sourcepath='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' sourcefile='DirectoryScanner.java' startBytecode='33' primary='true'><Message>At DirectoryScanner.java:[line 602]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b9680165a5c65d686e5d02c77ae7d81' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents.isEnabled</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents' start='38' end='54' sourcepath='org/apache/hadoop/hdfs/server/datanode/FaultInjectorFileIoEvents.java' sourcefile='FaultInjectorFileIoEvents.java'><Message>At FaultInjectorFileIoEvents.java:[lines 38-54]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents' signature='Z' name='isEnabled' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents' sourcepath='org/apache/hadoop/hdfs/server/datanode/FaultInjectorFileIoEvents.java' sourcefile='FaultInjectorFileIoEvents.java'><Message>In FaultInjectorFileIoEvents.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents.isEnabled</Message></Field><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents' start='40' end='40' sourcepath='org/apache/hadoop/hdfs/server/datanode/FaultInjectorFileIoEvents.java' sourcefile='FaultInjectorFileIoEvents.java' startBytecode='16' primary='true'><Message>At FaultInjectorFileIoEvents.java:[line 40]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9f367b1b9c18adf131751d5428c686d5' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.getLastPartialChunkChecksum() may expose internal representation by returning FinalizedReplica.lastPartialChunkChecksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='43' end='160' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java'><Message>At FinalizedReplica.java:[lines 43-160]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.FinalizedReplica</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' signature='()[B' name='getLastPartialChunkChecksum' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='148' end='148' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.getLastPartialChunkChecksum()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' signature='[B' name='lastPartialChunkChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java'><Message>In FinalizedReplica.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.lastPartialChunkChecksum</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='148' end='148' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java' startBytecode='4' primary='true'><Message>At FinalizedReplica.java:[line 148]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2d84491a949586bf2f080a1fbf636c30' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.setLastPartialChunkChecksum(byte[]) may expose internal representation by storing an externally mutable object into FinalizedReplica.lastPartialChunkChecksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='43' end='160' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java'><Message>At FinalizedReplica.java:[lines 43-160]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.FinalizedReplica</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' signature='([B)V' name='setLastPartialChunkChecksum' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='152' end='153' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.setLastPartialChunkChecksum(byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' signature='[B' name='lastPartialChunkChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java'><Message>In FinalizedReplica.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.FinalizedReplica.lastPartialChunkChecksum</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='2' name='checksum' register='1'><Message>Local variable named checksum</Message></LocalVariable><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='152' end='152' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java' startBytecode='2' primary='true'><Message>At FinalizedReplica.java:[line 152]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9fa2bb9c7abd8c462e1aaccf7bf021b' rank='17' abbrev='Wa' category='MT_CORRECTNESS' priority='3' type='WA_NOT_IN_LOOP' instanceOccurrenceMax='0'><ShortMessage>Wait not in loop </ShortMessage><LongMessage>Wait not in loop in org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(long)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager' start='48' end='301' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java'><Message>At IncrementalBlockReportManager.java:[lines 48-301]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager' signature='(J)V' name='waitTillNextIBR' primary='true'><SourceLine endBytecode='202' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager' start='156' end='163' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(long)</Message></Method><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager' start='158' end='158' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java' startBytecode='40' primary='true'><Message>At IncrementalBlockReportManager.java:[line 158]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f50c8d1493c93c78f51221be12483dde' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR.removeAll() return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR' start='53' end='118' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java'><Message>At IncrementalBlockReportManager.java:[lines 53-118]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR' signature='()[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;' name='removeAll' primary='true'><SourceLine endBytecode='142' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR' start='70' end='78' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR.removeAll()</Message></Method><SourceLine endBytecode='15' classname='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR' start='72' end='72' sourcepath='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' sourcefile='IncrementalBlockReportManager.java' startBytecode='15' primary='true'><Message>At IncrementalBlockReportManager.java:[line 72]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f6d1949a66c2cde0e86bd6d9f3216bba' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of file, which is known to be non-null in org.apache.hadoop.hdfs.server.datanode.LocalReplica.breakHardLinksIfNeeded()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' start='65' end='527' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' sourcefile='LocalReplica.java'><Message>At LocalReplica.java:[lines 65-527]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.LocalReplica</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' signature='()Z' name='breakHardLinksIfNeeded' primary='true'><SourceLine endBytecode='285' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' start='230' end='246' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' sourcefile='LocalReplica.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.LocalReplica.breakHardLinksIfNeeded()</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='10' name='file' register='1'><Message>Value loaded from file</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' signature='()Ljava/io/File;' name='getBlockFile'><SourceLine endBytecode='57' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' start='108' end='108' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' sourcefile='LocalReplica.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.datanode.LocalReplica.getBlockFile() of type java.io.File</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='11' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' start='232' end='232' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' sourcefile='LocalReplica.java' startBytecode='11' primary='true'><Message>Redundant null check at LocalReplica.java:[line 232]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8bffe06d737b554336c16ca63f00ea79' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline.setLastChecksumAndDataLen(long, byte[]) may expose internal representation by storing an externally mutable object into LocalReplicaInPipeline.lastChecksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' start='49' end='410' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java' sourcefile='LocalReplicaInPipeline.java'><Message>At LocalReplicaInPipeline.java:[lines 49-410]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' signature='(J[B)V' name='setLastChecksumAndDataLen' primary='true'><SourceLine endBytecode='80' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' start='173' end='175' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java' sourcefile='LocalReplicaInPipeline.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline.setLastChecksumAndDataLen(long, byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' signature='[B' name='lastChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java' sourcefile='LocalReplicaInPipeline.java'><Message>In LocalReplicaInPipeline.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline.lastChecksum</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='7' name='checksum' register='3'><Message>Local variable named checksum</Message></LocalVariable><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline' start='174' end='174' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java' sourcefile='LocalReplicaInPipeline.java' startBytecode='7' primary='true'><Message>At LocalReplicaInPipeline.java:[line 174]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d7593632d92f10bb857e01730c3a968f' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder.setLastPartialChunkChecksum(byte[]) may expose internal representation by storing an externally mutable object into ReplicaBuilder.lastPartialChunkChecksum</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' start='62' end='388' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java' sourcefile='ReplicaBuilder.java'><Message>At ReplicaBuilder.java:[lines 62-388]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' signature='([B)Lorg/apache/hadoop/hdfs/server/datanode/ReplicaBuilder;' name='setLastPartialChunkChecksum' primary='true'><SourceLine endBytecode='62' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' start='183' end='184' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java' sourcefile='ReplicaBuilder.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder.setLastPartialChunkChecksum(byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' signature='[B' name='lastPartialChunkChecksum' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java' sourcefile='ReplicaBuilder.java'><Message>In ReplicaBuilder.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder.lastPartialChunkChecksum</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='2' name='checksum' register='1'><Message>Local variable named checksum</Message></LocalVariable><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder' start='183' end='183' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java' sourcefile='ReplicaBuilder.java' startBytecode='2' primary='true'><Message>At ReplicaBuilder.java:[line 183]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='50a70731755334e5c4286c27c54e26c0' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.datanode.ReplicaInfo to org.apache.hadoop.hdfs.server.datanode.LocalReplica in new org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery(ReplicaInfo, long)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' start='38' end='148' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java' sourcefile='ReplicaUnderRecovery.java'><Message>At ReplicaUnderRecovery.java:[lines 38-148]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' signature='(Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;J)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='39' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' start='38' end='46' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java' sourcefile='ReplicaUnderRecovery.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery(ReplicaInfo, long)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ReplicaInfo' start='49' end='317' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java' sourcefile='ReplicaInfo.java'><Message>At ReplicaInfo.java:[lines 49-317]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.datanode.ReplicaInfo</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/LocalReplica;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.LocalReplica' start='65' end='527' sourcepath='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' sourcefile='LocalReplica.java'><Message>At LocalReplica.java:[lines 65-527]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.datanode.LocalReplica</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='6' name='replica' register='1'><Message>Value loaded from replica</Message></LocalVariable><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' start='38' end='38' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java' sourcefile='ReplicaUnderRecovery.java' startBytecode='7' primary='true'><Message>At ReplicaUnderRecovery.java:[line 38]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='75' classname='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery' start='44' end='44' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java' sourcefile='ReplicaUnderRecovery.java' startBytecode='75'><Message>Another occurrence at ReplicaUnderRecovery.java:[line 44]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='69b07eea01a238e150d954191b333346' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm to org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm of return value in org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.getClientNames(ExtendedBlockId)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry' start='84' end='404' sourcepath='org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java' sourcefile='ShortCircuitRegistry.java'><Message>At ShortCircuitRegistry.java:[lines 84-404]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry' signature='(Lorg/apache/hadoop/hdfs/ExtendedBlockId;)Ljava/lang/String;' name='getClientNames' primary='true'><SourceLine endBytecode='36' classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry' start='257' end='263' sourcepath='org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java' sourcefile='ShortCircuitRegistry.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.getClientNames(ExtendedBlockId)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm;'><SourceLine classname='org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm' start='51' end='639' sourcepath='org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java' sourcefile='ShortCircuitShm.java'><Message>At ShortCircuitShm.java:[lines 51-639]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry$RegisteredShm;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm' start='95' end='111' sourcepath='org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java' sourcefile='ShortCircuitRegistry.java'><Message>At ShortCircuitRegistry.java:[lines 95-111]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm</Message></Type><SourceLine endBytecode='63' classname='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry' start='261' end='261' sourcepath='org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java' sourcefile='ShortCircuitRegistry.java' startBytecode='63' primary='true'><Message>At ShortCircuitRegistry.java:[line 261]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3e07851e6d4d0b8d3e86722790247300' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of org.apache.hadoop.hdfs.server.datanode.StorageLocation.getNormalizedUri(), which is known to be non-null in org.apache.hadoop.hdfs.server.datanode.StorageLocation.compareTo(StorageLocation)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' start='52' end='244' sourcepath='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' sourcefile='StorageLocation.java'><Message>At StorageLocation.java:[lines 52-244]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.StorageLocation</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' signature='(Lorg/apache/hadoop/hdfs/server/datanode/StorageLocation;)I' name='compareTo' primary='true'><SourceLine endBytecode='217' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' start='168' end='185' sourcepath='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' sourcefile='StorageLocation.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.StorageLocation.compareTo(StorageLocation)</Message></Method><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' signature='()Ljava/net/URI;' name='getNormalizedUri'><SourceLine endBytecode='49' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' start='95' end='95' sourcepath='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' sourcefile='StorageLocation.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.datanode.StorageLocation.getNormalizedUri() of type java.net.URI</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='19' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' start='175' end='175' sourcepath='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' sourcefile='StorageLocation.java' startBytecode='19' primary='true'><Message>Redundant null check at StorageLocation.java:[line 175]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='26' classname='org.apache.hadoop.hdfs.server.datanode.StorageLocation' start='176' end='176' sourcepath='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' sourcefile='StorageLocation.java' startBytecode='26'><Message>Another occurrence at StorageLocation.java:[line 176]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9c4639f402fc66e8c1fbd3f8795c7c09' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.VolumeScanner.disableBlockPoolId(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='51' end='753' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java'><Message>At VolumeScanner.java:[lines 51-753]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.VolumeScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' signature='(Ljava/lang/String;)V' name='disableBlockPoolId' primary='true'><SourceLine endBytecode='271' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='731' end='747' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.VolumeScanner.disableBlockPoolId(String)</Message></Method><SourceLine endBytecode='86' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='741' end='741' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='86' primary='true'><Message>At VolumeScanner.java:[line 741]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='12f6373d9c392657b2312a62209f47e5' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.VolumeScanner.enableBlockPoolId(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='51' end='753' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java'><Message>At VolumeScanner.java:[lines 51-753]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.VolumeScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' signature='(Ljava/lang/String;)V' name='enableBlockPoolId' primary='true'><SourceLine endBytecode='424' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='700' end='723' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.VolumeScanner.enableBlockPoolId(String)</Message></Method><SourceLine endBytecode='190' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='722' end='722' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='190' primary='true'><Message>At VolumeScanner.java:[line 722]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ef47c4f30e9532081e54c0b9af858088' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.VolumeScanner.markSuspectBlock(ExtendedBlock)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='51' end='753' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java'><Message>At VolumeScanner.java:[lines 51-753]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.VolumeScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;)V' name='markSuspectBlock' primary='true'><SourceLine endBytecode='248' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='672' end='692' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.VolumeScanner.markSuspectBlock(ExtendedBlock)</Message></Method><SourceLine endBytecode='111' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='691' end='691' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='111' primary='true'><Message>At VolumeScanner.java:[line 691]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='709d5346b2e5673844ec0bf474c204d0' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.datanode.VolumeScanner.shutdown()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='51' end='753' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java'><Message>At VolumeScanner.java:[lines 51-753]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.VolumeScanner</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' signature='()V' name='shutdown' primary='true'><SourceLine endBytecode='67' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='665' end='668' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.VolumeScanner.shutdown()</Message></Method><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.datanode.VolumeScanner' start='666' end='666' sourcepath='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' sourcefile='VolumeScanner.java' startBytecode='6' primary='true'><Message>At VolumeScanner.java:[line 666]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cfcd9d3914fd99eeaeb0790042ae404b' cweid='563' rank='17' abbrev='DLS' category='STYLE' priority='2' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to $L6 in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.setFuture(ListenableFuture)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='(Lcom/google/common/util/concurrent/ListenableFuture;)Z' name='setFuture' primary='true'><SourceLine endBytecode='74' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='772' end='815' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.setFuture(ListenableFuture)</Message></Method><LocalVariable role='LOCAL_VARIABLE_UNKNOWN' pc='116' name='?' register='6'><Message>Local variable stored in JVM register 6</Message></LocalVariable><SourceLine endBytecode='116' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='803' end='803' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='116' primary='true'><Message>At AbstractFuture.java:[line 803]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='?'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.METHOD_RESULT' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='406f3786aa5ba739dbd29d5ec943ee2b' cweid='563' rank='9' abbrev='DLS' category='CORRECTNESS' priority='2' type='DLS_DEAD_STORE_OF_CLASS_LITERAL' instanceOccurrenceMax='0'><ShortMessage>Dead store of class literal</ShortMessage><LongMessage>Dead store of java.util.concurrent.locks.LockSupport.class in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.&lt;static initializer for AbstractFuture&gt;()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='()V' name='&lt;clinit&gt;' primary='true'><SourceLine endBytecode='65' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='74' end='272' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.&lt;static initializer for AbstractFuture&gt;()</Message></Method><LocalVariable role='LOCAL_VARIABLE_UNKNOWN' pc='133' name='?' register='1'><Message>Local variable stored in JVM register 1</Message></LocalVariable><Type descriptor='Ljava.util.concurrent.locks.LockSupport;'><SourceLine classname='java.util.concurrent.locks.LockSupport' start='121' end='412' sourcepath='java/util/concurrent/locks/LockSupport.java' sourcefile='LockSupport.java'><Message>At LockSupport.java:[lines 121-412]</Message></SourceLine><Message>Type java.util.concurrent.locks.LockSupport</Message></Type><SourceLine endBytecode='133' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='164' end='164' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='133' primary='true'><Message>At AbstractFuture.java:[line 164]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='956aaa4db081075ce3efa7f6852402e9' rank='15' abbrev='NS' category='STYLE' priority='1' type='NS_DANGEROUS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Potentially dangerous use of non-short-circuit logic</ShortMessage><LongMessage>Potentially dangerous use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.cancel(boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='(Z)Z' name='cancel' primary='true'><SourceLine endBytecode='510' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='586' end='646' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.cancel(boolean)</Message></Method><SourceLine endBytecode='141' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='624' end='624' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='141' primary='true'><Message>At AbstractFuture.java:[line 624]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='644c09275d65d585e6eee0bc2d7f521e' rank='20' abbrev='NS' category='STYLE' priority='3' type='NS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Questionable use of non-short-circuit logic</ShortMessage><LongMessage>Questionable use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.cancel(boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='(Z)Z' name='cancel' primary='true'><SourceLine endBytecode='510' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='586' end='646' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.cancel(boolean)</Message></Method><SourceLine endBytecode='21' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='588' end='588' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='21' primary='true'><Message>At AbstractFuture.java:[line 588]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d445a4b3659bb4b6bc7816a7d0264402' rank='20' abbrev='NS' category='STYLE' priority='3' type='NS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Questionable use of non-short-circuit logic</ShortMessage><LongMessage>Questionable use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.get()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='()Ljava/lang/Object;' name='get' primary='true'><SourceLine endBytecode='408' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='504' end='539' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.get()</Message></Method><SourceLine endBytecode='41' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='508' end='508' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='41' primary='true'><Message>At AbstractFuture.java:[line 508]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='137' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='529' end='529' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='137'><Message>Another occurrence at AbstractFuture.java:[line 529]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7fb045f5c4dda148a0cb9710bb07b809' rank='20' abbrev='NS' category='STYLE' priority='3' type='NS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Questionable use of non-short-circuit logic</ShortMessage><LongMessage>Questionable use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.get(long, TimeUnit)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='(JLjava/util/concurrent/TimeUnit;)Ljava/lang/Object;' name='get' primary='true'><SourceLine endBytecode='728' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='416' end='482' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.get(long, TimeUnit)</Message></Method><SourceLine endBytecode='51' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='422' end='422' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='51' primary='true'><Message>At AbstractFuture.java:[line 422]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='189' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='449' end='449' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='189'><Message>Another occurrence at AbstractFuture.java:[line 449]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='285' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='474' end='474' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='285'><Message>Another occurrence at AbstractFuture.java:[line 474]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b0e8756721ad0453beafbeb2c4dc6196' rank='20' abbrev='NS' category='STYLE' priority='3' type='NS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Questionable use of non-short-circuit logic</ShortMessage><LongMessage>Questionable use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.isDone()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='()Z' name='isDone' primary='true'><SourceLine endBytecode='134' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='566' end='567' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.isDone()</Message></Method><SourceLine endBytecode='27' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='567' end='567' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='27' primary='true'><Message>At AbstractFuture.java:[line 567]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5df8b4b017fe2f67946768cb81c5a6a1' rank='20' abbrev='NS' category='STYLE' priority='3' type='NS_NON_SHORT_CIRCUIT' instanceOccurrenceMax='0'><ShortMessage>Questionable use of non-short-circuit logic</ShortMessage><LongMessage>Questionable use of non-short-circuit logic in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.maybePropagateCancellation(Future)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='70' end='1276' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 70-1276]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' signature='(Ljava/util/concurrent/Future;)V' name='maybePropagateCancellation' primary='true'><SourceLine endBytecode='128' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='939' end='942' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture.maybePropagateCancellation(Future)</Message></Method><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture' start='939' end='939' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='14' primary='true'><Message>At AbstractFuture.java:[line 939]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='747b081f2f31ce8b6aaebc988f94de56' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper.&lt;static initializer for UnsafeAtomicHelper&gt;()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper' start='1043' end='1142' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java'><Message>At AbstractFuture.java:[lines 1043-1142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper' signature='()V' name='&lt;clinit&gt;' primary='true'><SourceLine endBytecode='410' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper' start='1052' end='1095' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper.&lt;static initializer for UnsafeAtomicHelper&gt;()</Message></Method><SourceLine endBytecode='119' classname='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper' start='1091' end='1091' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' sourcefile='AbstractFuture.java' startBytecode='119' primary='true'><Message>At AbstractFuture.java:[line 1091]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3b6c94de2e2e85bab06ded934ed66c55' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$1'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$1' start='224' end='229' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java' sourcefile='DatasetVolumeChecker.java'><Message>At DatasetVolumeChecker.java:[lines 224-229]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker' start='69' end='458' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java' sourcefile='DatasetVolumeChecker.java'><Message>At DatasetVolumeChecker.java:[lines 69-458]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker' signature='(Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi;)Ljava/util/Set;' name='checkAllVolumes' primary='true'><SourceLine endBytecode='859' classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker' start='189' end='254' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java' sourcefile='DatasetVolumeChecker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(FsDatasetSpi)</Message></Method><SourceLine endBytecode='259' classname='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker' start='222' end='222' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java' sourcefile='DatasetVolumeChecker.java' startBytecode='259' primary='true'><Message>At DatasetVolumeChecker.java:[line 222]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f921bf88fe0365c2615d171380d9ec28' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.exception</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult' start='211' end='247' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java' sourcefile='ThrottledAsyncChecker.java'><Message>At ThrottledAsyncChecker.java:[lines 211-247]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult' signature='Ljava/lang/Throwable;' name='exception' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java' sourcefile='ThrottledAsyncChecker.java'><Message>In ThrottledAsyncChecker.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.exception</Message></Field><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult' start='234' end='234' sourcepath='org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java' sourcefile='ThrottledAsyncChecker.java' startBytecode='11' primary='true'><Message>At ThrottledAsyncChecker.java:[line 234]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9ee5713f43bd339726bffcc7d08d19e9' cweid='253' rank='19' abbrev='RV' category='BAD_PRACTICE' priority='3' type='RV_RETURN_VALUE_IGNORED_BAD_PRACTICE' instanceOccurrenceMax='0'><ShortMessage>Method ignores exceptional return value</ShortMessage><LongMessage>Exceptional return value of java.util.concurrent.AbstractExecutorService.submit(Runnable) ignored in org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker.processErasureCodingTasks(Collection)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' start='46' end='171' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' sourcefile='ErasureCodingWorker.java'><Message>At ErasureCodingWorker.java:[lines 46-171]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' signature='(Ljava/util/Collection;)V' name='processErasureCodingTasks' primary='true'><SourceLine endBytecode='434' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' start='122' end='154' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' sourcefile='ErasureCodingWorker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker.processErasureCodingTasks(Collection)</Message></Method><Method isStatic='false' role='METHOD_CALLED' classname='java.util.concurrent.AbstractExecutorService' signature='(Ljava/lang/Runnable;)Ljava/util/concurrent/Future;' name='submit'><SourceLine endBytecode='70' classname='java.util.concurrent.AbstractExecutorService' start='110' end='113' sourcepath='java/util/concurrent/AbstractExecutorService.java' sourcefile='AbstractExecutorService.java' startBytecode='0'></SourceLine><Message>Called method java.util.concurrent.AbstractExecutorService.submit(Runnable)</Message></Method><SourceLine endBytecode='119' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker' start='143' end='143' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' sourcefile='ErasureCodingWorker.java' startBytecode='119' primary='true'><Message>At ErasureCodingWorker.java:[line 143]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe7cb8ac1fbd9e8a0ae6c1316ed1441b' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor.getDigestObject() may expose internal representation by returning StripedBlockChecksumCompositeCrcReconstructor.digestValue</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' start='42' end='79' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java' sourcefile='StripedBlockChecksumCompositeCrcReconstructor.java'><Message>At StripedBlockChecksumCompositeCrcReconstructor.java:[lines 42-79]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' signature='()Ljava/lang/Object;' name='getDigestObject' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' start='48' end='48' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java' sourcefile='StripedBlockChecksumCompositeCrcReconstructor.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor.getDigestObject()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' signature='[B' name='digestValue' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java' sourcefile='StripedBlockChecksumCompositeCrcReconstructor.java'><Message>In StripedBlockChecksumCompositeCrcReconstructor.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor.digestValue</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor' start='48' end='48' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java' sourcefile='StripedBlockChecksumCompositeCrcReconstructor.java' startBytecode='4' primary='true'><Message>At StripedBlockChecksumCompositeCrcReconstructor.java:[line 48]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a66b1faadda04ddc01961f03f5e52ca8' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo.getTargetStorageIds() and org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageIDs()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo' start='48' end='119' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java' sourcefile='StripedReconstructionInfo.java'><Message>At StripedReconstructionInfo.java:[lines 48-119]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo' signature='()[Ljava/lang/String;' name='getTargetStorageIds' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo' start='104' end='104' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java' sourcefile='StripedReconstructionInfo.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo.getTargetStorageIds()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[Ljava/lang/String;' name='getTargetStorageIDs'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='119' end='119' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageIDs()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo' start='104' end='104' sourcepath='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java' sourcefile='StripedReconstructionInfo.java' startBytecode='0'><Message>At StripedReconstructionInfo.java:[line 104]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9d450dbbdb3f974d752ebfe2ea560c0' rank='19' abbrev='It' category='BAD_PRACTICE' priority='3' type='IT_NO_SUCH_ELEMENT' instanceOccurrenceMax='0'><ShortMessage>Iterator next() method can't throw NoSuchElementException</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator.next() can't throw NoSuchElementException</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator' start='117' end='139' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java' sourcefile='FsDatasetSpi.java'><Message>At FsDatasetSpi.java:[lines 117-139]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator' signature='()Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi;' name='next' primary='true'><SourceLine endBytecode='86' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator' start='133' end='134' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java' sourcefile='FsDatasetSpi.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator.next()</Message></Method><SourceLine synthetic='true' endBytecode='86' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator' start='133' end='134' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java' sourcefile='FsDatasetSpi.java' startBytecode='0'><Message>At FsDatasetSpi.java:[lines 133-134]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bc8ac0cf2161fd68d6988b4768fb4d2b' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(ReplicaMap, RamDiskReplicaTracker)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='73' end='887' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java'><Message>At BlockPoolSlice.java:[lines 73-887]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' signature='(Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaMap;Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker;)Z' name='readReplicasFromCache' primary='true'><SourceLine endBytecode='1295' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='773' end='844' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.readReplicasFromCache(ReplicaMap, RamDiskReplicaTracker)</Message></Method><SourceLine endBytecode='515' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='829' end='829' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java' startBytecode='515' primary='true'><Message>At BlockPoolSlice.java:[line 829]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b2e8d00a38f9ea8ae06db507a9e0cb8c' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveReplicas(BlockListAsLongs)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='73' end='887' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java'><Message>At BlockPoolSlice.java:[lines 73-887]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' signature='(Lorg/apache/hadoop/hdfs/protocol/BlockListAsLongs;)V' name='saveReplicas' primary='true'><SourceLine endBytecode='480' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='848' end='876' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveReplicas(BlockListAsLongs)</Message></Method><SourceLine endBytecode='134' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice' start='866' end='866' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' sourcefile='BlockPoolSlice.java' startBytecode='134' primary='true'><Message>At BlockPoolSlice.java:[line 866]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1989c91a42d2cd72cb84fd6d3ebd7254' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2' start='207' end='215' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java' sourcefile='FsDatasetAsyncDiskService.java'><Message>At FsDatasetAsyncDiskService.java:[lines 207-215]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService' start='61' end='358' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java' sourcefile='FsDatasetAsyncDiskService.java'><Message>At FsDatasetAsyncDiskService.java:[lines 61-358]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService' signature='(Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl;Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams;JJI)V' name='submitSyncFileRangeRequest' primary='true'><SourceLine endBytecode='115' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService' start='207' end='217' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java' sourcefile='FsDatasetAsyncDiskService.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService.submitSyncFileRangeRequest(FsVolumeImpl, ReplicaOutputStreams, long, long, int)</Message></Method><SourceLine endBytecode='13' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService' start='207' end='207' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java' sourcefile='FsDatasetAsyncDiskService.java' startBytecode='13' primary='true'><Message>At FsDatasetAsyncDiskService.java:[line 207]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5eb34c8c1c218884055587c0ddabd6f0' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of info, which is known to be non-null in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetaDataInputStream(ExtendedBlock)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='130' end='3298' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java'><Message>At FsDatasetImpl.java:[lines 130-3298]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;)Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/LengthInputStream;' name='getMetaDataInputStream' primary='true'><SourceLine endBytecode='113' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='224' end='228' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getMetaDataInputStream(ExtendedBlock)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='6' name='info' register='2'><Message>Value loaded from info</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;)Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;' name='getBlockReplica'><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='738' end='738' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockReplica(ExtendedBlock) of type org.apache.hadoop.hdfs.server.datanode.ReplicaInfo</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='7' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='225' end='225' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='7' primary='true'><Message>Redundant null check at FsDatasetImpl.java:[line 225]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ecf4e2580eee1c2395fd393c6e03488d' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of blockFile, which is known to be non-null in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(String, Block[], boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='130' end='3298' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java'><Message>At FsDatasetImpl.java:[lines 130-3298]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' signature='(Ljava/lang/String;[Lorg/apache/hadoop/hdfs/protocol/Block;Z)V' name='invalidate' primary='true'><SourceLine endBytecode='1932' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='2012' end='2110' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(String, Block[], boolean)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='304' name='blockFile' register='11'><Message>Value loaded from blockFile</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='java.io.File' signature='(Ljava/net/URI;)V' name='&lt;init&gt;'><SourceLine endBytecode='352' classname='java.io.File' start='412' end='438' sourcepath='java/io/File.java' sourcefile='File.java' startBytecode='0'></SourceLine><Message>Return value of new java.io.File(URI) of type void</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='306' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='2042' end='2042' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='306' primary='true'><Message>Redundant null check at FsDatasetImpl.java:[line 2042]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5a99e4b6ec8f98c8b87360ef363649e5' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of volumeRef, which is known to be non-null in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockAcrossStorage(ExtendedBlock, StorageType, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='130' end='3298' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java'><Message>At FsDatasetImpl.java:[lines 130-3298]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;Lorg/apache/hadoop/fs/StorageType;Ljava/lang/String;)Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;' name='moveBlockAcrossStorage' primary='true'><SourceLine endBytecode='836' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='925' end='961' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockAcrossStorage(ExtendedBlock, StorageType, String)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='368' name='volumeRef' register='5'><Message>Value loaded from volumeRef</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' signature='(Lorg/apache/hadoop/fs/StorageType;Ljava/lang/String;J)Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeReference;' name='getNextVolume'><SourceLine endBytecode='228' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='111' end='117' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getNextVolume(StorageType, String, long) of type org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='370' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl' start='955' end='955' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='370' primary='true'><Message>Redundant null check at FsDatasetImpl.java:[line 955]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a41e0e66bacb9060e1950df837089ee6' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter' start='3039' end='3186' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java'><Message>At FsDatasetImpl.java:[lines 3039-3186]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter' signature='()V' name='run' primary='true'><SourceLine endBytecode='259' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter' start='3162' end='3182' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter.run()</Message></Method><SourceLine endBytecode='56' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter' start='3172' end='3172' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' sourcefile='FsDatasetImpl.java' startBytecode='56' primary='true'><Message>At FsDatasetImpl.java:[line 3172]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fc3d01a95fd821317b2b4d9c468ead95' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil.createNullChecksumByteArray() return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil' start='46' end='179' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java' sourcefile='FsDatasetUtil.java'><Message>At FsDatasetUtil.java:[lines 46-179]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil' signature='()[B' name='createNullChecksumByteArray' primary='true'><SourceLine endBytecode='221' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil' start='52' end='64' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java' sourcefile='FsDatasetUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil.createNullChecksumByteArray()</Message></Method><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil' start='62' end='62' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java' sourcefile='FsDatasetUtil.java' startBytecode='68' primary='true'><Message>At FsDatasetUtil.java:[line 62]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8065f94ddf7439840da38d8244aab952' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.datanode.ReplicaInfo to org.apache.hadoop.hdfs.server.datanode.FinalizedReplica in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.append(String, ReplicaInfo, long, long)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='100' end='1484' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java'><Message>At FsVolumeImpl.java:[lines 100-1484]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' signature='(Ljava/lang/String;Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;JJ)Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline;' name='append' primary='true'><SourceLine endBytecode='82' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='1183' end='1215' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.append(String, ReplicaInfo, long, long)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/ReplicaInfo;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.ReplicaInfo' start='49' end='317' sourcepath='org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java' sourcefile='ReplicaInfo.java'><Message>At ReplicaInfo.java:[lines 49-317]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.datanode.ReplicaInfo</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/FinalizedReplica;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica' start='43' end='160' sourcepath='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' sourcefile='FinalizedReplica.java'><Message>At FinalizedReplica.java:[lines 43-160]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.datanode.FinalizedReplica</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='145' name='replicaInfo' register='2'><Message>Value loaded from replicaInfo</Message></LocalVariable><SourceLine endBytecode='146' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='1205' end='1205' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java' startBytecode='146' primary='true'><Message>At FsVolumeImpl.java:[line 1205]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9d047d381a2207f0aded0bc4872dc344' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.loadLastPartialChunkChecksum(File, File) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='100' end='1484' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java'><Message>At FsVolumeImpl.java:[lines 100-1484]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' signature='(Ljava/io/File;Ljava/io/File;)[B' name='loadLastPartialChunkChecksum' primary='true'><SourceLine endBytecode='967' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='1146' end='1177' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.loadLastPartialChunkChecksum(File, File)</Message></Method><SourceLine endBytecode='140' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl' start='1157' end='1157' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' sourcefile='FsVolumeImpl.java' startBytecode='140' primary='true'><Message>At FsVolumeImpl.java:[line 1157]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='db7fccd52f57dce4508dd59c04fb0938' cweid='253' rank='19' abbrev='RV' category='BAD_PRACTICE' priority='3' type='RV_RETURN_VALUE_IGNORED_BAD_PRACTICE' instanceOccurrenceMax='0'><ShortMessage>Method ignores exceptional return value</ShortMessage><LongMessage>Exceptional return value of java.util.concurrent.locks.Condition.await(long, TimeUnit) ignored in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.waitVolumeRemoved(int, Condition)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='49' end='456' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java'><Message>At FsVolumeList.java:[lines 49-456]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' signature='(ILjava/util/concurrent/locks/Condition;)V' name='waitVolumeRemoved' primary='true'><SourceLine endBytecode='208' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='267' end='280' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.waitVolumeRemoved(int, Condition)</Message></Method><Method isStatic='false' role='METHOD_CALLED' classname='java.util.concurrent.locks.Condition' signature='(JLjava/util/concurrent/TimeUnit;)Z' name='await'><SourceLine classname='java.util.concurrent.locks.Condition' sourcepath='java/util/concurrent/locks/Condition.java' sourcefile='Condition.java'></SourceLine><Message>Called method java.util.concurrent.locks.Condition.await(long, TimeUnit)</Message></Method><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='272' end='272' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='34' primary='true'><Message>At FsVolumeList.java:[line 272]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='434afe006ad05ad4f580e2e1dbcd25d2' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1' start='195' end='213' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java'><Message>At FsVolumeList.java:[lines 195-213]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='49' end='456' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java'><Message>At FsVolumeList.java:[lines 49-456]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' signature='(Ljava/lang/String;Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaMap;Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker;)V' name='getAllVolumesMap' primary='true'><SourceLine endBytecode='557' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='190' end='231' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.getAllVolumesMap(String, ReplicaMap, RamDiskReplicaTracker)</Message></Method><SourceLine endBytecode='69' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='195' end='195' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='69' primary='true'><Message>At FsVolumeList.java:[line 195]</Message></SourceLine><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='74' name='t' register='10'><Message>Local variable named t</Message></LocalVariable></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5ec69cdd3aa6295ea3d141c1bb360d97' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2' start='405' end='422' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java'><Message>At FsVolumeList.java:[lines 405-422]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='49' end='456' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java'><Message>At FsVolumeList.java:[lines 49-456]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' signature='(Ljava/lang/String;Lorg/apache/hadoop/conf/Configuration;)V' name='addBlockPool' primary='true'><SourceLine endBytecode='547' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='399' end='441' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(String, Configuration)</Message></Method><SourceLine endBytecode='67' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList' start='405' end='405' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' sourcefile='FsVolumeList.java' startBytecode='67' primary='true'><Message>At FsVolumeList.java:[line 405]</Message></SourceLine><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='72' name='t' register='9'><Message>Local variable named t</Message></LocalVariable></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7935b5108785cb3edaec0c33ac3ec5dd' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask' start='216' end='259' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java' sourcefile='RamDiskAsyncLazyPersistService.java'><Message>At RamDiskAsyncLazyPersistService.java:[lines 216-259]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask' signature='()V' name='run' primary='true'><SourceLine endBytecode='692' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask' start='237' end='259' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java' sourcefile='RamDiskAsyncLazyPersistService.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask.run()</Message></Method><SourceLine endBytecode='205' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask' start='250' end='250' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java' sourcefile='RamDiskAsyncLazyPersistService.java' startBytecode='205' primary='true'><Message>At RamDiskAsyncLazyPersistService.java:[line 250]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b51f38ab53120823bff90d8c6b087b0c' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica to org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker$RamDiskReplicaLru in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker.reenqueueReplicaNotPersisted(RamDiskReplicaTracker$RamDiskReplica)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker' start='34' end='237' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java' sourcefile='RamDiskReplicaLruTracker.java'><Message>At RamDiskReplicaLruTracker.java:[lines 34-237]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker' signature='(Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker$RamDiskReplica;)V' name='reenqueueReplicaNotPersisted' primary='true'><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker' start='166' end='167' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java' sourcefile='RamDiskReplicaLruTracker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker.reenqueueReplicaNotPersisted(RamDiskReplicaTracker$RamDiskReplica)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker$RamDiskReplica;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' start='43' end='176' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java' sourcefile='RamDiskReplicaTracker.java'><Message>At RamDiskReplicaTracker.java:[lines 43-176]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker$RamDiskReplicaLru;'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker$RamDiskReplicaLru' start='38' end='54' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java' sourcefile='RamDiskReplicaLruTracker.java'><Message>At RamDiskReplicaLruTracker.java:[lines 38-54]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker$RamDiskReplicaLru</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='4' name='ramDiskReplicaLru' register='1'><Message>Value loaded from ramDiskReplicaLru</Message></LocalVariable><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker' start='166' end='166' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java' sourcefile='RamDiskReplicaLruTracker.java' startBytecode='5' primary='true'><Message>At RamDiskReplicaLruTracker.java:[line 166]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e3017df6670c93cc9ce2c1636bdf95d' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica.ramDiskVolume</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' start='43' end='176' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java' sourcefile='RamDiskReplicaTracker.java'><Message>At RamDiskReplicaTracker.java:[lines 43-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' signature='Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi;' name='ramDiskVolume' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java' sourcefile='RamDiskReplicaTracker.java'><Message>In RamDiskReplicaTracker.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica.ramDiskVolume</Message></Field><SourceLine endBytecode='29' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica' start='69' end='69' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java' sourcefile='RamDiskReplicaTracker.java' startBytecode='29' primary='true'><Message>At RamDiskReplicaTracker.java:[line 69]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ea44ef5a7d8ddd03b6b05643f877a18a' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder' start='49' end='77' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java' sourcefile='ReservedSpaceCalculator.java'><Message>At ReservedSpaceCalculator.java:[lines 49-77]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder' signature='()Lorg/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator;' name='build' primary='true'><SourceLine endBytecode='211' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder' start='66' end='77' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java' sourcefile='ReservedSpaceCalculator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build()</Message></Method><SourceLine endBytecode='72' classname='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder' start='76' end='76' sourcepath='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java' sourcefile='ReservedSpaceCalculator.java' startBytecode='72' primary='true'><Message>At ReservedSpaceCalculator.java:[line 76]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7b102283ba4359e580d97c102d30f2a5' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2' start='184' end='187' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java' sourcefile='DatanodeHttpServer.java'><Message>At DatanodeHttpServer.java:[lines 184-187]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer' start='77' end='334' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java' sourcefile='DatanodeHttpServer.java'><Message>At DatanodeHttpServer.java:[lines 77-334]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer' signature='(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/hdfs/server/datanode/DataNode;Ljava/nio/channels/ServerSocketChannel;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='969' classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer' start='102' end='230' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java' sourcefile='DatanodeHttpServer.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer(Configuration, DataNode, ServerSocketChannel)</Message></Method><SourceLine endBytecode='411' classname='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer' start='184' end='184' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java' sourcefile='DatanodeHttpServer.java' startBytecode='411' primary='true'><Message>At DatanodeHttpServer.java:[line 184]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='76177fb528bbd574a7a00cf249aca48b' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1' start='78' end='87' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java' sourcefile='SimpleHttpProxyHandler.java'><Message>At SimpleHttpProxyHandler.java:[lines 78-87]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder' start='62' end='95' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java' sourcefile='SimpleHttpProxyHandler.java'><Message>At SimpleHttpProxyHandler.java:[lines 62-95]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder' signature='(Lio/netty/channel/ChannelHandlerContext;Ljava/lang/Object;)V' name='channelRead' primary='true'><SourceLine endBytecode='91' classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder' start='78' end='89' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java' sourcefile='SimpleHttpProxyHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder.channelRead(ChannelHandlerContext, Object)</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder' start='78' end='78' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java' sourcefile='SimpleHttpProxyHandler.java' startBytecode='16' primary='true'><Message>At SimpleHttpProxyHandler.java:[line 78]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e781a5e63319fe839359c80dbb4005b8' rank='20' abbrev='UPM' category='PERFORMANCE' priority='3' type='UPM_UNCALLED_PRIVATE_METHOD' instanceOccurrenceMax='0'><ShortMessage>Private method is never called</ShortMessage><LongMessage>Private method org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser.decodeHexNibble(char) is never called</LongMessage><Class classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser' start='60' end='179' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java' sourcefile='ParameterParser.java'><Message>At ParameterParser.java:[lines 60-179]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser' signature='(C)C' name='decodeHexNibble' primary='true'><SourceLine endBytecode='139' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser' start='172' end='179' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java' sourcefile='ParameterParser.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser.decodeHexNibble(char)</Message></Method><SourceLine synthetic='true' endBytecode='139' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser' start='172' end='179' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java' sourcefile='ParameterParser.java' startBytecode='0'><Message>At ParameterParser.java:[lines 172-179]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d38a5bd37880780ff10000ced63f9389' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2' start='280' end='285' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java' sourcefile='WebHdfsHandler.java'><Message>At WebHdfsHandler.java:[lines 280-285]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler' start='89' end='344' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java' sourcefile='WebHdfsHandler.java'><Message>At WebHdfsHandler.java:[lines 89-344]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler' signature='(Lio/netty/channel/ChannelHandlerContext;)V' name='onOpen' primary='true'><SourceLine endBytecode='536' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler' start='249' end='287' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java' sourcefile='WebHdfsHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler.onOpen(ChannelHandlerContext)</Message></Method><SourceLine endBytecode='223' classname='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler' start='280' end='280' sourcepath='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java' sourcefile='WebHdfsHandler.java' startBytecode='223' primary='true'><Message>At WebHdfsHandler.java:[line 280]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1897820345c6b4a002e6671cdcc1a26f' rank='20' abbrev='Dm' category='I18N' priority='3' type='DM_CONVERT_CASE' instanceOccurrenceMax='0'><ShortMessage>Consider using Locale parameterized version of invoked method</ShortMessage><LongMessage>Use of non-localized String.toUpperCase() or String.toLowerCase() in org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand.execute(CommandLine)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand' start='39' end='106' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java' sourcefile='HelpCommand.java'><Message>At HelpCommand.java:[lines 39-106]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand' signature='(Lorg/apache/commons/cli/CommandLine;)V' name='execute' primary='true'><SourceLine endBytecode='541' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand' start='50' end='89' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java' sourcefile='HelpCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand.execute(CommandLine)</Message></Method><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand' start='65' end='65' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java' sourcefile='HelpCommand.java' startBytecode='64' primary='true'><Message>At HelpCommand.java:[line 65]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4c891699086f8db92b4396a1c80b327' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand.execute(CommandLine)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand' start='57' end='271' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java' sourcefile='PlanCommand.java'><Message>At PlanCommand.java:[lines 57-271]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand' signature='(Lorg/apache/commons/cli/CommandLine;)V' name='execute' primary='true'><SourceLine endBytecode='1540' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand' start='92' end='184' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java' sourcefile='PlanCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand.execute(CommandLine)</Message></Method><SourceLine endBytecode='654' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand' start='175' end='175' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java' sourcefile='PlanCommand.java' startBytecode='654' primary='true'><Message>At PlanCommand.java:[line 175]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1ad146ca785a6bec04d17dbc30d40cc8' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to dbdns in org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand.handleNodeReport(CommandLine, StrBuilder, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand' start='51' end='235' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java' sourcefile='ReportCommand.java'><Message>At ReportCommand.java:[lines 51-235]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand' signature='(Lorg/apache/commons/cli/CommandLine;Lorg/apache/commons/lang/text/StrBuilder;Ljava/lang/String;Ljava/lang/String;)V' name='handleNodeReport' primary='true'><SourceLine endBytecode='67' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand' start='136' end='173' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java' sourcefile='ReportCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand.handleNodeReport(CommandLine, StrBuilder, String, String)</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='62' name='dbdns' register='7'><Message>Local variable named dbdns</Message></LocalVariable><SourceLine endBytecode='60' classname='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand' start='156' end='156' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java' sourcefile='ReportCommand.java' startBytecode='60' primary='true'><Message>At ReportCommand.java:[line 156]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.KILLED_BY_SUBSEQUENT_STORE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='dbdns'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.METHOD_RESULT' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c48f8d8f5a7008b21d8e873cd5d5aa20' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster$1'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster$1' start='320' end='324' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java' sourcefile='DiskBalancerCluster.java'><Message>At DiskBalancerCluster.java:[lines 320-324]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster' start='74' end='393' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java' sourcefile='DiskBalancerCluster.java'><Message>At DiskBalancerCluster.java:[lines 74-393]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster' signature='(D)Ljava/util/List;' name='computePlan' primary='true'><SourceLine endBytecode='596' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster' start='304' end='338' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java' sourcefile='DiskBalancerCluster.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster.computePlan(double)</Message></Method><SourceLine endBytecode='114' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster' start='320' end='320' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java' sourcefile='DiskBalancerCluster.java' startBytecode='114' primary='true'><Message>At DiskBalancerCluster.java:[line 320]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='11d7a55b82ad6e40d775c3a9facac3b2' rank='19' abbrev='SnVI' category='BAD_PRACTICE' priority='3' type='SE_NO_SERIALVERSIONID' instanceOccurrenceMax='0'><ShortMessage>Class is Serializable, but doesn't define serialVersionUID</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap is Serializable; consider declaring a serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap' start='329' end='339' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java' sourcefile='DiskBalancerVolumeSet.java'><Message>At DiskBalancerVolumeSet.java:[lines 329-339]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap' start='329' end='339' sourcepath='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java' sourcefile='DiskBalancerVolumeSet.java'><Message>At DiskBalancerVolumeSet.java:[lines 329-339]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9d1278b1bb29cc41d55d75e411ff20e5' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.protocol.LocatedBlock to org.apache.hadoop.hdfs.protocol.LocatedStripedBlock in org.apache.hadoop.hdfs.server.mover.Mover.newDBlock(LocatedBlock, List, ErasureCodingPolicy)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.mover.Mover' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.mover.Mover' start='70' end='884' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java'><Message>At Mover.java:[lines 70-884]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.mover.Mover</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.mover.Mover' signature='(Lorg/apache/hadoop/hdfs/protocol/LocatedBlock;Ljava/util/List;Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;)Lorg/apache/hadoop/hdfs/server/balancer/Dispatcher$DBlock;' name='newDBlock' primary='true'><SourceLine endBytecode='69' classname='org.apache.hadoop.hdfs.server.mover.Mover' start='192' end='211' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.mover.Mover.newDBlock(LocatedBlock, List, ErasureCodingPolicy)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/protocol/LocatedBlock;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LocatedBlock' start='81' end='279' sourcepath='org/apache/hadoop/hdfs/protocol/LocatedBlock.java' sourcefile='LocatedBlock.java'><Message>At LocatedBlock.java:[lines 81-279]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.protocol.LocatedBlock</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/protocol/LocatedStripedBlock;'><SourceLine classname='org.apache.hadoop.hdfs.protocol.LocatedStripedBlock' start='36' end='86' sourcepath='org/apache/hadoop/hdfs/protocol/LocatedStripedBlock.java' sourcefile='LocatedStripedBlock.java'><Message>At LocatedStripedBlock.java:[lines 36-86]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.protocol.LocatedStripedBlock</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='16' name='lb' register='1'><Message>Value loaded from lb</Message></LocalVariable><SourceLine endBytecode='17' classname='org.apache.hadoop.hdfs.server.mover.Mover' start='195' end='195' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java' startBytecode='17' primary='true'><Message>At Mover.java:[line 195]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='43e1b5a598fc2b67c71a465a12745c4d' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.mover.Mover$MLocation.size</LongMessage><Class classname='org.apache.hadoop.hdfs.server.mover.Mover$MLocation' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.mover.Mover$MLocation' start='547' end='561' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java'><Message>At Mover.java:[lines 547-561]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.mover.Mover$MLocation</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.mover.Mover$MLocation' signature='J' name='size' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.mover.Mover$MLocation' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java'><Message>In Mover.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.mover.Mover$MLocation.size</Message></Field><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.mover.Mover$MLocation' start='550' end='550' sourcepath='org/apache/hadoop/hdfs/server/mover/Mover.java' sourcefile='Mover.java' startBytecode='16' primary='true'><Message>At Mover.java:[line 550]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a290d945e2a56692b503ebf456dc1df0' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.namenode.AclFeature(int[]) may expose internal representation by storing an externally mutable object into AclFeature.entries</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' start='35' end='92' sourcepath='org/apache/hadoop/hdfs/server/namenode/AclFeature.java' sourcefile='AclFeature.java'><Message>At AclFeature.java:[lines 35-92]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.AclFeature</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' signature='([I)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='78' classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' start='41' end='43' sourcepath='org/apache/hadoop/hdfs/server/namenode/AclFeature.java' sourcefile='AclFeature.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.AclFeature(int[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' signature='[I' name='entries' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' sourcepath='org/apache/hadoop/hdfs/server/namenode/AclFeature.java' sourcefile='AclFeature.java'><Message>In AclFeature.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.AclFeature.entries</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='entries' register='1'><Message>Local variable named entries</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.AclFeature' start='42' end='42' sourcepath='org/apache/hadoop/hdfs/server/namenode/AclFeature.java' sourcefile='AclFeature.java' startBytecode='11' primary='true'><Message>At AclFeature.java:[line 42]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='34cc29a45422e7357e493d8d2b1ced08' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSImage to org.apache.hadoop.hdfs.server.namenode.BackupImage of return value in org.apache.hadoop.hdfs.server.namenode.BackupNode.registerWith(NamespaceInfo)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.BackupNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.BackupNode' start='70' end='442' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java'><Message>At BackupNode.java:[lines 70-442]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.BackupNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode' signature='(Lorg/apache/hadoop/hdfs/server/protocol/NamespaceInfo;)V' name='registerWith' primary='true'><SourceLine endBytecode='118' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode' start='369' end='408' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.BackupNode.registerWith(NamespaceInfo)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/BackupImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.BackupImage' start='41' end='391' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupImage.java' sourcefile='BackupImage.java'><Message>At BackupImage.java:[lines 41-391]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.BackupImage</Message></Type><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode' start='369' end='369' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java' startBytecode='4' primary='true'><Message>At BackupNode.java:[line 369]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='344cf018d226148a0f505c05016e5bf3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSImage to org.apache.hadoop.hdfs.server.namenode.BackupImage of return value in org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer.getBNImage()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer' start='238' end='300' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java'><Message>At BackupNode.java:[lines 238-300]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer' signature='()Lorg/apache/hadoop/hdfs/server/namenode/BackupImage;' name='getBNImage' primary='true'><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer' start='293' end='293' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer.getBNImage()</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/BackupImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.BackupImage' start='41' end='391' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupImage.java' sourcefile='BackupImage.java'><Message>At BackupImage.java:[lines 41-391]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.BackupImage</Message></Type><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer' start='293' end='293' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' sourcefile='BackupNode.java' startBytecode='7' primary='true'><Message>At BackupNode.java:[line 293]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f0e0f1dd0dd937f6f20e74d0c640c3f8' rank='17' abbrev='BSHIFT' category='STYLE' priority='2' type='ICAST_QUESTIONABLE_UNSIGNED_RIGHT_SHIFT' instanceOccurrenceMax='0'><ShortMessage>Unsigned right shift cast to short/byte</ShortMessage><LongMessage>Unsigned right shift cast to short/byte in org.apache.hadoop.hdfs.server.namenode.CachedBlock.getReplication()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.CachedBlock' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.CachedBlock' start='36' end='249' sourcepath='org/apache/hadoop/hdfs/server/namenode/CachedBlock.java' sourcefile='CachedBlock.java'><Message>At CachedBlock.java:[lines 36-249]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.CachedBlock</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.CachedBlock' signature='()S' name='getReplication' primary='true'><SourceLine endBytecode='49' classname='org.apache.hadoop.hdfs.server.namenode.CachedBlock' start='105' end='105' sourcepath='org/apache/hadoop/hdfs/server/namenode/CachedBlock.java' sourcefile='CachedBlock.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.CachedBlock.getReplication()</Message></Method><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.namenode.CachedBlock' start='105' end='105' sourcepath='org/apache/hadoop/hdfs/server/namenode/CachedBlock.java' sourcefile='CachedBlock.java' startBytecode='6' primary='true'><Message>At CachedBlock.java:[line 105]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e4a6de5e0aedaa37129c5909791a6886' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector.instance should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector' start='27' end='55' sourcepath='org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.java' sourcefile='CheckpointFaultInjector.java'><Message>At CheckpointFaultInjector.java:[lines 27-55]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector' signature='Lorg/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector;' name='instance' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector' sourcepath='org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.java' sourcefile='CheckpointFaultInjector.java'><Message>In CheckpointFaultInjector.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector.instance</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector' start='28' end='28' sourcepath='org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.java' sourcefile='CheckpointFaultInjector.java' startBytecode='7' primary='true'><Message>At CheckpointFaultInjector.java:[line 28]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d21de92a914b15a053e9edf683e6519' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.protocol.NamenodeCommand to org.apache.hadoop.hdfs.server.protocol.CheckpointCommand of return value in org.apache.hadoop.hdfs.server.namenode.Checkpointer.doCheckpoint()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='58' end='317' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java'><Message>At Checkpointer.java:[lines 58-317]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.Checkpointer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' signature='()V' name='doCheckpoint' primary='true'><SourceLine endBytecode='316' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='182' end='286' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.Checkpointer.doCheckpoint()</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/NamenodeCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.NamenodeCommand' start='31' end='32' sourcepath='org/apache/hadoop/hdfs/server/protocol/NamenodeCommand.java' sourcefile='NamenodeCommand.java'><Message>At NamenodeCommand.java:[lines 31-32]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.protocol.NamenodeCommand</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/protocol/CheckpointCommand;'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.CheckpointCommand' start='46' end='71' sourcepath='org/apache/hadoop/hdfs/server/protocol/CheckpointCommand.java' sourcefile='CheckpointCommand.java'><Message>At CheckpointCommand.java:[lines 46-71]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.protocol.CheckpointCommand</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='114' name='cmd' register='5'><Message>Value loaded from cmd</Message></LocalVariable><SourceLine endBytecode='116' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='197' end='197' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='116' primary='true'><Message>At Checkpointer.java:[line 197]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5bb7bd7511f8649efccd3ebffd22709' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSImage to org.apache.hadoop.hdfs.server.namenode.BackupImage of return value in org.apache.hadoop.hdfs.server.namenode.Checkpointer.getFSImage()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='58' end='317' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java'><Message>At Checkpointer.java:[lines 58-317]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.Checkpointer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' signature='()Lorg/apache/hadoop/hdfs/server/namenode/BackupImage;' name='getFSImage' primary='true'><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='71' end='71' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.Checkpointer.getFSImage()</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/BackupImage;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.BackupImage' start='41' end='391' sourcepath='org/apache/hadoop/hdfs/server/namenode/BackupImage.java' sourcefile='BackupImage.java'><Message>At BackupImage.java:[lines 41-391]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.BackupImage</Message></Type><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='71' end='71' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='7' primary='true'><Message>At Checkpointer.java:[line 71]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e855bb77a85775da17bd7fec1d593a25' rank='20' abbrev='UPM' category='PERFORMANCE' priority='3' type='UPM_UNCALLED_PRIVATE_METHOD' instanceOccurrenceMax='0'><ShortMessage>Private method is never called</ShortMessage><LongMessage>Private method org.apache.hadoop.hdfs.server.namenode.Checkpointer.getImageListenAddress() is never called</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='58' end='317' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java'><Message>At Checkpointer.java:[lines 58-317]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.Checkpointer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' signature='()Ljava/net/URL;' name='getImageListenAddress' primary='true'><SourceLine endBytecode='192' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='289' end='295' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.Checkpointer.getImageListenAddress()</Message></Method><SourceLine synthetic='true' endBytecode='192' classname='org.apache.hadoop.hdfs.server.namenode.Checkpointer' start='289' end='295' sourcepath='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' sourcefile='Checkpointer.java' startBytecode='0'><Message>At Checkpointer.java:[lines 289-295]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b70b9f7296bf1d6365438e77d1f82e01' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.namenode.ContentCounts.getStoragespace() and org.apache.hadoop.hdfs.server.namenode.QuotaCounts.getStorageSpace()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ContentCounts' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ContentCounts' start='27' end='145' sourcepath='org/apache/hadoop/hdfs/server/namenode/ContentCounts.java' sourcefile='ContentCounts.java'><Message>At ContentCounts.java:[lines 27-145]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ContentCounts</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ContentCounts' signature='()J' name='getStoragespace' primary='true'><SourceLine endBytecode='52' classname='org.apache.hadoop.hdfs.server.namenode.ContentCounts' start='109' end='109' sourcepath='org/apache/hadoop/hdfs/server/namenode/ContentCounts.java' sourcefile='ContentCounts.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ContentCounts.getStoragespace()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' start='27' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java' sourcefile='QuotaCounts.java'><Message>At QuotaCounts.java:[lines 27-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.QuotaCounts</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' signature='()J' name='getStorageSpace'><SourceLine endBytecode='52' classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' start='118' end='118' sourcepath='org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java' sourcefile='QuotaCounts.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.QuotaCounts.getStorageSpace()</Message></Method><SourceLine synthetic='true' endBytecode='52' classname='org.apache.hadoop.hdfs.server.namenode.ContentCounts' start='109' end='109' sourcepath='org/apache/hadoop/hdfs/server/namenode/ContentCounts.java' sourcefile='ContentCounts.java' startBytecode='0'><Message>At ContentCounts.java:[line 109]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b2b50801402f326add6d11417f10d03' cweid='218' rank='20' abbrev='MS' category='MALICIOUS_CODE' priority='3' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider.DEFAULT_PROVIDER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' start='24' end='42' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java'><Message>At DefaultINodeAttributesProvider.java:[lines 24-42]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' signature='Lorg/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider;' name='DEFAULT_PROVIDER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java'><Message>In DefaultINodeAttributesProvider.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider.DEFAULT_PROVIDER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' start='26' end='26' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java' startBytecode='7' primary='true'><Message>At DefaultINodeAttributesProvider.java:[line 26]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3ab4a7f00070aed2a19285349574ba5' rank='20' abbrev='UrF' category='STYLE' priority='3' type='URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread public/protected field</ShortMessage><LongMessage>Unread public/protected field: org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider.DEFAULT_PROVIDER</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' start='24' end='42' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java'><Message>At DefaultINodeAttributesProvider.java:[lines 24-42]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' signature='Lorg/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider;' name='DEFAULT_PROVIDER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java'><Message>In DefaultINodeAttributesProvider.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider.DEFAULT_PROVIDER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider' start='26' end='26' sourcepath='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' sourcefile='DefaultINodeAttributesProvider.java' startBytecode='7' primary='true'><Message>At DefaultINodeAttributesProvider.java:[line 26]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='60e56c1e1c47534c280c9a7e085bab6f' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_METHOD_NAMING_CONVENTION' instanceOccurrenceMax='0'><ShortMessage>Method names should start with a lower case letter</ShortMessage><LongMessage>The method name org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.AddCurrentSpaceUsage(QuotaCounts) doesn't start with a lower case letter</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature' start='33' end='261' sourcepath='org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java' sourcefile='DirectoryWithQuotaFeature.java'><Message>At DirectoryWithQuotaFeature.java:[lines 33-261]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature' signature='(Lorg/apache/hadoop/hdfs/server/namenode/QuotaCounts;)Lorg/apache/hadoop/hdfs/server/namenode/QuotaCounts;' name='AddCurrentSpaceUsage' primary='true'><SourceLine endBytecode='66' classname='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature' start='124' end='125' sourcepath='org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java' sourcefile='DirectoryWithQuotaFeature.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.AddCurrentSpaceUsage(QuotaCounts)</Message></Method><SourceLine synthetic='true' endBytecode='66' classname='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature' start='124' end='125' sourcepath='org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java' sourcefile='DirectoryWithQuotaFeature.java' startBytecode='0'><Message>At DirectoryWithQuotaFeature.java:[lines 124-125]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f6be26c29e6c99762da39f6237b71118' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector.instance isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector' start='27' end='56' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.java' sourcefile='EncryptionFaultInjector.java'><Message>At EncryptionFaultInjector.java:[lines 27-56]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector' signature='Lorg/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector;' name='instance' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.java' sourcefile='EncryptionFaultInjector.java'><Message>In EncryptionFaultInjector.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector.instance</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector' start='29' end='29' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.java' sourcefile='EncryptionFaultInjector.java' startBytecode='7' primary='true'><Message>At EncryptionFaultInjector.java:[line 29]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='404c0967d8ef66d20eace2b0c919183a' rank='19' abbrev='Nm' category='BAD_PRACTICE' priority='3' type='NM_CONFUSING' instanceOccurrenceMax='0'><ShortMessage>Confusing method names</ShortMessage><LongMessage>Confusing to have methods org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt.getINodeId() and org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry.getInodeId()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt' start='92' end='140' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java' sourcefile='EncryptionZoneManager.java'><Message>At EncryptionZoneManager.java:[lines 92-140]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt' signature='()J' name='getINodeId' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt' start='102' end='102' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java' sourcefile='EncryptionZoneManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt.getINodeId()</Message></Method><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' start='12346' end='12862' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12346-12862]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' signature='()J' name='getInodeId'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' start='12457' end='12457' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry.getInodeId()</Message></Method><SourceLine synthetic='true' endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt' start='102' end='102' sourcepath='org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java' sourcefile='EncryptionZoneManager.java' startBytecode='0'><Message>At EncryptionZoneManager.java:[line 102]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2c617aee01a4b6201283724f35eb5c91' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.getEnabledPolicies() may expose internal representation by returning ErasureCodingPolicyManager.enabledPolicies</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='427' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>At ErasureCodingPolicyManager.java:[lines 53-427]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='()[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;' name='getEnabledPolicies' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='162' end='162' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.getEnabledPolicies()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;' name='enabledPolicies' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>In ErasureCodingPolicyManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.enabledPolicies</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='162' end='162' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='4' primary='true'><Message>At ErasureCodingPolicyManager.java:[line 162]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bfcce6c391ea8c25a1c42919fefd82d1' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.getPolicies() may expose internal representation by returning ErasureCodingPolicyManager.allPolicies</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='427' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>At ErasureCodingPolicyManager.java:[lines 53-427]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='()[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicyInfo;' name='getPolicies' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='199' end='199' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.getPolicies()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicyInfo;' name='allPolicies' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>In ErasureCodingPolicyManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.allPolicies</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='199' end='199' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='4' primary='true'><Message>At ErasureCodingPolicyManager.java:[line 199]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e83591e36b70d684aab6df609c699' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.allPolicies; locked 66% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='427' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>At ErasureCodingPolicyManager.java:[lines 53-427]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicyInfo;' name='allPolicies' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>In ErasureCodingPolicyManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.allPolicies</Message></Field><Int role='INT_SYNC_PERCENT' value='66'><Message>Synchronized 66% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='199' end='199' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='1' primary='true'><Message>Unsynchronized access at ErasureCodingPolicyManager.java:[line 199]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='296' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='150' end='150' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='296'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 150]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='385' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='300' end='300' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='385'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 300]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='426' end='426' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='61'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 426]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7ef1f95cea20a333c168716676382af2' cweid='366' rank='20' abbrev='IS' category='MT_CORRECTNESS' priority='3' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.enabledPolicies; locked 75% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='427' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>At ErasureCodingPolicyManager.java:[lines 53-427]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='[Lorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;' name='enabledPolicies' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>In ErasureCodingPolicyManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.enabledPolicies</Message></Field><Int role='INT_SYNC_PERCENT' value='75'><Message>Synchronized 75% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='162' end='162' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='1' primary='true'><Message>Unsynchronized access at ErasureCodingPolicyManager.java:[line 162]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='140' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='333' end='333' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='140'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 333]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='271' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='148' end='148' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='271'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 148]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='96' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='369' end='369' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='96'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 369]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='96' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='389' end='389' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='96'><Message>Synchronized access at ErasureCodingPolicyManager.java:[line 389]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.InconsistentSyncWarningProperty.ONLY_UNSYNC_IN_GETTERS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='392a6ba6825894909e929a09a4c7d314' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.LOG isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='427' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>At ErasureCodingPolicyManager.java:[lines 53-427]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' signature='Lorg/slf4j/Logger;' name='LOG' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java'><Message>In ErasureCodingPolicyManager.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.LOG</Message></Field><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager' start='53' end='53' sourcepath='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' sourcefile='ErasureCodingPolicyManager.java' startBytecode='5' primary='true'><Message>At ErasureCodingPolicyManager.java:[line 53]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7a8b3bcb39a10e267b5b4c0a1b93ebc9' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetReplication(FSDirectory, INodesInPath, short) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='52' end='495' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java'><Message>At FSDirAttrOp.java:[lines 52-495]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/INodesInPath;S)[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='unprotectedSetReplication' primary='true'><SourceLine endBytecode='611' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='373' end='415' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetReplication(FSDirectory, INodesInPath, short)</Message></Method><SourceLine endBytecode='57' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='379' end='379' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='57' primary='true'><Message>At FSDirAttrOp.java:[line 379]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cd48b961a1bbe4802ed1584d275b3752' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of oldTypeQuotas, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota(FSDirectory, INodesInPath, long, long, StorageType)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='52' end='495' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java'><Message>At FSDirAttrOp.java:[lines 52-495]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/INodesInPath;JJLorg/apache/hadoop/fs/StorageType;)Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;' name='unprotectedSetQuota' primary='true'><SourceLine endBytecode='626' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='313' end='365' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota(FSDirectory, INodesInPath, long, long, StorageType)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='282' name='oldTypeQuotas' register='13'><Message>Value loaded from oldTypeQuotas</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' signature='()Lorg/apache/hadoop/hdfs/util/EnumCounters;' name='getTypeSpaces'><SourceLine endBytecode='97' classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' start='130' end='133' sourcepath='org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java' sourcefile='QuotaCounts.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.QuotaCounts.getTypeSpaces() of type org.apache.hadoop.hdfs.util.EnumCounters</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='284' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='357' end='357' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='284' primary='true'><Message>Redundant null check at FSDirAttrOp.java:[line 357]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='72dd4edf20b04f8d9df08c6bbcddb698' rank='19' abbrev='UC' category='STYLE' priority='3' type='UC_USELESS_CONDITION' instanceOccurrenceMax='0'><ShortMessage>Condition has no effect</ShortMessage><LongMessage>Useless condition: it's known that nsQuota != Long.MAX_VALUE at this point</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='52' end='495' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java'><Message>At FSDirAttrOp.java:[lines 52-495]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/INodesInPath;JJLorg/apache/hadoop/fs/StorageType;)Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;' name='unprotectedSetQuota' primary='true'><SourceLine endBytecode='626' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='313' end='365' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota(FSDirectory, INodesInPath, long, long, StorageType)</Message></Method><String value='nsQuota != Long.MAX_VALUE'><Message>Value nsQuota != Long.MAX_VALUE</Message></String><SourceLine endBytecode='32' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='315' end='315' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='32' primary='true'><Message>At FSDirAttrOp.java:[line 315]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d47c936182c2e1b0eefdf89ae75b5276' rank='19' abbrev='UC' category='STYLE' priority='3' type='UC_USELESS_CONDITION' instanceOccurrenceMax='0'><ShortMessage>Condition has no effect</ShortMessage><LongMessage>Useless condition: it's known that ssQuota != Long.MAX_VALUE at this point</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='52' end='495' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java'><Message>At FSDirAttrOp.java:[lines 52-495]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/INodesInPath;JJLorg/apache/hadoop/fs/StorageType;)Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;' name='unprotectedSetQuota' primary='true'><SourceLine endBytecode='626' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='313' end='365' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetQuota(FSDirectory, INodesInPath, long, long, StorageType)</Message></Method><String value='ssQuota != Long.MAX_VALUE'><Message>Value ssQuota != Long.MAX_VALUE</Message></String><SourceLine endBytecode='56' classname='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp' start='315' end='315' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' sourcefile='FSDirAttrOp.java' startBytecode='56' primary='true'><Message>At FSDirAttrOp.java:[line 315]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='70fc6b45210a699587d96f6db8f7a89a' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo.ezKeyName</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo' start='678' end='683' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java' sourcefile='FSDirEncryptionZoneOp.java'><Message>At FSDirEncryptionZoneOp.java:[lines 678-683]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo' signature='Ljava/lang/String;' name='ezKeyName' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java' sourcefile='FSDirEncryptionZoneOp.java'><Message>In FSDirEncryptionZoneOp.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo.ezKeyName</Message></Field><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo' start='681' end='681' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java' sourcefile='FSDirEncryptionZoneOp.java' startBytecode='16' primary='true'><Message>At FSDirEncryptionZoneOp.java:[line 681]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8185f1ea5fa6066f05d449e64458e2a0' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.getSnapshotDiffReportListing(FSDirectory, FSPermissionChecker, SnapshotManager, String, String, String, byte[], int, int)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' start='43' end='329' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' sourcefile='FSDirSnapshotOp.java'><Message>At FSDirSnapshotOp.java:[lines 43-329]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/FSPermissionChecker;Lorg/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager;Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;[BII)Lorg/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing;' name='getSnapshotDiffReportListing' primary='true'><SourceLine endBytecode='376' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' start='175' end='190' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' sourcefile='FSDirSnapshotOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.getSnapshotDiffReportListing(FSDirectory, FSPermissionChecker, SnapshotManager, String, String, String, byte[], int, int)</Message></Method><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' start='185' end='185' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' sourcefile='FSDirSnapshotOp.java' startBytecode='64' primary='true'><Message>At FSDirSnapshotOp.java:[line 185]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='29d98fcfad3e9de86c15ecd878da4476' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSDirectory.DOT_INODES should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' start='103' end='1970' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 103-1970]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' signature='[B' name='DOT_INODES' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirectory.DOT_INODES</Message></Field><SourceLine endBytecode='54' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' start='136' end='136' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java' startBytecode='54' primary='true'><Message>At FSDirectory.java:[line 136]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='29b351b10b3c926a9699aebd222dda0a' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSDirectory.DOT_RESERVED should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' start='103' end='1970' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 103-1970]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' signature='[B' name='DOT_RESERVED' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirectory.DOT_RESERVED</Message></Field><SourceLine endBytecode='36' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory' start='130' end='130' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java' startBytecode='36' primary='true'><Message>At FSDirectory.java:[line 130]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c536ec5f7bf58c4f60b9d1041b123c09' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask defines non-transient non-serializable instance field bsps</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' start='797' end='871' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 797-871]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' signature='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite;' name='bsps' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask.bsps</Message></Field><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite;'><SourceLine classname='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite' start='37' end='150' sourcepath='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' sourcefile='BlockStoragePolicySuite.java'><Message>At BlockStoragePolicySuite.java:[lines 37-150]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e52b42114bd814f637779430b1cbbc5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask defines non-transient non-serializable instance field counts</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' start='797' end='871' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 797-871]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' signature='Lorg/apache/hadoop/hdfs/server/namenode/QuotaCounts;' name='counts' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask.counts</Message></Field><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/QuotaCounts;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.QuotaCounts' start='27' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java' sourcefile='QuotaCounts.java'><Message>At QuotaCounts.java:[lines 27-184]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.QuotaCounts</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f7c70dd28269dc04cefb3299752c1d0b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask defines non-transient non-serializable instance field dir</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' start='797' end='871' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 797-871]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' signature='Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;' name='dir' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask.dir</Message></Field><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeDirectory' start='52' end='950' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java' sourcefile='INodeDirectory.java'><Message>At INodeDirectory.java:[lines 52-950]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.INodeDirectory</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>In FSDirectory.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b31de529ef2cb1dec912e818b7e08393' rank='19' abbrev='SnVI' category='BAD_PRACTICE' priority='3' type='SE_NO_SERIALVERSIONID' instanceOccurrenceMax='0'><ShortMessage>Class is Serializable, but doesn't define serialVersionUID</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask is Serializable; consider declaring a serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' start='797' end='871' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 797-871]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask' start='797' end='871' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' sourcefile='FSDirectory.java'><Message>At FSDirectory.java:[lines 797-871]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c68828627721e52a8ac5d078a37fb8e1' cweid='366' rank='17' abbrev='IS' category='MT_CORRECTNESS' priority='2' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSEditLog.editLogStream; locked 78% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='122' end='1850' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>At FSEditLog.java:[lines 122-1850]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLog</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='Lorg/apache/hadoop/hdfs/server/namenode/EditLogOutputStream;' name='editLogStream' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>In FSEditLog.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLog.editLogStream</Message></Field><Int role='INT_SYNC_PERCENT' value='78'><Message>Synchronized 78% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1840' end='1840' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1' primary='true'><Message>Unsynchronized access at FSEditLog.java:[line 1840]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='12' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1845' end='1845' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='12'><Message>Unsynchronized access at FSEditLog.java:[line 1845]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='480' end='480' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='14'><Message>Synchronized access at FSEditLog.java:[line 480]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='6' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1587' end='1587' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='6'><Message>Synchronized access at FSEditLog.java:[line 1587]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='210' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='685' end='685' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='210'><Message>Synchronized access at FSEditLog.java:[line 685]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='342' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='703' end='703' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='342'><Message>Synchronized access at FSEditLog.java:[line 703]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='238' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1382' end='1382' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='238'><Message>Synchronized access at FSEditLog.java:[line 1382]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='519' end='519' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 519]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1441' end='1441' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1441]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1442' end='1442' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='8'><Message>Synchronized access at FSEditLog.java:[line 1442]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1443' end='1443' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='16'><Message>Synchronized access at FSEditLog.java:[line 1443]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='156' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1427' end='1427' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='156'><Message>Synchronized access at FSEditLog.java:[line 1427]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='104' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='771' end='771' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='104'><Message>Synchronized access at FSEditLog.java:[line 771]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cb7e6323705e997d940af50130666627' cweid='366' rank='17' abbrev='IS' category='MT_CORRECTNESS' priority='2' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSEditLog.journalSet; locked 85% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='122' end='1850' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>At FSEditLog.java:[lines 122-1850]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLog</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='Lorg/apache/hadoop/hdfs/server/namenode/JournalSet;' name='journalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>In FSEditLog.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLog.journalSet</Message></Field><Int role='INT_SYNC_PERCENT' value='85'><Message>Synchronized 85% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1274' end='1274' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1' primary='true'><Message>Unsynchronized access at FSEditLog.java:[line 1274]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1610' end='1610' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Unsynchronized access at FSEditLog.java:[line 1610]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1672' end='1672' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Unsynchronized access at FSEditLog.java:[line 1672]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1282' end='1282' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Unsynchronized access at FSEditLog.java:[line 1282]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='18' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1703' end='1703' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='18'><Message>Unsynchronized access at FSEditLog.java:[line 1703]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='33' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1602' end='1602' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='33'><Message>Synchronized access at FSEditLog.java:[line 1602]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1303' end='1303' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1303]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1506' end='1506' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1506]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1635' end='1635' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1635]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1654' end='1654' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1654]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1627' end='1627' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1627]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='48' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='325' end='325' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='48'><Message>Synchronized access at FSEditLog.java:[line 325]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='36' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='436' end='436' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='36'><Message>Synchronized access at FSEditLog.java:[line 436]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='649' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='737' end='737' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='649'><Message>Synchronized access at FSEditLog.java:[line 737]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='190' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='682' end='682' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='190'><Message>Synchronized access at FSEditLog.java:[line 682]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='84' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='737' end='737' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='84'><Message>Synchronized access at FSEditLog.java:[line 737]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='550' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='737' end='737' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='550'><Message>Synchronized access at FSEditLog.java:[line 737]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='298' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='693' end='693' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='298'><Message>Synchronized access at FSEditLog.java:[line 693]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='450' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='719' end='719' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='450'><Message>Synchronized access at FSEditLog.java:[line 719]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1644' end='1644' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1644]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='32' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='424' end='424' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='32'><Message>Synchronized access at FSEditLog.java:[line 424]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='230' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1382' end='1382' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='230'><Message>Synchronized access at FSEditLog.java:[line 1382]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1619' end='1619' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1619]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='27' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='282' end='282' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='27'><Message>Synchronized access at FSEditLog.java:[line 282]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='187' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='300' end='300' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='187'><Message>Synchronized access at FSEditLog.java:[line 300]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='146' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='294' end='294' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='146'><Message>Synchronized access at FSEditLog.java:[line 294]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='106' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='290' end='290' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='106'><Message>Synchronized access at FSEditLog.java:[line 290]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='2' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1287' end='1287' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='2'><Message>Synchronized access at FSEditLog.java:[line 1287]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1663' end='1663' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1663]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1559' end='1559' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1559]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='143' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1426' end='1426' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='143'><Message>Synchronized access at FSEditLog.java:[line 1426]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1547' end='1547' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='46'><Message>Synchronized access at FSEditLog.java:[line 1547]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='104' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1537' end='1537' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='104'><Message>Synchronized access at FSEditLog.java:[line 1537]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='83' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1476' end='1476' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='83'><Message>Synchronized access at FSEditLog.java:[line 1476]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='125' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='773' end='773' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='125'><Message>Synchronized access at FSEditLog.java:[line 773]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c56e9ba5b364b72cc763411a226878a9' cweid='366' rank='17' abbrev='IS' category='MT_CORRECTNESS' priority='2' type='IS2_INCONSISTENT_SYNC' instanceOccurrenceMax='0'><ShortMessage>Inconsistent synchronization</ShortMessage><LongMessage>Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSEditLog.state; locked 90% of time</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='122' end='1850' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>At FSEditLog.java:[lines 122-1850]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLog</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLog$State;' name='state' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>In FSEditLog.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLog.state</Message></Field><Int role='INT_SYNC_PERCENT' value='90'><Message>Synchronized 90% of the time</Message></Int><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='355' end='355' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1' primary='true'><Message>Unsynchronized access at FSEditLog.java:[line 355]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='355' end='355' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='11'><Message>Unsynchronized access at FSEditLog.java:[line 355]</Message></SourceLine><SourceLine role='SOURCE_LINE_UNSYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='374' end='374' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Unsynchronized access at FSEditLog.java:[line 374]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1598' end='1598' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 1598]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='25' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1598' end='1598' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='25'><Message>Synchronized access at FSEditLog.java:[line 1598]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='344' end='344' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 344]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='344' end='344' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='11'><Message>Synchronized access at FSEditLog.java:[line 344]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='318' end='318' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 318]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='24' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='318' end='318' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='24'><Message>Synchronized access at FSEditLog.java:[line 318]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='141' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='336' end='336' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='141'><Message>Synchronized access at FSEditLog.java:[line 336]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='167' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='336' end='336' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='167'><Message>Synchronized access at FSEditLog.java:[line 336]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='432' end='432' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 432]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='24' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='432' end='432' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='24'><Message>Synchronized access at FSEditLog.java:[line 432]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='264' end='264' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 264]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='29' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='269' end='269' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='29'><Message>Synchronized access at FSEditLog.java:[line 269]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='39' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='269' end='269' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='39'><Message>Synchronized access at FSEditLog.java:[line 269]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='68' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='273' end='273' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='68'><Message>Synchronized access at FSEditLog.java:[line 273]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='256' end='256' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 256]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='256' end='256' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='11'><Message>Synchronized access at FSEditLog.java:[line 256]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='34' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='256' end='256' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='34'><Message>Synchronized access at FSEditLog.java:[line 256]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='53' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='260' end='260' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='53'><Message>Synchronized access at FSEditLog.java:[line 260]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='381' end='381' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 381]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='421' end='421' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 421]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='24' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='421' end='421' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='24'><Message>Synchronized access at FSEditLog.java:[line 421]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='78' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1364' end='1364' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='78'><Message>Synchronized access at FSEditLog.java:[line 1364]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='101' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1364' end='1364' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='101'><Message>Synchronized access at FSEditLog.java:[line 1364]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='291' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1389' end='1389' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='291'><Message>Synchronized access at FSEditLog.java:[line 1389]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='23' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1444' end='1444' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='23'><Message>Synchronized access at FSEditLog.java:[line 1444]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='13' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='566' end='566' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='13'><Message>Synchronized access at FSEditLog.java:[line 566]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='364' end='364' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='1'><Message>Synchronized access at FSEditLog.java:[line 364]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='57' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1408' end='1408' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='57'><Message>Synchronized access at FSEditLog.java:[line 1408]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='168' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1432' end='1432' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='168'><Message>Synchronized access at FSEditLog.java:[line 1432]</Message></SourceLine><SourceLine role='SOURCE_LINE_SYNC_ACCESS' endBytecode='36' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='456' end='456' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='36'><Message>Synchronized access at FSEditLog.java:[line 456]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fda4621ac4cc73824e2dd9adcafb91ee' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(URI)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='122' end='1850' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>At FSEditLog.java:[lines 122-1850]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLog</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='(Ljava/net/URI;)Lorg/apache/hadoop/hdfs/server/namenode/JournalManager;' name='createJournal' primary='true'><SourceLine endBytecode='521' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1802' end='1823' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(URI)</Message></Method><SourceLine endBytecode='156' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='1818' end='1818' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='156' primary='true'><Message>At FSEditLog.java:[line 1818]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='21a8f17ac50060371670c7c6bf1a4636' rank='12' abbrev='VO' category='MT_CORRECTNESS' priority='1' type='VO_VOLATILE_INCREMENT' instanceOccurrenceMax='0'><ShortMessage>An increment to a volatile field isn't atomic</ShortMessage><LongMessage>Increment of volatile field org.apache.hadoop.hdfs.server.namenode.FSEditLog.txid in org.apache.hadoop.hdfs.server.namenode.FSEditLog.beginTransaction()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='122' end='1850' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>At FSEditLog.java:[lines 122-1850]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLog</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='()J' name='beginTransaction' primary='true'><SourceLine endBytecode='129' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='523' end='532' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLog.beginTransaction()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' signature='J' name='txid' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java'><Message>In FSEditLog.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLog.txid</Message></Field><SourceLine endBytecode='28' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLog' start='525' end='525' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' sourcefile='FSEditLog.java' startBytecode='28' primary='true'><Message>At FSEditLog.java:[line 525]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a2c108164c94bc8755209978b5f44c2c' cweid='563' rank='17' abbrev='DLS' category='STYLE' priority='2' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to $L8 in org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount(FSEditLogOpCodes, EnumMap, Step, StartupProgress$Counter)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader' start='121' end='1398' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java' sourcefile='FSEditLogLoader.java'><Message>At FSEditLogLoader.java:[lines 121-1398]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOpCodes;Ljava/util/EnumMap;Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/Step;Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress$Counter;)V' name='incrOpCount' primary='true'><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader' start='1199' end='1207' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java' sourcefile='FSEditLogLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.incrOpCount(FSEditLogOpCodes, EnumMap, Step, StartupProgress$Counter)</Message></Method><LocalVariable role='LOCAL_VARIABLE_UNKNOWN' pc='75' name='?' register='8'><Message>Local variable stored in JVM register 8</Message></LocalVariable><SourceLine endBytecode='75' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader' start='1204' end='1204' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java' sourcefile='FSEditLogLoader.java' startBytecode='75' primary='true'><Message>At FSEditLogLoader.java:[line 1204]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='?'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d57b93b675468730cf20fd65fded70c8' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getClientId() may expose internal representation by returning FSEditLogOp.rpcClientId</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' signature='()[B' name='getClientId' primary='true'><SourceLine endBytecode='79' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='261' end='262' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.getClientId()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' signature='[B' name='rpcClientId' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>In FSEditLogOp.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.rpcClientId</Message></Field><SourceLine endBytecode='22' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='262' end='262' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java' startBytecode='22' primary='true'><Message>At FSEditLogOp.java:[line 262]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7f7588942e2ffa2d13f3546fc26b959' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.setRpcClientId(byte[]) may expose internal representation by storing an externally mutable object into FSEditLogOp.rpcClientId</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' signature='([B)V' name='setRpcClientId' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='266' end='267' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.setRpcClientId(byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' signature='[B' name='rpcClientId' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>In FSEditLogOp.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.rpcClientId</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='2' name='clientId' register='1'><Message>Local variable named clientId</Message></LocalVariable><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='266' end='266' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java' startBytecode='2' primary='true'><Message>At FSEditLogOp.java:[line 266]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e07cf16ca1b24eceab8ecfe4176de346' rank='19' abbrev='SnVI' category='BAD_PRACTICE' priority='3' type='SE_NO_SERIALVERSIONID' instanceOccurrenceMax='0'><ShortMessage>Class is Serializable, but doesn't define serialVersionUID</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap is Serializable; consider declaring a serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap' start='172' end='176' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 172-176]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap' start='172' end='176' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 172-176]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bd46aafd77582276ca4f03fe44f799ed' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of checkpointDirs, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint(FSNamesystem)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSNamesystem;)V' name='doImportCheckpoint' primary='true'><SourceLine endBytecode='444' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='562' end='596' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint(FSNamesystem)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='18' name='checkpointDirs' register='2'><Message>Value loaded from checkpointDirs</Message></LocalVariable><Method isStatic='true' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;)Ljava/util/Collection;' name='getCheckpointDirs'><SourceLine endBytecode='139' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='1449' end='1454' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointDirs(Configuration, String) of type java.util.Collection</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='19' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='567' end='567' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='19' primary='true'><Message>Redundant null check at FSImage.java:[line 567]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='329009384417a7b658d2397caf77276a' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of checkpointEditsDirs, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint(FSNamesystem)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSNamesystem;)V' name='doImportCheckpoint' primary='true'><SourceLine endBytecode='444' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='562' end='596' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImage.doImportCheckpoint(FSNamesystem)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='41' name='checkpointEditsDirs' register='3'><Message>Value loaded from checkpointEditsDirs</Message></LocalVariable><Method isStatic='true' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;)Ljava/util/List;' name='getCheckpointEditsDirs'><SourceLine endBytecode='139' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='1459' end='1464' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.FSImage.getCheckpointEditsDirs(Configuration, String) of type java.util.List</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='42' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='572' end='572' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='42' primary='true'><Message>Redundant null check at FSImage.java:[line 572]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='96a66719b7d6bf2915b33cba803f4b71' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of images, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSImage.hasRollbackFSImage()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='83' end='1514' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java'><Message>At FSImage.java:[lines 83-1514]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' signature='()Z' name='hasRollbackFSImage' primary='true'><SourceLine endBytecode='205' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='426' end='433' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImage.hasRollbackFSImage()</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='27' name='images' register='2'><Message>Value loaded from images</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector' signature='()Ljava/util/List;' name='getLatestImages'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageStorageInspector.java' sourcefile='FSImageStorageInspector.java'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector.getLatestImages() of type java.util.List</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='28' classname='org.apache.hadoop.hdfs.server.namenode.FSImage' start='431' end='431' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImage.java' sourcefile='FSImage.java' startBytecode='28' primary='true'><Message>Redundant null check at FSImage.java:[line 431]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4576b7ba2ba0ffc1cf02e1bf03a9b136' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_MUTABLE_COLLECTION_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable collection which should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSImageFormat.renameReservedMap is a mutable collection which should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat' start='183' end='1184' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>At FSImageFormat.java:[lines 183-1184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormat</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat' signature='Ljava/util/TreeMap;' name='renameReservedMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>In FSImageFormat.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSImageFormat.renameReservedMap</Message></Field><SourceLine endBytecode='13' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat' start='1041' end='1041' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java' startBytecode='13' primary='true'><Message>At FSImageFormat.java:[line 1041]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dae53baf073be16a8c3b1dffe7491171' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>FSImageFormat$LoaderDelegator.impl not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.getLoadedImageMd5()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='199' end='236' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>At FSImageFormat.java:[lines 199-236]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' signature='Lorg/apache/hadoop/hdfs/server/namenode/FSImageFormat$AbstractLoader;' name='impl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>In FSImageFormat.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.impl</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' signature='()Lorg/apache/hadoop/io/MD5Hash;' name='getLoadedImageMd5' primary='true'><SourceLine endBytecode='51' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='206' end='206' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.getLoadedImageMd5()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='206' end='206' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java' startBytecode='4' primary='true'><Message>At FSImageFormat.java:[line 206]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e509f20bc5a847d3c1e17b3597906c9' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>FSImageFormat$LoaderDelegator.impl not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.getLoadedImageTxId()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='199' end='236' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>At FSImageFormat.java:[lines 199-236]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' signature='Lorg/apache/hadoop/hdfs/server/namenode/FSImageFormat$AbstractLoader;' name='impl' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java'><Message>In FSImageFormat.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.impl</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' signature='()J' name='getLoadedImageTxId' primary='true'><SourceLine endBytecode='51' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='211' end='211' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.getLoadedImageTxId()</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator' start='211' end='211' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' sourcefile='FSImageFormat.java' startBytecode='4' primary='true'><Message>At FSImageFormat.java:[line 211]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b5855226f7a43edc18dd68980d99c64' rank='11' abbrev='RpC' category='CORRECTNESS' priority='3' type='RpC_REPEATED_CONDITIONAL_TEST' instanceOccurrenceMax='0'><ShortMessage>Repeated conditional tests</ShortMessage><LongMessage>Repeated conditional test in org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeFile(FsImageProto$INodeSection$INode)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' start='109' end='438' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java' sourcefile='FSImageFormatPBINode.java'><Message>At FSImageFormatPBINode.java:[lines 109-438]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FsImageProto$INodeSection$INode;)Lorg/apache/hadoop/hdfs/server/namenode/INodeFile;' name='loadINodeFile' primary='true'><SourceLine endBytecode='1154' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' start='329' end='397' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java' sourcefile='FSImageFormatPBINode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeFile(FsImageProto$INodeSection$INode)</Message></Method><SourceLine endBytecode='65' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' start='335' end='335' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java' sourcefile='FSImageFormatPBINode.java' startBytecode='64' primary='true'><Message>At FSImageFormatPBINode.java:[line 335]</Message></SourceLine><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader' start='335' end='335' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java' sourcefile='FSImageFormatPBINode.java' startBytecode='69'><Message>At FSImageFormatPBINode.java:[line 335]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a1c5a7257be7c1e7b94fb1987bd29312' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1' start='219' end='229' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java'><Message>At FSImageFormatProtobuf.java:[lines 219-229]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader' start='161' end='390' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java'><Message>At FSImageFormatProtobuf.java:[lines 161-390]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader' signature='(Ljava/io/RandomAccessFile;Ljava/io/FileInputStream;)V' name='loadInternal' primary='true'><SourceLine endBytecode='1090' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader' start='199' end='306' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(RandomAccessFile, FileInputStream)</Message></Method><SourceLine endBytecode='130' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader' start='219' end='219' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java' startBytecode='130' primary='true'><Message>At FSImageFormatProtobuf.java:[line 219]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b7e3176be7ba6d2194b517aa2b34eea3' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext.getStringTable() may expose internal representation by returning FSImageFormatProtobuf$LoaderContext.stringTable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' start='87' end='96' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java'><Message>At FSImageFormatProtobuf.java:[lines 87-96]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' signature='()[Ljava/lang/String;' name='getStringTable' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' start='92' end='92' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext.getStringTable()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' signature='[Ljava/lang/String;' name='stringTable' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java'><Message>In FSImageFormatProtobuf.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext.stringTable</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext' start='92' end='92' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' sourcefile='FSImageFormatProtobuf.java' startBytecode='4' primary='true'><Message>At FSImageFormatProtobuf.java:[line 92]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3fcd157f56fd90e67b8752f8eace96ca' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_MUTABLE_ARRAY' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable array</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FSImageUtil.MAGIC_HEADER is a mutable array</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSImageUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageUtil' start='35' end='91' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageUtil.java' sourcefile='FSImageUtil.java'><Message>At FSImageUtil.java:[lines 35-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSImageUtil</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FSImageUtil' signature='[B' name='MAGIC_HEADER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSImageUtil' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageUtil.java' sourcefile='FSImageUtil.java'><Message>In FSImageUtil.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSImageUtil.MAGIC_HEADER</Message></Field><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.FSImageUtil' start='37' end='37' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSImageUtil.java' sourcefile='FSImageUtil.java' startBytecode='8' primary='true'><Message>At FSImageUtil.java:[line 37]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e135b367c9eb964ae6c1d27131c2c3dc' rank='20' abbrev='Dm' category='I18N' priority='3' type='DM_CONVERT_CASE' instanceOccurrenceMax='0'><ShortMessage>Consider using Locale parameterized version of invoked method</ShortMessage><LongMessage>Use of non-localized String.toUpperCase() or String.toLowerCase() in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode(HdfsConstants$SafeModeAction)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Lorg/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction;)Z' name='setSafeMode' primary='true'><SourceLine endBytecode='260' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='4480' end='4502' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setSafeMode(HdfsConstants$SafeModeAction)</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='4480' end='4480' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='4' primary='true'><Message>At FSNamesystem.java:[line 4480]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dc6762c6d509543fafd47515be4277a9' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addCachePool(CachePoolInfo, boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Lorg/apache/hadoop/hdfs/protocol/CachePoolInfo;Z)V' name='addCachePool' primary='true'><SourceLine endBytecode='71' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='7008' end='7029' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addCachePool(CachePoolInfo, boolean)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='100' name='poolInfoStr' register='5'><Message>Value loaded from poolInfoStr</Message></LocalVariable><SourceLine endBytecode='103' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='7022' end='7022' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='103' primary='true'><Message>At FSNamesystem.java:[line 7022]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='833c42540218fa16eb335f9aab053938' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(String, String[], boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Ljava/lang/String;[Ljava/lang/String;Z)V' name='concat' primary='true'><SourceLine endBytecode='76' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='2034' end='2056' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(String, String[], boolean)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='110' name='stat' register='5'><Message>Value loaded from stat</Message></LocalVariable><SourceLine endBytecode='111' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='2046' end='2046' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='111' primary='true'><Message>At FSNamesystem.java:[line 2046]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2f838b3e9a7530de8c5c7cf0092d412b' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSnapshot(String, String, boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Ljava/lang/String;Ljava/lang/String;Z)Ljava/lang/String;' name='createSnapshot' primary='true'><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6405' end='6427' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSnapshot(String, String, boolean)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='98' name='snapshotPath' register='5'><Message>Value loaded from snapshotPath</Message></LocalVariable><SourceLine endBytecode='99' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6418' end='6418' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='99' primary='true'><Message>At FSNamesystem.java:[line 6418]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b8c0a73bd68eacd2a036ec200395d1a0' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_LOAD_OF_KNOWN_NULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Load of known null value</ShortMessage><LongMessage>Load of known null value in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Ljava/lang/String;)Lorg/apache/hadoop/hdfs/protocol/EncryptionZone;' name='getEZForPath' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='7275' end='7296' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(String)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='90' name='resultingStat' register='3'><Message>Value loaded from resultingStat</Message></LocalVariable><SourceLine endBytecode='91' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='7290' end='7290' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='91' primary='true'><Message>At FSNamesystem.java:[line 7290]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='22c9bcfaf94d7acaeef25db733a00676' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of blocksToBeDeleted, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteSnapshot(String, String, boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Ljava/lang/String;Ljava/lang/String;Z)V' name='deleteSnapshot' primary='true'><SourceLine endBytecode='456' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6607' end='6635' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteSnapshot(String, String, boolean)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='137' name='blocksToBeDeleted' register='7'><Message>Value loaded from blocksToBeDeleted</Message></LocalVariable><Method isStatic='true' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Lorg/apache/hadoop/hdfs/server/namenode/FSPermissionChecker;Lorg/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager;Ljava/lang/String;Ljava/lang/String;Z)Lorg/apache/hadoop/hdfs/server/namenode/INode$BlocksMapUpdateInfo;' name='deleteSnapshot'><SourceLine endBytecode='439' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' start='242' end='265' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' sourcefile='FSDirSnapshotOp.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.deleteSnapshot(FSDirectory, FSPermissionChecker, SnapshotManager, String, String, boolean) of type org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='139' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6631' end='6631' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='139' primary='true'><Message>Redundant null check at FSNamesystem.java:[line 6631]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='85ed2680af6b08924a230380eec34140' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of snaps, which is known to be non-null in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.listCorruptFileBlocksWithSnapshot(String, List, String[])</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Ljava/lang/String;Ljava/util/List;[Ljava/lang/String;)Ljava/util/List;' name='listCorruptFileBlocksWithSnapshot' primary='true'><SourceLine endBytecode='708' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6253' end='6285' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.listCorruptFileBlocksWithSnapshot(String, List, String[])</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='206' name='snaps' register='9'><Message>Value loaded from snaps</Message></LocalVariable><Method isStatic='true' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSDirectory;Ljava/util/List;Ljava/lang/String;)Ljava/util/Collection;' name='getSnapshotFiles'><SourceLine endBytecode='500' classname='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp' start='200' end='224' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' sourcefile='FSDirSnapshotOp.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.getSnapshotFiles(FSDirectory, List, String) of type java.util.Collection</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='208' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='6278' end='6278' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='208' primary='true'><Message>Redundant null check at FSNamesystem.java:[line 6278]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='167d6a27f8398afdb4f75a28f1a909e1' rank='11' abbrev='RpC' category='CORRECTNESS' priority='3' type='RpC_REPEATED_CONDITIONAL_TEST' instanceOccurrenceMax='0'><ShortMessage>Repeated conditional tests</ShortMessage><LongMessage>Repeated conditional test in org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(HdfsServerConstants$StartupOption)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Lorg/apache/hadoop/hdfs/server/common/HdfsServerConstants$StartupOption;)V' name='loadFSImage' primary='true'><SourceLine endBytecode='623' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='1071' end='1116' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(HdfsServerConstants$StartupOption)</Message></Method><SourceLine endBytecode='200' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='1104' end='1104' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='197' primary='true'><Message>At FSNamesystem.java:[line 1104]</Message></SourceLine><SourceLine endBytecode='207' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='1104' end='1104' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='204'><Message>At FSNamesystem.java:[line 1104]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='301afc43f8bc0bd4f00866e05659d999' rank='19' abbrev='UC' category='STYLE' priority='3' type='UC_USELESS_CONDITION' instanceOccurrenceMax='0'><ShortMessage>Condition has no effect</ShortMessage><LongMessage>Useless condition: it's known that this.haEnabled == true at this point</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='348' end='8039' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 348-8039]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' signature='(Lorg/apache/hadoop/hdfs/server/common/HdfsServerConstants$StartupOption;)V' name='loadFSImage' primary='true'><SourceLine endBytecode='623' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='1071' end='1116' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(HdfsServerConstants$StartupOption)</Message></Method><String value='this.haEnabled == true'><Message>Value this.haEnabled == true</Message></String><SourceLine endBytecode='208' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem' start='1104' end='1104' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='208' primary='true'><Message>At FSNamesystem.java:[line 1104]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bfc4970e913708a21304d88560d04a10' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber' start='4024' end='4107' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 4024-4107]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber' signature='()V' name='run' primary='true'><SourceLine endBytecode='254' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber' start='4080' end='4103' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run()</Message></Method><SourceLine endBytecode='78' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber' start='4096' end='4096' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='78' primary='true'><Message>At FSNamesystem.java:[line 4096]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d425f5d157d65c0efd073a9f0c0e9bc1' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor' start='3946' end='3975' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java'><Message>At FSNamesystem.java:[lines 3946-3975]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor' signature='()V' name='run' primary='true'><SourceLine endBytecode='311' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor' start='3951' end='3971' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run()</Message></Method><SourceLine endBytecode='132' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor' start='3968' end='3968' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' sourcefile='FSNamesystem.java' startBytecode='132' primary='true'><Message>At FSNamesystem.java:[line 3968]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5d2efa3c5854f36ad562fede60bbb618' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock$1' start='91' end='94' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' sourcefile='FSNamesystemLock.java'><Message>At FSNamesystemLock.java:[lines 91-94]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' start='77' end='335' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' sourcefile='FSNamesystemLock.java'><Message>At FSNamesystemLock.java:[lines 77-335]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' signature='(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/metrics2/lib/MutableRatesWithAggregation;Lorg/apache/hadoop/util/Timer;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='364' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' start='119' end='141' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' sourcefile='FSNamesystemLock.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock(Configuration, MutableRatesWithAggregation, Timer)</Message></Method><SourceLine endBytecode='25' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' start='90' end='90' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' sourcefile='FSNamesystemLock.java' startBytecode='25' primary='true'><Message>At FSNamesystemLock.java:[line 90]</Message></SourceLine><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' signature='Ljava/lang/ThreadLocal;' name='readLockHeldTimeStampNanos' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' sourcefile='FSNamesystemLock.java'><Message>In FSNamesystemLock.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.readLockHeldTimeStampNanos</Message></Field></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7778d26d43192e4b20dec293b72217d9' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' start='24113' end='24684' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24113-24684]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' start='24200' end='24200' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 24200]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e61a9231810fcdc9fc662676309e34b0' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' start='24113' end='24684' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24113-24684]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bef6ca3e3a465c29031a091149e545a6' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder' start='24414' end='24675' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24414-24675]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder' start='24440' end='24442' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder' start='24440' end='24440' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 24440]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1ab6611ae4fe1bff74c9943b5626d707' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' start='24720' end='25370' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24720-25370]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' start='24803' end='24803' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 24803]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cba3c45e299e815a71ec04487e604424' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' start='24720' end='25370' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24720-25370]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7fa4adb4410e7cfbe5c000aa325c6603' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder' start='24972' end='25361' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 24972-25361]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder' signature='()Lorg/apache/hadoop/hdfs/server/namenode/FsImageProto$ErasureCodingSection;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder' start='25039' end='25051' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder' start='25040' end='25040' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='14' primary='true'><Message>At FsImageProto.java:[line 25040]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9a143a9bd9ee6b4ee6bf0316af6d8392' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' start='95' end='1730' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 95-1730]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' start='193' end='193' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 193]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6034601ae3f5a99f9e4ecebfac99c892' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' start='95' end='1730' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 95-1730]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='527281b4dddf2154c18cca409ccaa5c7' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' start='253' end='844' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 253-844]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' start='340' end='340' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 340]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6c0d74e4c3768b82f90ab61a599c43c6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' start='253' end='844' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 253-844]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fc7c3cf320c6a089330e5051ed948c0' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder' start='563' end='835' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 563-835]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder' start='589' end='591' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder' start='589' end='589' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 589]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5dd83d6a051aadcbf24ccf7d55bd06c3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' start='12229' end='13086' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12229-13086]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' start='12300' end='12300' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 12300]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='23c14078c5b1764373f7445b9730c8e3' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' start='12229' end='13086' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12229-13086]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dcee9b97b36c9bbf43b9b9d0a49d8935' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder' start='12978' end='13077' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12978-13077]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder' start='13004' end='13006' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder' start='13004' end='13004' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 13004]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='20db0df6651e45424c66638c0c0e8584' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' start='12346' end='12862' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12346-12862]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' start='12428' end='12428' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 12428]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d369bfa4690280c7336001cf39156afb' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' start='12346' end='12862' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12346-12862]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='af9c0a51f793878350cf284fed97ee1a' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder' start='12623' end='12853' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 12623-12853]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder' start='12649' end='12651' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder' start='12649' end='12649' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 12649]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e63cf471380f7875e2315e4a0e0e4d58' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' start='13103' end='14253' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 13103-14253]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' start='13174' end='13174' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 13174]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b73269a5d9f232cc4863c0935abcb39' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' start='13103' end='14253' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 13103-14253]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='72de7c042d368ec8575124462b5c5613' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder' start='14145' end='14244' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14145-14244]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder' start='14171' end='14173' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder' start='14171' end='14171' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 14171]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e94dbcb7316a716adb766dfe3535c7b1' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' start='13263' end='14030' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 13263-14030]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' start='13388' end='13388' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 13388]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6b08f6e3316a586eab71174d74af2d08' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' start='13263' end='14030' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 13263-14030]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0b8478e34914bef7018939eb17c146c' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder' start='13654' end='14021' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 13654-14021]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder' start='13680' end='13682' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder' start='13680' end='13680' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 13680]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e23bd29c111fc3a0fd0823549ef6dee6' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' start='14264' end='15332' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14264-15332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' start='14335' end='14335' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 14335]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3cab879b82785dfc0a505f2756bce804' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' start='14264' end='15332' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14264-15332]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='682e03d5571bdbcab30ddcd5af1a11e4' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder' start='15224' end='15323' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 15224-15323]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder' start='15250' end='15252' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder' start='15250' end='15250' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 15250]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7d0507cf59182ba92162d654857b431c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' start='14428' end='15115' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14428-15115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' start='14520' end='14520' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 14520]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b803569c0a3f27c8fb0da7444b0d3b8c' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' start='14428' end='15115' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14428-15115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fe93c6db66c81671257e38e63a9289ed' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference defines non-transient non-serializable instance field name_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' start='14428' end='15115' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14428-15115]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' signature='Lcom/google/protobuf/ByteString;' name='name_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference.name_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b6886dc7816fb173f48ef93cd53fedb8' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder' start='14768' end='15106' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 14768-15106]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder' start='14794' end='14796' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder' start='14794' end='14794' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 14794]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9e8d77d9592373d36ce0e3b82fd90837' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' start='2815' end='12211' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 2815-12211]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' start='2897' end='2897' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 2897]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='deb5d8e13fb8ad48b52bb586a92ee326' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' start='2815' end='12211' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 2815-12211]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7eaa11f7adc935d1ff373eb07ed76e40' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' start='3611' end='4202' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 3611-4202]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' start='3707' end='3707' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 3707]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9b6b27f21e3c04aa8256ba4b392fafc6' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' start='3611' end='4202' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 3611-4202]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f61a59c73e60d2e595d78ade4ad51ad7' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3911' end='4193' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 3911-4193]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' signature='()Lorg/apache/hadoop/hdfs/server/namenode/FsImageProto$INodeSection$AclFeatureProto;' name='buildPartial' primary='true'><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3973' end='3981' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3974' end='3974' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='14' primary='true'><Message>At FsImageProto.java:[line 3974]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7e99f98a81e6023501b33f1e879d24c' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3911' end='4193' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 3911-4193]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3937' end='3939' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder' start='3937' end='3937' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 3937]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a9a8ffc2eac3418a75acc0e1453222ae' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder' start='11999' end='12202' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 11999-12202]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder' start='12025' end='12027' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder' start='12025' end='12025' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 12025]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e74acb88ff9c3565df9874ec5034a91' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' start='2953' end='3544' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 2953-3544]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' start='3035' end='3035' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 3035]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a18e0ce7c362118d9194264357ec1233' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' start='2953' end='3544' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 2953-3544]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8a38dbd2e9717557b9801b8dddb48536' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder' start='3262' end='3535' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 3262-3535]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder' start='3288' end='3290' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder' start='3288' end='3288' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 3288]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='91184c0c2398fb4ff980dc7483252308' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' start='10630' end='11824' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 10630-11824]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' start='10762' end='10762' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 10762]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c7d839ed1dfcdc5492eee37cbeec6462' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' start='10630' end='11824' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 10630-11824]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='254c648a0dd9e2425e10e4a3622cae98' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode defines non-transient non-serializable instance field name_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' start='10630' end='11824' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 10630-11824]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' signature='Lcom/google/protobuf/ByteString;' name='name_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode.name_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='12a17ee7c54f6ace58bc2332bd08b666' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' start='8711' end='9905' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 8711-9905]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' start='8842' end='8842' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 8842]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5d2e695d662b93dc236d8d86cd898d87' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' start='8711' end='9905' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 8711-9905]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a23387521a3bf3a5b9786187fb11aa1b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' start='5638' end='7414' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 5638-7414]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' start='5806' end='5806' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 5806]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ce1331e41c66d21ff91e5cfb6903e566' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' start='5638' end='7414' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 5638-7414]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='35334790fb9ea80719c77d76e4d27872' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' start='9956' end='10547' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 9956-10547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' start='10048' end='10048' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 10048]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b4467b9f32d99f7d954517c4bb47166d' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' start='9956' end='10547' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 9956-10547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='728ae2fc05ee09bb896cf8e87681e64e' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink defines non-transient non-serializable instance field target_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' start='9956' end='10547' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 9956-10547]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' signature='Lcom/google/protobuf/ByteString;' name='target_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink.target_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='28f236923cfb074abef1627f988a779f' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder' start='10264' end='10538' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 10264-10538]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder' start='10290' end='10292' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder' start='10290' end='10290' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 10290]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='721740ebbba86ede4167fc246e6d0b96' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' start='7445' end='7916' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 7445-7916]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' start='7533' end='7533' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 7533]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b529c19f6b8e8c99259daeeb5269c0b2' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' start='7445' end='7916' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 7445-7916]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='927dd0d56461831451ea4b7afad75dd' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder' start='7709' end='7907' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 7709-7907]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder' start='7735' end='7737' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder' start='7735' end='7735' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 7735]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b5a888b5253fd7df14bf1f9dfee0854' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' start='7952' end='8602' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 7952-8602]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' start='8035' end='8035' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 8035]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f09bd0f0017ea322583d30f2c57c19b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' start='7952' end='8602' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 7952-8602]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9459a14e2e9482dff6bdf0e74bb2a90' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder' start='8204' end='8593' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 8204-8593]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder' signature='()Lorg/apache/hadoop/hdfs/server/namenode/FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder' start='8271' end='8283' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder' start='8272' end='8272' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='14' primary='true'><Message>At FsImageProto.java:[line 8272]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c2dec9b2d00ce2a19546fa5fc838cfc3' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' start='4259' end='4794' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4259-4794]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' start='4341' end='4341' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 4341]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='527437193e215095858da35a99048b47' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' start='4259' end='4794' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4259-4794]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='63e8ef119b517554e1ea163a58f4c993' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto defines non-transient non-serializable instance field value_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' start='4259' end='4794' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4259-4794]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' signature='Lcom/google/protobuf/ByteString;' name='value_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto.value_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='22535af634c3eb5a87a5497b7da12f97' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder' start='4539' end='4785' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4539-4785]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder' start='4565' end='4567' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder' start='4565' end='4565' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 4565]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ee629ebdc2e23e7478b4ddbb5b5a46e0' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' start='4830' end='5480' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4830-5480]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' start='4913' end='4913' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 4913]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='75144a0b08fd88a79f3ca80b5732dbac' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' start='4830' end='5480' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 4830-5480]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='caf63ac5b50c6d5f2107a0d92a432ccc' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to from_bitField0_ in org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder.buildPartial()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder' start='5082' end='5471' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 5082-5471]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder' signature='()Lorg/apache/hadoop/hdfs/server/namenode/FsImageProto$INodeSection$XAttrFeatureProto;' name='buildPartial' primary='true'><SourceLine endBytecode='44' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder' start='5149' end='5161' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder.buildPartial()</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='15' name='from_bitField0_' register='2'><Message>Local variable named from_bitField0_</Message></LocalVariable><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder' start='5150' end='5150' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='14' primary='true'><Message>At FsImageProto.java:[line 5150]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.BASE_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.CACHING_VALUE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='from_bitField0_'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.NO_LOADS' value='true'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1ef5ee0d65bcafc149faa31399b5b6ad' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1842' end='2767' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 1842-2767]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1954' end='1954' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 1954]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='44895c203de832a228a9ba080deb89ca' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' start='1842' end='2767' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 1842-2767]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='147eb360520486aedc71550b03fa6382' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder' start='2287' end='2758' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 2287-2758]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder' start='2313' end='2315' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder' start='2313' end='2313' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 2313]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a20f4c3ecf0a50265feb6141b313f88f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' start='21614' end='24062' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21614-24062]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' start='21706' end='21706' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 21706]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1fede0f42d07f679027dc2b57f7ce3c4' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' start='21614' end='24062' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21614-24062]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='48de385d00a7a2b7b17c1f3722428482' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder' start='23762' end='24053' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 23762-24053]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder' start='23788' end='23790' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder' start='23788' end='23788' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 23788]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1ea7e68c3736ae16996899eb9ceca49b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' start='21757' end='22277' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21757-22277]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' start='21844' end='21844' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 21844]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='53004119197b0e50e3767cfc26ba314e' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' start='21757' end='22277' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21757-22277]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d25f6066dd445f1f025d2ebe842faafa' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey defines non-transient non-serializable instance field key_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' start='21757' end='22277' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21757-22277]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' signature='Lcom/google/protobuf/ByteString;' name='key_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey.key_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bbb4ec164270c44771d0eb65bb70b82b' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder' start='22036' end='22268' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 22036-22268]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder' start='22062' end='22064' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder' start='22062' end='22062' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 22062]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7eaac602ebc9ea29543a0f2a39264211' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' start='22393' end='23546' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 22393-23546]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' start='22510' end='22510' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 22510]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d8f379391bee9d4f404dae965494202b' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' start='22393' end='23546' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 22393-23546]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bc84ced6c588289596592ec2e3804ec2' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder' start='22927' end='23537' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 22927-23537]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder' start='22953' end='22955' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder' start='22953' end='22953' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 22953]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='efc1d9edcdcf61193f71070f542733b8' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' start='16697' end='20561' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16697-20561]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' start='16768' end='16768' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 16768]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9bf2f6919766ffafba22047b16ecad20' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' start='16697' end='20561' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16697-20561]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c777e1fddf37d193c5ebf7fb2066fea2' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder' start='20453' end='20552' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20453-20552]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder' start='20479' end='20481' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder' start='20479' end='20479' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 20479]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a56d820f52a69db04865bfb56e7e3cea' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' start='16799' end='17177' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16799-17177]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' start='16876' end='16876' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 16876]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='16986eead7db4581f4c5941610b33c05' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' start='16799' end='17177' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16799-17177]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f3b4c830f3b6dd7608eb253485ef7be9' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry defines non-transient non-serializable instance field name_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' start='16799' end='17177' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16799-17177]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' signature='Lcom/google/protobuf/ByteString;' name='name_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry.name_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9cc93abdd81aa543c7881015775b60d6' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder' start='17020' end='17168' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 17020-17168]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder' start='17046' end='17048' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder' start='17046' end='17046' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 17046]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3887a9da53c1083fd58b9deed380c1e9' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' start='19722' end='20338' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 19722-20338]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' start='19815' end='19815' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 19815]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f5e49d72a35d425c85bb6c9f910877ba' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' start='19722' end='20338' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 19722-20338]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='580a751ec0f12554d7d9233722cfe33e' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder' start='20093' end='20329' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20093-20329]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder' start='20119' end='20121' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder' start='20119' end='20119' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 20119]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='405be0e757594cdb8c4114b3d909faa7' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' start='17304' end='18542' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 17304-18542]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' start='17462' end='17462' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 17462]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9f9138728f056b484eee56479afc9388' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' start='17304' end='18542' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 17304-18542]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='14f289fe7effdc90de25a6c835d3eab7' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff defines non-transient non-serializable instance field name_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' start='17304' end='18542' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 17304-18542]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' signature='Lcom/google/protobuf/ByteString;' name='name_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff.name_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b34f5e3fbffb0066bc2f5c4b1a19025' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' start='18622' end='19681' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 18622-19681]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' start='18733' end='18733' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 18733]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5f4d730994d9bf4d4af59071ea338db4' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' start='18622' end='19681' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 18622-19681]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c421afb831478c1d7ce5d08d43420781' rank='19' abbrev='Se' category='BAD_PRACTICE' priority='3' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff defines non-transient non-serializable instance field name_</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' start='18622' end='19681' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 18622-19681]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' signature='Lcom/google/protobuf/ByteString;' name='name_' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff.name_</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/ByteString;'><SourceLine classname='com.google.protobuf.ByteString' start='60' end='967' sourcepath='com/google/protobuf/ByteString.java' sourcefile='ByteString.java'><Message>At ByteString.java:[lines 60-967]</Message></SourceLine><Message>Actual type com.google.protobuf.ByteString</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='81632c294c5207f35af54de7d65242d2' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' start='15391' end='16680' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 15391-16680]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' start='15497' end='15497' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 15497]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5714afd4de6043ae6b8dc079937de4e9' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' start='15391' end='16680' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 15391-16680]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f6fb42eb6235359df8ac0ebd8acc2311' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder' start='16385' end='16671' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 16385-16671]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder' start='16411' end='16413' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder' start='16411' end='16411' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 16411]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0f5d0b4becda20511a48afdc14d772f' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' start='15554' end='16167' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 15554-16167]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' start='15644' end='15644' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 15644]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8b38a9396494074ce1f1d2cfca386431' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' start='15554' end='16167' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 15554-16167]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='46205eed996d847dd3fbca7367a3dd66' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' start='20596' end='21553' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20596-21553]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' start='20673' end='20673' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 20673]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4778e207800bcd40568613677722def' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' start='20596' end='21553' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20596-21553]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1f4ddbb70a5b09dc8326176a7000951e' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder' start='21383' end='21544' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 21383-21544]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder' start='21409' end='21411' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder' start='21409' end='21409' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 21409]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='747adc3ae549ca43dbff3606d4965448' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' start='20719' end='21235' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20719-21235]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' start='20801' end='20801' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='7' primary='true'><Message>At FsImageProto.java:[line 20801]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5e09016a20a1fce3a80a4a908743c5c5' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' start='20719' end='21235' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20719-21235]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>In FsImageProto.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e9bab6eb81c431b0cd99b504c9210bd2' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder' start='20996' end='21226' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java'><Message>At FsImageProto.java:[lines 20996-21226]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder' start='21022' end='21024' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder' start='21022' end='21022' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' sourcefile='FsImageProto.java' startBytecode='3' primary='true'><Message>At FsImageProto.java:[line 21022]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7bb58fa1b02f55ddb367363093cd1e16' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.FsckServlet$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet$1' start='58' end='71' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsckServlet.java' sourcefile='FsckServlet.java'><Message>At FsckServlet.java:[lines 58-71]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.FsckServlet$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet' start='40' end='77' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsckServlet.java' sourcefile='FsckServlet.java'><Message>At FsckServlet.java:[lines 40-77]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.FsckServlet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet' signature='(Ljavax/servlet/http/HttpServletRequest;Ljavax/servlet/http/HttpServletResponse;)V' name='doGet' primary='true'><SourceLine endBytecode='338' classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet' start='49' end='77' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsckServlet.java' sourcefile='FsckServlet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(HttpServletRequest, HttpServletResponse)</Message></Method><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.namenode.FsckServlet' start='58' end='58' sourcepath='org/apache/hadoop/hdfs/server/namenode/FsckServlet.java' sourcefile='FsckServlet.java' startBytecode='64' primary='true'><Message>At FsckServlet.java:[line 58]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bc38e17a2c8ba47705ac83a3bdb4c512' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy.getLocalNameBytes() may expose internal representation by returning INodeAttributes$SnapshotCopy.name</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' start='78' end='150' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java' sourcefile='INodeAttributes.java'><Message>At INodeAttributes.java:[lines 78-150]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' signature='()[B' name='getLocalNameBytes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' start='105' end='105' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java' sourcefile='INodeAttributes.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy.getLocalNameBytes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' signature='[B' name='name' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java' sourcefile='INodeAttributes.java'><Message>In INodeAttributes.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy.name</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy' start='105' end='105' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java' sourcefile='INodeAttributes.java' startBytecode='4' primary='true'><Message>At INodeAttributes.java:[line 105]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e52226d995c7922a3b33627c8c1051d5' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.INodeMap$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.INodeMap$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeMap$1' start='92' end='128' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeMap.java' sourcefile='INodeMap.java'><Message>At INodeMap.java:[lines 92-128]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.INodeMap$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeMap' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeMap' start='39' end='140' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeMap.java' sourcefile='INodeMap.java'><Message>At INodeMap.java:[lines 39-140]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeMap</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeMap' signature='(J)Lorg/apache/hadoop/hdfs/server/namenode/INode;' name='get' primary='true'><SourceLine endBytecode='111' classname='org.apache.hadoop.hdfs.server.namenode.INodeMap' start='91' end='132' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeMap.java' sourcefile='INodeMap.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeMap.get(long)</Message></Method><SourceLine endBytecode='28' classname='org.apache.hadoop.hdfs.server.namenode.INodeMap' start='91' end='91' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeMap.java' sourcefile='INodeMap.java' startBytecode='28' primary='true'><Message>At INodeMap.java:[line 91]</Message></SourceLine><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='32' name='inode' register='3'><Message>Local variable named inode</Message></LocalVariable></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='14576426858e44f349c989c4c3b536c1' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName.getLocalNameBytes() may expose internal representation by returning INodeReference$WithName.name</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='466' end='617' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java'><Message>At INodeReference.java:[lines 466-617]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' signature='()[B' name='getLocalNameBytes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='488' end='488' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName.getLocalNameBytes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' signature='[B' name='name' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java'><Message>In INodeReference.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName.name</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='488' end='488' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java' startBytecode='4' primary='true'><Message>At INodeReference.java:[line 488]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='704a3235014c65a55108d94026b7a7a6' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName(INodeDirectory, INodeReference$WithCount, byte[], int) may expose internal representation by storing an externally mutable object into INodeReference$WithName.name</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='466' end='617' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java'><Message>At INodeReference.java:[lines 466-617]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' signature='(Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;Lorg/apache/hadoop/hdfs/server/namenode/INodeReference$WithCount;[BI)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='120' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='480' end='484' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName(INodeDirectory, INodeReference$WithCount, byte[], int)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' signature='[B' name='name' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java'><Message>In INodeReference.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName.name</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='8' name='name' register='3'><Message>Local variable named name</Message></LocalVariable><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName' start='481' end='481' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' sourcefile='INodeReference.java' startBytecode='8' primary='true'><Message>At INodeReference.java:[line 481]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b1173905a0c4e48dd12f992a7027fce0' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeSymlink.getSymlink() may expose internal representation by returning INodeSymlink.symlink</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' start='37' end='143' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java' sourcefile='INodeSymlink.java'><Message>At INodeSymlink.java:[lines 37-143]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeSymlink</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' signature='()[B' name='getSymlink' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' start='71' end='71' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java' sourcefile='INodeSymlink.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeSymlink.getSymlink()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' signature='[B' name='symlink' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java' sourcefile='INodeSymlink.java'><Message>In INodeSymlink.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeSymlink.symlink</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodeSymlink' start='71' end='71' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java' sourcefile='INodeSymlink.java' startBytecode='4' primary='true'><Message>At INodeSymlink.java:[line 71]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9df5f1db12da4da4138cd51560af6e97' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.getFeatures() may expose internal representation by returning INodeWithAdditionalFields.features</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='85' end='377' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>At INodeWithAdditionalFields.java:[lines 85-377]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='()[Lorg/apache/hadoop/hdfs/server/namenode/INode$Feature;' name='getFeatures' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='377' end='377' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.getFeatures()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='[Lorg/apache/hadoop/hdfs/server/namenode/INode$Feature;' name='features' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>In INodeWithAdditionalFields.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.features</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='377' end='377' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='4' primary='true'><Message>At INodeWithAdditionalFields.java:[line 377]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4da69433f717ca2e60c6b2e53d6690a5' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.getLocalNameBytes() may expose internal representation by returning INodeWithAdditionalFields.name</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='85' end='377' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>At INodeWithAdditionalFields.java:[lines 85-377]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='()[B' name='getLocalNameBytes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='145' end='145' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.getLocalNameBytes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='[B' name='name' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>In INodeWithAdditionalFields.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.name</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='145' end='145' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='4' primary='true'><Message>At INodeWithAdditionalFields.java:[line 145]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='98183ce219a164ec750a601634b7c7c7' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.setLocalName(byte[]) may expose internal representation by storing an externally mutable object into INodeWithAdditionalFields.name</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='85' end='377' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>At INodeWithAdditionalFields.java:[lines 85-377]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='([B)V' name='setLocalName' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='150' end='151' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.setLocalName(byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' signature='[B' name='name' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java'><Message>In INodeWithAdditionalFields.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields.name</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='2' name='name' register='1'><Message>Local variable named name</Message></LocalVariable><SourceLine endBytecode='2' classname='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields' start='150' end='150' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' sourcefile='INodeWithAdditionalFields.java' startBytecode='2' primary='true'><Message>At INodeWithAdditionalFields.java:[line 150]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fc80cb9abbfdcab50122e1f1998792a2' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.INodesInPath.getPathComponents() may expose internal representation by returning INodesInPath.path</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' start='39' end='527' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodesInPath.java' sourcefile='INodesInPath.java'><Message>At INodesInPath.java:[lines 39-527]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.INodesInPath</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' signature='()[[B' name='getPathComponents' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' start='349' end='349' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodesInPath.java' sourcefile='INodesInPath.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.INodesInPath.getPathComponents()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' signature='[[B' name='path' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodesInPath.java' sourcefile='INodesInPath.java'><Message>In INodesInPath.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.INodesInPath.path</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.INodesInPath' start='349' end='349' sourcepath='org/apache/hadoop/hdfs/server/namenode/INodesInPath.java' sourcefile='INodesInPath.java' startBytecode='4' primary='true'><Message>At INodesInPath.java:[line 349]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='299329094dfb49f5f9eb658b33f18d40' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.ImageServlet defines non-transient non-serializable instance field currentlyDownloadingCheckpoints</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' start='69' end='576' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>At ImageServlet.java:[lines 69-576]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ImageServlet</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' signature='Ljava/util/SortedSet;' name='currentlyDownloadingCheckpoints' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>In ImageServlet.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ImageServlet.currentlyDownloadingCheckpoints</Message></Field><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/ImageServlet$ImageUploadRequest;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet$ImageUploadRequest' start='619' end='654' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>At ImageServlet.java:[lines 619-654]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.ImageServlet$ImageUploadRequest</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>In ImageServlet.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8dbc17a03da98f7d95029bcedcb4275f' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.ImageServlet$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet$1' start='105' end='169' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>At ImageServlet.java:[lines 105-169]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.ImageServlet$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' start='69' end='576' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java'><Message>At ImageServlet.java:[lines 69-576]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ImageServlet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' signature='(Ljavax/servlet/http/HttpServletRequest;Ljavax/servlet/http/HttpServletResponse;)V' name='doGet' primary='true'><SourceLine endBytecode='399' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' start='95' end='179' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ImageServlet.doGet(HttpServletRequest, HttpServletResponse)</Message></Method><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.namenode.ImageServlet' start='105' end='105' sourcepath='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' sourcefile='ImageServlet.java' startBytecode='73' primary='true'><Message>At ImageServlet.java:[line 105]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0085077c370136fd1b325c725a9cca4' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$AddOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp' start='797' end='814' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 797-814]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='92' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='93' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='46' end='46' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='93' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 46]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fdb30f32a546f7b06623a6409637edf0' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AppendOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$AppendOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AppendOp' start='853' end='934' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 853-934]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AppendOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='284' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='285' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='69' end='69' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='285' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 69]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8528e69c952d3166bb0e988a972bac2e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$CloseOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp' start='825' end='842' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 825-842]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='241' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='242' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='65' end='65' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='242' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 65]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='34a5ca924bb107ef5ccdfcb9d874cfd7' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ConcatDeleteOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$ConcatDeleteOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ConcatDeleteOp' start='1228' end='1375' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1228-1375]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ConcatDeleteOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='389' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='390' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='80' end='80' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='390' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 80]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='203a983cab0b59a84977ef2ada3d9c99' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$DeleteOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteOp' start='1498' end='1585' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1498-1585]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='664' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='665' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='110' end='110' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='665' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 110]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2babf1131b7da3c638d19fc0bda404ea' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$MkdirOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$MkdirOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$MkdirOp' start='1599' end='1758' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1599-1758]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$MkdirOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='715' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='716' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='117' end='117' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='716' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 117]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='70256f4af0e392b5d728cf94817b2672' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveXAttrOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$RemoveXAttrOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveXAttrOp' start='4177' end='4222' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 4177-4222]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveXAttrOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='1090' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='1091' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='157' end='157' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='1091' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 157]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8388d56e9248a0d53777b85dd7391372' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOldOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$RenameOldOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOldOp' start='1386' end='1488' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1386-1488]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOldOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='546' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='547' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='94' end='94' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='547' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 94]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='53433481e2175aa1fe47078e38d1d203' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$RenameOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOp' start='2655' end='2808' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 2655-2808]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='605' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='606' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='102' end='102' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='606' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 102]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2def4293c52456ceb6384400b1115471' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetAclOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SetAclOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetAclOp' start='4279' end='4328' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 4279-4328]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetAclOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='1212' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='1213' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='173' end='173' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='1213' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 173]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='db86c7e4ab69c333e3a6bfec098a65f8' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetOwnerOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SetOwnerOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetOwnerOp' start='2020' end='2099' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 2020-2099]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetOwnerOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='862' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='863' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='863' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 133]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='85296c8136c910b7dc2cf0a353032cc8' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetPermissionsOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SetPermissionsOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetPermissionsOp' start='1947' end='2010' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1947-2010]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetPermissionsOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='805' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='806' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='126' end='126' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='806' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 126]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a126464133bba3e24293f07f423ea0d3' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetReplicationOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SetReplicationOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetReplicationOp' start='1149' end='1216' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 1149-1216]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetReplicationOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='332' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='333' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='73' end='73' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='333' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 73]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9bb30dbbb86d60bbe58ec5c9233c8e6e' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetXAttrOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SetXAttrOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetXAttrOp' start='4230' end='4275' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 4230-4275]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetXAttrOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='1151' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='1152' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='165' end='165' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='1152' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 165]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='37f74b712b489b758400f81f113330f5' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SymlinkOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$SymlinkOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SymlinkOp' start='2496' end='2643' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 2496-2643]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SymlinkOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='992' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='993' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='147' end='147' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='993' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 147]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bf8319e8aacfd417339507d6a98fff2' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$TimesOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp' start='2387' end='2482' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 2387-2482]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='927' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='928' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='140' end='140' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='928' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 140]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d455a71044c500c3fbbfbf6cb612e65a' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.server.namenode.FSEditLogOp to org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TruncateOp in org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='33' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java'><Message>At InotifyFSEditLogOpTranslator.java:[lines 33-184]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;)Lorg/apache/hadoop/hdfs/inotify/EventBatch;' name='translate' primary='true'><SourceLine endBytecode='561' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='44' end='184' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator.translate(FSEditLogOp)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp' start='153' end='5504' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 153-5504]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.server.namenode.FSEditLogOp</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/server/namenode/FSEditLogOp$TruncateOp;'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TruncateOp' start='2811' end='2938' sourcepath='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' sourcefile='FSEditLogOp.java'><Message>At FSEditLogOp.java:[lines 2811-2938]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TruncateOp</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='1269' name='op' register='0'><Message>Value loaded from op</Message></LocalVariable><SourceLine endBytecode='1270' classname='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator' start='180' end='180' sourcepath='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' sourcefile='InotifyFSEditLogOpTranslator.java' startBytecode='1270' primary='true'><Message>At InotifyFSEditLogOpTranslator.java:[line 180]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='27844c0720f5191a930120cc0c29afa5' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$1' start='207' end='211' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 207-211]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='(JI)Lorg/apache/hadoop/hdfs/server/namenode/EditLogOutputStream;' name='startLogSegment' primary='true'><SourceLine endBytecode='107' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='207' end='213' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.startLogSegment(long, int)</Message></Method><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='207' end='207' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='8' primary='true'><Message>At JournalSet.java:[line 207]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bf0509cc3cda69be67444a27e53163f6' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$2'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$2' start='219' end='226' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 219-226]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='(JJ)V' name='finalizeLogSegment' primary='true'><SourceLine endBytecode='108' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='219' end='228' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.finalizeLogSegment(long, long)</Message></Method><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='219' end='219' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='8' primary='true'><Message>At JournalSet.java:[line 219]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='352c505721746d3d817be12b5dc6e574' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$3 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$3'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$3' start='232' end='236' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 232-236]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$3</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='()V' name='close' primary='true'><SourceLine endBytecode='69' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='232' end='239' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.close()</Message></Method><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='232' end='232' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='6' primary='true'><Message>At JournalSet.java:[line 232]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dfa93bfa1c5a384ddf9d1eae82ef20df' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$4 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$4'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$4' start='555' end='559' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 555-559]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$4</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='(I)V' name='setOutputBufferCapacity' primary='true'><SourceLine endBytecode='128' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='555' end='564' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.setOutputBufferCapacity(int)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='555' end='555' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='7' primary='true'><Message>At JournalSet.java:[line 555]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='40d83388cd870869d5b0b22c9b8c4ad1' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$5 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$5'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$5' start='603' end='607' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 603-607]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$5</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='(J)V' name='purgeLogsOlderThan' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='603' end='609' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.purgeLogsOlderThan(long)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='603' end='603' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='7' primary='true'><Message>At JournalSet.java:[line 603]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='255b1c2e297a615aa327afb1c303f9ef' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.JournalSet$6 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$6'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet$6' start='613' end='617' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 613-617]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.JournalSet$6</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='55' end='740' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java'><Message>At JournalSet.java:[lines 55-740]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.JournalSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' signature='()V' name='recoverUnfinalizedSegments' primary='true'><SourceLine endBytecode='60' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='613' end='619' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.JournalSet.recoverUnfinalizedSegments()</Message></Method><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.namenode.JournalSet' start='613' end='613' sourcepath='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' sourcefile='JournalSet.java' startBytecode='6' primary='true'><Message>At JournalSet.java:[line 613]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a9c8bf5a2b845f201d7523bcbc4c3ab6' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.LeaseManager$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$1' start='96' end='102' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java'><Message>At LeaseManager.java:[lines 96-102]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.LeaseManager$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager' start='79' end='676' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java'><Message>At LeaseManager.java:[lines 79-676]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.LeaseManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSNamesystem;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='152' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager' start='112' end='115' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.LeaseManager(FSNamesystem)</Message></Method><SourceLine endBytecode='39' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager' start='95' end='95' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java' startBytecode='39' primary='true'><Message>At LeaseManager.java:[line 95]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6519d91a4d1f56bd9fac7d1f3b241c49' rank='19' abbrev='HE' category='BAD_PRACTICE' priority='3' type='HE_HASHCODE_USE_OBJECT_EQUALS' instanceOccurrenceMax='0'><ShortMessage>Class defines hashCode() and uses Object.equals()</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease defines hashCode and uses Object.equals()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease' start='446' end='499' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java'><Message>At LeaseManager.java:[lines 446-499]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease' signature='()I' name='hashCode' primary='true'><SourceLine endBytecode='49' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease' start='486' end='486' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease.hashCode()</Message></Method><SourceLine synthetic='true' endBytecode='49' classname='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease' start='486' end='486' sourcepath='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' sourcefile='LeaseManager.java' startBytecode='0'><Message>At LeaseManager.java:[line 486]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='14976cd82008c896d4cb832016769e94' cweid='382' rank='16' abbrev='Dm' category='BAD_PRACTICE' priority='2' type='DM_EXIT' instanceOccurrenceMax='0'><ShortMessage>Method invokes System.exit(...)</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit() invokes System.exit(...), which shuts down the entire virtual machine</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext' start='32' end='129' sourcepath='org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java' sourcefile='MetaRecoveryContext.java'><Message>At MetaRecoveryContext.java:[lines 32-129]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext' signature='()V' name='quit' primary='true'><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext' start='119' end='121' sourcepath='org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java' sourcefile='MetaRecoveryContext.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.quit()</Message></Method><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext' start='120' end='120' sourcepath='org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java' sourcefile='MetaRecoveryContext.java' startBytecode='11' primary='true'><Message>At MetaRecoveryContext.java:[line 120]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='12f00f3075a576ae14c38062fdc2a02a' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$1' start='138' end='144' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java'><Message>At NNStorageRetentionManager.java:[lines 138-144]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='54' end='294' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java'><Message>At NNStorageRetentionManager.java:[lines 54-294]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' signature='(Lorg/apache/hadoop/hdfs/server/namenode/NNStorage$NameNodeFile;)V' name='purgeOldStorage' primary='true'><SourceLine endBytecode='468' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='112' end='170' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.purgeOldStorage(NNStorage$NameNodeFile)</Message></Method><SourceLine endBytecode='89' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='138' end='138' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java' startBytecode='89' primary='true'><Message>At NNStorageRetentionManager.java:[line 138]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='430f89383c5d43947a8ce19be0d1072d' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$2'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$2' start='250' end='253' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java'><Message>At NNStorageRetentionManager.java:[lines 250-253]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='54' end='294' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java'><Message>At NNStorageRetentionManager.java:[lines 54-294]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' signature='(Ljava/lang/String;J)V' name='purgeOldLegacyOIVImages' primary='true'><SourceLine endBytecode='750' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='245' end='294' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.purgeOldLegacyOIVImages(String, long)</Message></Method><SourceLine endBytecode='27' classname='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager' start='250' end='250' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' sourcefile='NNStorageRetentionManager.java' startBytecode='27' primary='true'><Message>At NNStorageRetentionManager.java:[line 250]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2ace2b5ee348f326693dda7edc966805' cweid='476' rank='13' abbrev='NP' category='STYLE' priority='2' type='NP_NULL_ON_SOME_PATH_FROM_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Possible null pointer dereference due to return value of called method</ShortMessage><LongMessage>Possible null pointer dereference in org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1.visitFile(Path, BasicFileAttributes) due to return value of called method</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' start='127' end='142' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java' sourcefile='NNUpgradeUtil.java'><Message>At NNUpgradeUtil.java:[lines 127-142]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' signature='(Ljava/nio/file/Path;Ljava/nio/file/attribute/BasicFileAttributes;)Ljava/nio/file/FileVisitResult;' name='visitFile' primary='true'><SourceLine endBytecode='177' classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' start='133' end='142' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java' sourcefile='NNUpgradeUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1.visitFile(Path, BasicFileAttributes)</Message></Method><LocalVariable role='LOCAL_VARIABLE_UNKNOWN' pc='6' name='?' register='-1'><Message>Local variable stored in JVM register ?</Message></LocalVariable><SourceLine role='SOURCE_LINE_DEREF' endBytecode='6' classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java' sourcefile='NNUpgradeUtil.java' startBytecode='6' primary='true'><Message>Dereferenced at NNUpgradeUtil.java:[line 133]</Message></SourceLine><SourceLine role='SOURCE_LINE_KNOWN_NULL' endBytecode='1' classname='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java' sourcefile='NNUpgradeUtil.java' startBytecode='1'><Message>Known null at NNUpgradeUtil.java:[line 133]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6469718a5f694bae77c3acd81e3e93b2' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.NameNode.NAMENODE_SPECIFIC_KEYS should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='207' end='2128' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>At NameNode.java:[lines 207-2128]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNode</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' signature='[Ljava/lang/String;' name='NAMENODE_SPECIFIC_KEYS' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>In NameNode.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNode.NAMENODE_SPECIFIC_KEYS</Message></Field><SourceLine endBytecode='175' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='250' end='250' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java' startBytecode='175' primary='true'><Message>At NameNode.java:[line 250]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dda82dba0a52e69e14f6dd34963b24a7' cweid='218' rank='18' abbrev='MS' category='MALICIOUS_CODE' priority='2' type='MS_PKGPROTECT' instanceOccurrenceMax='0'><ShortMessage>Field should be package protected</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.NameNode.NAMESERVICE_SPECIFIC_KEYS should be package protected</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='207' end='2128' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>At NameNode.java:[lines 207-2128]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNode</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' signature='[Ljava/lang/String;' name='NAMESERVICE_SPECIFIC_KEYS' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>In NameNode.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNode.NAMESERVICE_SPECIFIC_KEYS</Message></Field><SourceLine endBytecode='188' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='284' end='284' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java' startBytecode='188' primary='true'><Message>At NameNode.java:[line 284]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='24788d20deab6aa015b046df54ab5262' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.NameNode$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.NameNode$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode$1' start='849' end='852' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>At NameNode.java:[lines 849-852]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.NameNode$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNode' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='207' end='2128' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java'><Message>At NameNode.java:[lines 207-2128]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNode</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' signature='(Lorg/apache/hadoop/conf/Configuration;)V' name='startTrashEmptier' primary='true'><SourceLine endBytecode='214' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='836' end='858' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNode.startTrashEmptier(Configuration)</Message></Method><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.server.namenode.NameNode' start='848' end='848' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNode.java' sourcefile='NameNode.java' startBytecode='37' primary='true'><Message>At NameNode.java:[line 848]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ffc6b902d09dc7648d3ae8cba6b50ae3' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>NameNodeHttpServer.httpServer not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setFSImage(FSImage)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='73' end='338' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>At NameNodeHttpServer.java:[lines 73-338]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='Lorg/apache/hadoop/http/HttpServer2;' name='httpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>In NameNodeHttpServer.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.httpServer</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSImage;)V' name='setFSImage' primary='true'><SourceLine endBytecode='66' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='268' end='269' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setFSImage(FSImage)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='268' end='268' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='7' primary='true'><Message>At NameNodeHttpServer.java:[line 268]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e67daacd28408906227d7bbc7c95a76b' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>NameNodeHttpServer.httpServer not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setNameNodeAddress(InetSocketAddress)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='73' end='338' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>At NameNodeHttpServer.java:[lines 73-338]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='Lorg/apache/hadoop/http/HttpServer2;' name='httpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>In NameNodeHttpServer.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.httpServer</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='(Ljava/net/InetSocketAddress;)V' name='setNameNodeAddress' primary='true'><SourceLine endBytecode='77' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='277' end='279' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setNameNodeAddress(InetSocketAddress)</Message></Method><SourceLine endBytecode='10' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='277' end='277' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='10' primary='true'><Message>At NameNodeHttpServer.java:[line 277]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d0f3b87c663f593483afacf090e54dcb' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>NameNodeHttpServer.httpServer not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setStartupProgress(StartupProgress)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='73' end='338' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>At NameNodeHttpServer.java:[lines 73-338]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='Lorg/apache/hadoop/http/HttpServer2;' name='httpServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java'><Message>In NameNodeHttpServer.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.httpServer</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' signature='(Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress;)V' name='setStartupProgress' primary='true'><SourceLine endBytecode='66' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='287' end='288' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.setStartupProgress(StartupProgress)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer' start='287' end='287' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' sourcefile='NameNodeHttpServer.java' startBytecode='7' primary='true'><Message>At NameNodeHttpServer.java:[line 287]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='88d7cab81095db62217b6b63358fe2b' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_MUTABLE_COLLECTION' instanceOccurrenceMax='0'><ShortMessage>Field is a mutable collection</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion.FEATURES is a mutable collection</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion' start='31' end='51' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.java' sourcefile='NameNodeLayoutVersion.java'><Message>At NameNodeLayoutVersion.java:[lines 31-51]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion' signature='Ljava/util/Map;' name='FEATURES' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.java' sourcefile='NameNodeLayoutVersion.java'><Message>In NameNodeLayoutVersion.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion.FEATURES</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion' start='33' end='33' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.java' sourcefile='NameNodeLayoutVersion.java' startBytecode='7' primary='true'><Message>At NameNodeLayoutVersion.java:[line 33]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9cc3dbf375a126330f599bf34db052de' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' start='121' end='127' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java'><Message>At NameNodeResourceChecker.java:[lines 121-127]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker' start='49' end='210' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java'><Message>At NameNodeResourceChecker.java:[lines 49-210]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker' signature='(Lorg/apache/hadoop/conf/Configuration;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='398' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker' start='109' end='147' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker(Configuration)</Message></Method><SourceLine endBytecode='52' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker' start='120' end='120' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java' startBytecode='52' primary='true'><Message>At NameNodeResourceChecker.java:[line 120]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='edd7b01e8932b6e47d92b496ad4e3314' cweid='476' rank='19' abbrev='NP' category='STYLE' priority='3' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE' instanceOccurrenceMax='0'><ShortMessage>Parameter must be non-null but is marked as nullable</ShortMessage><LongMessage>input must be non-null but is marked as nullable</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' start='121' end='127' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java'><Message>At NameNodeResourceChecker.java:[lines 121-127]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' signature='(Ljava/net/URI;)Z' name='apply' primary='true'><SourceLine endBytecode='84' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' start='124' end='127' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1.apply(URI)</Message></Method><LocalVariable role='LOCAL_VARIABLE_PARAMETER_NAMED' pc='0' name='input' register='1'><Message>Parameter input</Message></LocalVariable><SourceLine synthetic='true' endBytecode='84' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1' start='124' end='127' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' sourcefile='NameNodeResourceChecker.java' startBytecode='0'><Message>At NameNodeResourceChecker.java:[lines 124-127]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='198d3deacfd307c9665c2be40c9bacc6' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1' start='1530' end='1533' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java' sourcefile='NameNodeRpcServer.java'><Message>At NameNodeRpcServer.java:[lines 1530-1533]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer' start='233' end='2513' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java' sourcefile='NameNodeRpcServer.java'><Message>At NameNodeRpcServer.java:[lines 233-2513]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeRegistration;Ljava/lang/String;[Lorg/apache/hadoop/hdfs/server/protocol/StorageBlockReport;Lorg/apache/hadoop/hdfs/server/protocol/BlockReportContext;)Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;' name='blockReport' primary='true'><SourceLine endBytecode='421' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer' start='1514' end='1550' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java' sourcefile='NameNodeRpcServer.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(DatanodeRegistration, String, StorageBlockReport[], BlockReportContext)</Message></Method><SourceLine endBytecode='111' classname='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer' start='1530' end='1530' sourcepath='org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java' sourcefile='NameNodeRpcServer.java' startBytecode='111' primary='true'><Message>At NameNodeRpcServer.java:[line 1530]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='456bc5f76923605a84834c02b40883b3' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.blockIdCK(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='113' end='1136' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java'><Message>At NamenodeFsck.java:[lines 113-1136]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NamenodeFsck</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' signature='(Ljava/lang/String;)V' name='blockIdCK' primary='true'><SourceLine endBytecode='1327' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='262' end='333' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.blockIdCK(String)</Message></Method><SourceLine endBytecode='736' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='326' end='326' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='736' primary='true'><Message>At NamenodeFsck.java:[line 326]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fdcee46a76f9558b05ec7c72fd50a91b' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(String, HdfsFileStatus, LocatedBlocks)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='113' end='1136' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java'><Message>At NamenodeFsck.java:[lines 113-1136]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NamenodeFsck</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' signature='(Ljava/lang/String;Lorg/apache/hadoop/hdfs/protocol/HdfsFileStatus;Lorg/apache/hadoop/hdfs/protocol/LocatedBlocks;)V' name='copyBlocksToLostFound' primary='true'><SourceLine endBytecode='1190' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='919' end='985' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(String, HdfsFileStatus, LocatedBlocks)</Message></Method><SourceLine endBytecode='514' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='978' end='978' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='514' primary='true'><Message>At NamenodeFsck.java:[line 978]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f681cd9903d53277ab9f2bc18939ee0f' cweid='396' rank='20' abbrev='REC' category='STYLE' priority='3' type='REC_CATCH_EXCEPTION' instanceOccurrenceMax='0'><ShortMessage>Exception is caught when Exception is not thrown</ShortMessage><LongMessage>Exception is caught when Exception is not thrown in org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='113' end='1136' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java'><Message>At NamenodeFsck.java:[lines 113-1136]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.NamenodeFsck</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' signature='()V' name='fsck' primary='true'><SourceLine endBytecode='1847' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='339' end='446' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck()</Message></Method><SourceLine endBytecode='972' classname='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck' start='436' end='436' sourcepath='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' sourcefile='NamenodeFsck.java' startBytecode='972' primary='true'><Message>At NamenodeFsck.java:[line 436]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6e8f79459930abf9d89e8b6c5f2b5863' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$1' start='118' end='121' sourcepath='org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java' sourcefile='RedundantEditLogInputStream.java'><Message>At RedundantEditLogInputStream.java:[lines 118-121]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream' start='40' end='286' sourcepath='org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java' sourcefile='RedundantEditLogInputStream.java'><Message>At RedundantEditLogInputStream.java:[lines 40-286]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream' signature='(Ljava/util/Collection;J)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='546' classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream' start='90' end='124' sourcepath='org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java' sourcefile='RedundantEditLogInputStream.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream(Collection, long)</Message></Method><SourceLine endBytecode='271' classname='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream' start='118' end='118' sourcepath='org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java' sourcefile='RedundantEditLogInputStream.java' startBytecode='271' primary='true'><Message>At RedundantEditLogInputStream.java:[line 118]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d4f1c504638f6f95fb663e08bf04de3a' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.notifyNewSubmission()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='88' end='594' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java'><Message>At ReencryptionHandler.java:[lines 88-594]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' signature='()V' name='notifyNewSubmission' primary='true'><SourceLine endBytecode='64' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='589' end='591' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.notifyNewSubmission()</Message></Method><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='590' end='590' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='11' primary='true'><Message>At ReencryptionHandler.java:[line 590]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='64df88520a579c5a2341f036ac318eb1' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.pauseForTesting()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='88' end='594' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java'><Message>At ReencryptionHandler.java:[lines 88-594]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' signature='()V' name='pauseForTesting' primary='true'><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='155' end='158' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.pauseForTesting()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='157' end='157' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='16' primary='true'><Message>At ReencryptionHandler.java:[line 157]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c804000c1f6d302dc15f740f3c1247cb' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.resumeForTesting()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='88' end='594' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java'><Message>At ReencryptionHandler.java:[lines 88-594]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' signature='()V' name='resumeForTesting' primary='true'><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='162' end='165' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.resumeForTesting()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='164' end='164' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='16' primary='true'><Message>At ReencryptionHandler.java:[line 164]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a85b001401ce659560cffb9dd053d87f' rank='17' abbrev='UW' category='MT_CORRECTNESS' priority='3' type='UW_UNCOND_WAIT' instanceOccurrenceMax='0'><ShortMessage>Unconditional wait</ShortMessage><LongMessage>Unconditional wait in org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.run()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='88' end='594' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java'><Message>At ReencryptionHandler.java:[lines 88-594]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' signature='()V' name='run' primary='true'><SourceLine endBytecode='664' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='325' end='371' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.run()</Message></Method><SourceLine endBytecode='26' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler' start='330' end='330' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' sourcefile='ReencryptionHandler.java' startBytecode='26' primary='true'><Message>At ReencryptionHandler.java:[line 330]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='941343c352513fb532020382db8a4e1f' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.pauseForTesting()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='61' end='548' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java'><Message>At ReencryptionUpdater.java:[lines 61-548]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' signature='()V' name='pauseForTesting' primary='true'><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='194' end='197' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.pauseForTesting()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='196' end='196' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java' startBytecode='16' primary='true'><Message>At ReencryptionUpdater.java:[line 196]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6131103bb9149004c1a61fc4e50abd21' rank='17' abbrev='No' category='MT_CORRECTNESS' priority='3' type='NO_NOTIFY_NOT_NOTIFYALL' instanceOccurrenceMax='0'><ShortMessage>Using notify() rather than notifyAll()</ShortMessage><LongMessage>Using notify rather than notifyAll in org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.resumeForTesting()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='61' end='548' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java'><Message>At ReencryptionUpdater.java:[lines 61-548]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' signature='()V' name='resumeForTesting' primary='true'><SourceLine endBytecode='73' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='201' end='204' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.resumeForTesting()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater' start='203' end='203' sourcepath='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' sourcefile='ReencryptionUpdater.java' startBytecode='16' primary='true'><Message>At ReencryptionUpdater.java:[line 203]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='329c31c8bf1ca50a49b6f9c30feb20ca' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$1' start='1052' end='1055' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java'><Message>At SecondaryNameNode.java:[lines 1052-1055]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='945' end='1073' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java'><Message>At SecondaryNameNode.java:[lines 945-1073]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' signature='()V' name='deleteTempEdits' primary='true'><SourceLine endBytecode='370' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='1052' end='1073' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.deleteTempEdits()</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='1052' end='1052' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java' startBytecode='5' primary='true'><Message>At SecondaryNameNode.java:[line 1052]</Message></SourceLine><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='9' name='filter' register='1'><Message>Local variable named filter</Message></LocalVariable></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='4746e7a612c41c4eb121a00e6612b353' rank='20' abbrev='UCF' category='STYLE' priority='3' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(boolean)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='945' end='1073' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java'><Message>At SecondaryNameNode.java:[lines 945-1073]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' signature='(Z)V' name='recoverCreate' primary='true'><SourceLine endBytecode='532' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='967' end='1017' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(boolean)</Message></Method><SourceLine endBytecode='51' classname='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage' start='975' end='975' sourcepath='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' sourcefile='SecondaryNameNode.java' startBytecode='51' primary='true'><Message>At SecondaryNameNode.java:[line 975]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='265715185ac72c6c875177b3ce14501f' rank='19' abbrev='SnVI' category='BAD_PRACTICE' priority='3' type='SE_NO_SERIALVERSIONID' instanceOccurrenceMax='0'><ShortMessage>Class is Serializable, but doesn't define serialVersionUID</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet is Serializable; consider declaring a serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet' start='41' end='138' sourcepath='org/apache/hadoop/hdfs/server/namenode/StartupProgressServlet.java' sourcefile='StartupProgressServlet.java'><Message>At StartupProgressServlet.java:[lines 41-138]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet' start='41' end='138' sourcepath='org/apache/hadoop/hdfs/server/namenode/StartupProgressServlet.java' sourcefile='StartupProgressServlet.java'><Message>At StartupProgressServlet.java:[lines 41-138]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='78287c0bece42e52471d748b8f17e006' rank='19' abbrev='RR' category='BAD_PRACTICE' priority='3' type='RR_NOT_CHECKED' instanceOccurrenceMax='0'><ShortMessage>Method ignores results of InputStream.read()</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.TransferFsImage.copyFileToStream(OutputStream, File, FileInputStream, DataTransferThrottler, Canceler) ignores result of java.io.FileInputStream.read(byte[])</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.TransferFsImage' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.TransferFsImage' start='68' end='440' sourcepath='org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java' sourcefile='TransferFsImage.java'><Message>At TransferFsImage.java:[lines 68-440]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.TransferFsImage</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.TransferFsImage' signature='(Ljava/io/OutputStream;Ljava/io/File;Ljava/io/FileInputStream;Lorg/apache/hadoop/hdfs/util/DataTransferThrottler;Lorg/apache/hadoop/hdfs/util/Canceler;)V' name='copyFileToStream' primary='true'><SourceLine endBytecode='998' classname='org.apache.hadoop.hdfs.server.namenode.TransferFsImage' start='341' end='402' sourcepath='org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java' sourcefile='TransferFsImage.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.TransferFsImage.copyFileToStream(OutputStream, File, FileInputStream, DataTransferThrottler, Canceler)</Message></Method><Method isStatic='false' role='METHOD_CALLED' classname='java.io.FileInputStream' signature='([B)I' name='read'><SourceLine endBytecode='32' classname='java.io.FileInputStream' start='233' end='233' sourcepath='java/io/FileInputStream.java' sourcefile='FileInputStream.java' startBytecode='0'></SourceLine><Message>Called method java.io.FileInputStream.read(byte[])</Message></Method><SourceLine endBytecode='101' classname='org.apache.hadoop.hdfs.server.namenode.TransferFsImage' start='358' end='358' sourcepath='org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java' sourcefile='TransferFsImage.java' startBytecode='101' primary='true'><Message>At TransferFsImage.java:[line 358]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2971c35ca945735352a3b5ea18aceff8' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.XAttrFormat.toBytes(List) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.XAttrFormat' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.XAttrFormat' start='41' end='159' sourcepath='org/apache/hadoop/hdfs/server/namenode/XAttrFormat.java' sourcefile='XAttrFormat.java'><Message>At XAttrFormat.java:[lines 41-159]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.XAttrFormat</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.XAttrFormat' signature='(Ljava/util/List;)[B' name='toBytes' primary='true'><SourceLine endBytecode='476' classname='org.apache.hadoop.hdfs.server.namenode.XAttrFormat' start='131' end='159' sourcepath='org/apache/hadoop/hdfs/server/namenode/XAttrFormat.java' sourcefile='XAttrFormat.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.XAttrFormat.toBytes(List)</Message></Method><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.XAttrFormat' start='132' end='132' sourcepath='org/apache/hadoop/hdfs/server/namenode/XAttrFormat.java' sourcefile='XAttrFormat.java' startBytecode='14' primary='true'><Message>At XAttrFormat.java:[line 132]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1501850f86daee393d2910f388de6740' cweid='192' rank='20' abbrev='ICAST' category='STYLE' priority='3' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG' instanceOccurrenceMax='0'><ShortMessage>Result of integer multiplication cast to long</ShortMessage><LongMessage>Result of integer multiplication cast to long in new org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer(FSNamesystem, Configuration)</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer' start='75' end='403' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java' sourcefile='EditLogTailer.java'><Message>At EditLogTailer.java:[lines 75-403]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer' signature='(Lorg/apache/hadoop/hdfs/server/namenode/FSNamesystem;Lorg/apache/hadoop/conf/Configuration;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='762' classname='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer' start='160' end='230' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java' sourcefile='EditLogTailer.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer(FSNamesystem, Configuration)</Message></Method><SourceLine endBytecode='284' classname='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer' start='199' end='199' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java' sourcefile='EditLogTailer.java' startBytecode='284' primary='true'><Message>At EditLogTailer.java:[line 199]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='54f86c29ec9874390a36e71a6802080c' cweid='218' rank='16' abbrev='MS' category='MALICIOUS_CODE' priority='1' type='MS_SHOULD_BE_FINAL' instanceOccurrenceMax='0'><ShortMessage>Field isn't final but should be</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo.PARSER isn't final but should be</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' start='82' end='991' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>At HAZKInfoProtos.java:[lines 82-991]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo</Message></Class><Field isStatic='true' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' signature='Lcom/google/protobuf/Parser;' name='PARSER' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>In HAZKInfoProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo.PARSER</Message></Field><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' start='179' end='179' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java' startBytecode='7' primary='true'><Message>At HAZKInfoProtos.java:[line 179]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='517ff7a331e7fb516a69025c07f0218d' rank='16' abbrev='Se' category='BAD_PRACTICE' priority='2' type='SE_BAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Non-transient non-serializable instance field in serializable class</ShortMessage><LongMessage>Class org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo defines non-transient non-serializable instance field unknownFields</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' start='82' end='991' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>At HAZKInfoProtos.java:[lines 82-991]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' signature='Lcom/google/protobuf/UnknownFieldSet;' name='unknownFields' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>In HAZKInfoProtos.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo.unknownFields</Message></Field><Type role='TYPE_FOUND' descriptor='Lcom/google/protobuf/UnknownFieldSet;'><SourceLine classname='com.google.protobuf.UnknownFieldSet' start='59' end='976' sourcepath='com/google/protobuf/UnknownFieldSet.java' sourcefile='UnknownFieldSet.java'><Message>At UnknownFieldSet.java:[lines 59-976]</Message></SourceLine><Message>Actual type com.google.protobuf.UnknownFieldSet</Message></Type><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>In HAZKInfoProtos.java</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fbe2d56227643a7865cb5dccdf67aeb1' rank='17' abbrev='UCF' category='STYLE' priority='2' type='UCF_USELESS_CONTROL_FLOW' instanceOccurrenceMax='0'><ShortMessage>Useless control flow</ShortMessage><LongMessage>Useless control flow in org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder.maybeForceBuilderInitialization()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder' start='520' end='982' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java'><Message>At HAZKInfoProtos.java:[lines 520-982]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder' signature='()V' name='maybeForceBuilderInitialization' primary='true'><SourceLine endBytecode='61' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder' start='546' end='548' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder.maybeForceBuilderInitialization()</Message></Method><SourceLine endBytecode='3' classname='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder' start='546' end='546' sourcepath='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' sourcefile='HAZKInfoProtos.java' startBytecode='3' primary='true'><Message>At HAZKInfoProtos.java:[line 546]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b0989e755efe158ec712d150d120797d' rank='19' abbrev='Eq' category='BAD_PRACTICE' priority='3' type='EQ_COMPARETO_USE_OBJECT_EQUALS' instanceOccurrenceMax='0'><ShortMessage>Class defines compareTo(...) and uses Object.equals()</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff defines compareTo(Object) and uses Object.equals()</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff' start='47' end='134' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiff.java' sourcefile='AbstractINodeDiff.java'><Message>At AbstractINodeDiff.java:[lines 47-134]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff' signature='(Ljava/lang/Object;)I' name='compareTo' primary='true'><SourceLine endBytecode='68' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff' start='47' end='47' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiff.java' sourcefile='AbstractINodeDiff.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff.compareTo(Object)</Message></Method><SourceLine synthetic='true' endBytecode='68' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff' start='47' end='47' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiff.java' sourcefile='AbstractINodeDiff.java' startBytecode='0'><Message>At AbstractINodeDiff.java:[line 47]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f5fe0d5bd7227b63e4aab5beedb250a' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList.changedBetweenSnapshots(Snapshot, Snapshot) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' start='34' end='317' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' sourcefile='AbstractINodeDiffList.java'><Message>At AbstractINodeDiffList.java:[lines 34-317]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' signature='(Lorg/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot;Lorg/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot;)[I' name='changedBetweenSnapshots' primary='true'><SourceLine endBytecode='330' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' start='245' end='269' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' sourcefile='AbstractINodeDiffList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList.changedBetweenSnapshots(Snapshot, Snapshot)</Message></Method><SourceLine endBytecode='8' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' start='246' end='246' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' sourcefile='AbstractINodeDiffList.java' startBytecode='8' primary='true'><Message>At AbstractINodeDiffList.java:[line 246]</Message></SourceLine><SourceLine endBytecode='82' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' start='262' end='262' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' sourcefile='AbstractINodeDiffList.java' startBytecode='82'><Message>At AbstractINodeDiffList.java:[line 262]</Message></SourceLine><SourceLine endBytecode='95' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList' start='267' end='267' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' sourcefile='AbstractINodeDiffList.java' startBytecode='95'><Message>At AbstractINodeDiffList.java:[line 267]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5cb4933afafe899f0b74872d339a4ee' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$1' start='516' end='525' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java' sourcefile='DiffListBySkipList.java'><Message>At DiffListBySkipList.java:[lines 516-525]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList' start='70' end='583' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java' sourcefile='DiffListBySkipList.java'><Message>At DiffListBySkipList.java:[lines 70-583]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList' signature='()Ljava/util/Iterator;' name='iterator' primary='true'><SourceLine endBytecode='93' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList' start='515' end='516' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java' sourcefile='DiffListBySkipList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList.iterator()</Message></Method><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList' start='516' end='516' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java' sourcefile='DiffListBySkipList.java' startBytecode='16' primary='true'><Message>At DiffListBySkipList.java:[line 516]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9c51df1c7281eb22b86a751d956f6c' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$1' start='178' end='185' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java' sourcefile='DirectoryWithSnapshotFeature.java'><Message>At DirectoryWithSnapshotFeature.java:[lines 178-185]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff' start='132' end='294' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java' sourcefile='DirectoryWithSnapshotFeature.java'><Message>At DirectoryWithSnapshotFeature.java:[lines 132-294]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff' signature='(Lorg/apache/hadoop/hdfs/server/namenode/INode$ReclaimContext;Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;Lorg/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature$DirectoryDiff;)V' name='combinePosteriorAndCollectBlocks' primary='true'><SourceLine endBytecode='96' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff' start='178' end='187' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java' sourcefile='DirectoryWithSnapshotFeature.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff.combinePosteriorAndCollectBlocks(INode$ReclaimContext, INodeDirectory, DirectoryWithSnapshotFeature$DirectoryDiff)</Message></Method><SourceLine endBytecode='14' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff' start='178' end='178' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java' sourcefile='DirectoryWithSnapshotFeature.java' startBytecode='14' primary='true'><Message>At DirectoryWithSnapshotFeature.java:[line 178]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1149093c68300f046d9970422e01ebd9' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader$1' start='323' end='326' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java' sourcefile='FSImageFormatPBSnapshot.java'><Message>At FSImageFormatPBSnapshot.java:[lines 323-326]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader' start='99' end='409' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java' sourcefile='FSImageFormatPBSnapshot.java'><Message>At FSImageFormatPBSnapshot.java:[lines 99-409]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader' signature='(Ljava/util/List;Ljava/io/InputStream;Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;Ljava/util/List;Ljava/util/List;)Ljava/util/List;' name='loadDeletedList' primary='true'><SourceLine endBytecode='442' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader' start='308' end='329' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java' sourcefile='FSImageFormatPBSnapshot.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader.loadDeletedList(List, InputStream, INodeDirectory, List, List)</Message></Method><SourceLine endBytecode='163' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader' start='323' end='323' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java' sourcefile='FSImageFormatPBSnapshot.java' startBytecode='163' primary='true'><Message>At FSImageFormatPBSnapshot.java:[line 323]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='abe1a9d557d683a64dbd09507421c385' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff.getBlocks() may expose internal representation by returning FileDiff.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' start='35' end='125' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java' sourcefile='FileDiff.java'><Message>At FileDiff.java:[lines 35-125]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' signature='()[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='getBlocks' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' start='77' end='77' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java' sourcefile='FileDiff.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff.getBlocks()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' signature='[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java' sourcefile='FileDiff.java'><Message>In FileDiff.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff.blocks</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff' start='77' end='77' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java' sourcefile='FileDiff.java' startBytecode='4' primary='true'><Message>At FileDiff.java:[line 77]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='12f2d2472521d5df7f9423afc7ec9a9a' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList.findEarlierSnapshotBlocks(int) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='29' end='143' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java'><Message>At FileDiffList.java:[lines 29-143]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' signature='(I)[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='findEarlierSnapshotBlocks' primary='true'><SourceLine endBytecode='262' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='61' end='74' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList.findEarlierSnapshotBlocks(int)</Message></Method><SourceLine endBytecode='28' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='63' end='63' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java' startBytecode='28' primary='true'><Message>At FileDiffList.java:[line 63]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e42997e6facf39a34aec1adbd9ece723' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList.findLaterSnapshotBlocks(int) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='29' end='143' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java'><Message>At FileDiffList.java:[lines 29-143]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' signature='(I)[Lorg/apache/hadoop/hdfs/server/blockmanagement/BlockInfo;' name='findLaterSnapshotBlocks' primary='true'><SourceLine endBytecode='270' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='78' end='91' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList.findLaterSnapshotBlocks(int)</Message></Method><SourceLine endBytecode='28' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList' start='80' end='80' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' sourcefile='FileDiffList.java' startBytecode='28' primary='true'><Message>At FileDiffList.java:[line 80]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5a2be8c8bc300052b559c8144425cece' rank='20' abbrev='UrF' category='PERFORMANCE' priority='3' type='URF_UNREAD_FIELD' instanceOccurrenceMax='0'><ShortMessage>Unread field</ShortMessage><LongMessage>Unread field: org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo.snapshotDiffScopeDir</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo' start='46' end='245' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffInfo.java' sourcefile='SnapshotDiffInfo.java'><Message>At SnapshotDiffInfo.java:[lines 46-245]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo' signature='Lorg/apache/hadoop/hdfs/server/namenode/INodeDirectory;' name='snapshotDiffScopeDir' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffInfo.java' sourcefile='SnapshotDiffInfo.java'><Message>In SnapshotDiffInfo.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo.snapshotDiffScopeDir</Message></Field><SourceLine endBytecode='70' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffInfo.java' sourcefile='SnapshotDiffInfo.java' startBytecode='70' primary='true'><Message>At SnapshotDiffInfo.java:[line 133]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f32d405c04d65375f27ffd21ffd355c9' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.getSnapshottableDirListing(String) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager' start='77' end='575' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java' sourcefile='SnapshotManager.java'><Message>At SnapshotManager.java:[lines 77-575]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager' signature='(Ljava/lang/String;)[Lorg/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus;' name='getSnapshottableDirListing' primary='true'><SourceLine endBytecode='518' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager' start='428' end='450' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java' sourcefile='SnapshotManager.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.getSnapshottableDirListing(String)</Message></Method><SourceLine endBytecode='13' classname='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager' start='429' end='429' sourcepath='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java' sourcefile='SnapshotManager.java' startBytecode='13' primary='true'><Message>At SnapshotManager.java:[line 429]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='db25e157f7211b829a359e62d540507c' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$1'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$1' start='154' end='158' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java'><Message>At StartupProgress.java:[lines 154-158]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$1</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='52' end='268' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java'><Message>At StartupProgress.java:[lines 52-268]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' signature='(Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/Phase;Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/Step;)Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress$Counter;' name='getCounter' primary='true'><SourceLine endBytecode='125' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='152' end='161' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress.getCounter(Phase, Step)</Message></Method><SourceLine endBytecode='20' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='154' end='154' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java' startBytecode='20' primary='true'><Message>At StartupProgress.java:[line 154]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c9b48ed9b2d1ee8906331e2b0f0da00c' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$2 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$2'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$2' start='161' end='165' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java'><Message>At StartupProgress.java:[lines 161-165]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$2</Message></Class><Class classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='52' end='268' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java'><Message>At StartupProgress.java:[lines 52-268]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' signature='(Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/Phase;Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/Step;)Lorg/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress$Counter;' name='getCounter' primary='true'><SourceLine endBytecode='125' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='152' end='161' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress.getCounter(Phase, Step)</Message></Method><SourceLine endBytecode='29' classname='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress' start='161' end='161' sourcepath='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' sourcefile='StartupProgress.java' startBytecode='29' primary='true'><Message>At StartupProgress.java:[line 161]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ccf821cce69e30759a2ba41d50727b32' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockCommand.getBlocks() may expose internal representation by returning BlockCommand.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='()[Lorg/apache/hadoop/hdfs/protocol/Block;' name='getBlocks' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='112' end='112' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockCommand.getBlocks()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[Lorg/apache/hadoop/hdfs/protocol/Block;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.blocks</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='112' end='112' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='4' primary='true'><Message>At BlockCommand.java:[line 112]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5014b31ac48273133f1b029e7c7acacc' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargetStorageIDs() may expose internal representation by returning BlockCommand.targetStorageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='()[[Ljava/lang/String;' name='getTargetStorageIDs' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='124' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargetStorageIDs()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Ljava/lang/String;' name='targetStorageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targetStorageIDs</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='124' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='4' primary='true'><Message>At BlockCommand.java:[line 124]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a36bbaee7584bb06ff38bf1ba362ab3f' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargetStorageTypes() may expose internal representation by returning BlockCommand.targetStorageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='()[[Lorg/apache/hadoop/fs/StorageType;' name='getTargetStorageTypes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='120' end='120' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargetStorageTypes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Lorg/apache/hadoop/fs/StorageType;' name='targetStorageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targetStorageTypes</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='120' end='120' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='4' primary='true'><Message>At BlockCommand.java:[line 120]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='732c4f9e95e8166a29bb337085c9dc75' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargets() may expose internal representation by returning BlockCommand.targets</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='()[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='getTargets' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='116' end='116' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockCommand.getTargets()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='targets' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targets</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='116' end='116' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='4' primary='true'><Message>At BlockCommand.java:[line 116]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c833e1c86abb5c8744bf8805c999b8c' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][]) may expose internal representation by storing an externally mutable object into BlockCommand.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='(ILjava/lang/String;[Lorg/apache/hadoop/hdfs/protocol/Block;[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[[Lorg/apache/hadoop/fs/StorageType;[[Ljava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='159' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='99' end='105' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[Lorg/apache/hadoop/hdfs/protocol/Block;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.blocks</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='12' name='blocks' register='3'><Message>Local variable named blocks</Message></LocalVariable><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='101' end='101' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='12' primary='true'><Message>At BlockCommand.java:[line 101]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='49d59760e3feb8de008acdac27e1cce4' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][]) may expose internal representation by storing an externally mutable object into BlockCommand.targetStorageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='(ILjava/lang/String;[Lorg/apache/hadoop/hdfs/protocol/Block;[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[[Lorg/apache/hadoop/fs/StorageType;[[Ljava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='159' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='99' end='105' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Ljava/lang/String;' name='targetStorageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targetStorageIDs</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='30' name='targetStorageIDs' register='6'><Message>Local variable named targetStorageIDs</Message></LocalVariable><SourceLine endBytecode='30' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='104' end='104' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='30' primary='true'><Message>At BlockCommand.java:[line 104]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2f00cec76f184febc5304c8831ef95e7' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][]) may expose internal representation by storing an externally mutable object into BlockCommand.targetStorageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='(ILjava/lang/String;[Lorg/apache/hadoop/hdfs/protocol/Block;[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[[Lorg/apache/hadoop/fs/StorageType;[[Ljava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='159' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='99' end='105' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Lorg/apache/hadoop/fs/StorageType;' name='targetStorageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targetStorageTypes</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='24' name='targetStorageTypes' register='5'><Message>Local variable named targetStorageTypes</Message></LocalVariable><SourceLine endBytecode='24' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='103' end='103' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='24' primary='true'><Message>At BlockCommand.java:[line 103]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b9b34f5db479bd3c6884c34e49a461b3' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][]) may expose internal representation by storing an externally mutable object into BlockCommand.targets</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='63' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>At BlockCommand.java:[lines 63-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='(ILjava/lang/String;[Lorg/apache/hadoop/hdfs/protocol/Block;[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[[Lorg/apache/hadoop/fs/StorageType;[[Ljava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='159' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='99' end='105' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockCommand(int, String, Block[], DatanodeInfo[][], StorageType[][], String[][])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' signature='[[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='targets' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java'><Message>In BlockCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockCommand.targets</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='18' name='targets' register='4'><Message>Local variable named targets</Message></LocalVariable><SourceLine endBytecode='18' classname='org.apache.hadoop.hdfs.server.protocol.BlockCommand' start='102' end='102' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' sourcefile='BlockCommand.java' startBytecode='18' primary='true'><Message>At BlockCommand.java:[line 102]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dc386c8693bd8e1aea8f44c6f6d78d1d' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getLiveBlockIndices() may expose internal representation by returning BlockECReconstructionCommand$BlockECReconstructionInfo.liveBlockIndices</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[B' name='getLiveBlockIndices' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='127' end='127' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getLiveBlockIndices()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[B' name='liveBlockIndices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.liveBlockIndices</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='127' end='127' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='4' primary='true'><Message>At BlockECReconstructionCommand.java:[line 127]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ff8ac6c56ecfc8e7f42886b90394366c' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getSourceDnInfos() may expose internal representation by returning BlockECReconstructionCommand$BlockECReconstructionInfo.sources</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='getSourceDnInfos' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='111' end='111' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getSourceDnInfos()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='sources' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.sources</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='111' end='111' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='4' primary='true'><Message>At BlockECReconstructionCommand.java:[line 111]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='95a98b04bb9a445d2fec334189c0327c' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetDnInfos() may expose internal representation by returning BlockECReconstructionCommand$BlockECReconstructionInfo.targets</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='getTargetDnInfos' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='115' end='115' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetDnInfos()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='targets' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targets</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='115' end='115' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='4' primary='true'><Message>At BlockECReconstructionCommand.java:[line 115]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d2f2ba3d05ce0c64a89ee1dfcef2e6ef' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageIDs() may expose internal representation by returning BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[Ljava/lang/String;' name='getTargetStorageIDs' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='119' end='119' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageIDs()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Ljava/lang/String;' name='targetStorageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageIDs</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='119' end='119' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='4' primary='true'><Message>At BlockECReconstructionCommand.java:[line 119]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a0ef351018cfdcbeb6f1d363fff6a3ac' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageTypes() may expose internal representation by returning BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='()[Lorg/apache/hadoop/fs/StorageType;' name='getTargetStorageTypes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='123' end='123' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.getTargetStorageTypes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/fs/StorageType;' name='targetStorageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageTypes</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='123' end='123' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='4' primary='true'><Message>At BlockECReconstructionCommand.java:[line 123]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7d32c62545c2ef254f302585d7ecdbf3' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy) may expose internal representation by storing an externally mutable object into BlockECReconstructionCommand$BlockECReconstructionInfo.sources</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;[BLorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='277' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='95' end='104' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='sources' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.sources</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='sources' register='2'><Message>Local variable named sources</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='97' end='97' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='11' primary='true'><Message>At BlockECReconstructionCommand.java:[line 97]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e291a1200d27b8e309a33f3677cabeae' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy) may expose internal representation by storing an externally mutable object into BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;[BLorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='277' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='95' end='104' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Ljava/lang/String;' name='targetStorageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageIDs</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='22' name='targetStorageIDs' register='4'><Message>Local variable named targetStorageIDs</Message></LocalVariable><SourceLine endBytecode='22' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='99' end='99' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='22' primary='true'><Message>At BlockECReconstructionCommand.java:[line 99]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6d73a90a5c0ebeb8093854a596744b4c' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy) may expose internal representation by storing an externally mutable object into BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;[BLorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='277' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='95' end='104' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/fs/StorageType;' name='targetStorageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targetStorageTypes</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='28' name='targetStorageTypes' register='5'><Message>Local variable named targetStorageTypes</Message></LocalVariable><SourceLine endBytecode='28' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='100' end='100' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='28' primary='true'><Message>At BlockECReconstructionCommand.java:[line 100]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e809893b96fee6133e7d05876b87b7a' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy) may expose internal representation by storing an externally mutable object into BlockECReconstructionCommand$BlockECReconstructionInfo.targets</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='86' end='141' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>At BlockECReconstructionCommand.java:[lines 86-141]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='(Lorg/apache/hadoop/hdfs/protocol/ExtendedBlock;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;[BLorg/apache/hadoop/hdfs/protocol/ErasureCodingPolicy;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='277' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='95' end='104' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo(ExtendedBlock, DatanodeInfo[], DatanodeInfo[], String[], StorageType[], byte[], ErasureCodingPolicy)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' signature='[Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;' name='targets' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java'><Message>In BlockECReconstructionCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo.targets</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='16' name='targets' register='3'><Message>Local variable named targets</Message></LocalVariable><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo' start='98' end='98' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' sourcefile='BlockECReconstructionCommand.java' startBytecode='16' primary='true'><Message>At BlockECReconstructionCommand.java:[line 98]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='dfd5cbcaf113feb5622b35a86f2a40d8' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockIdCommand.getBlockIds() may expose internal representation by returning BlockIdCommand.blockIds</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='37' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java'><Message>At BlockIdCommand.java:[lines 37-47]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockIdCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' signature='()[J' name='getBlockIds' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='47' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockIdCommand.getBlockIds()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' signature='[J' name='blockIds' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java'><Message>In BlockIdCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockIdCommand.blockIds</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='47' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java' startBytecode='4' primary='true'><Message>At BlockIdCommand.java:[line 47]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='bbf980720f9a6d970dfeabc1e0716d63' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlockIdCommand(int, String, long[]) may expose internal representation by storing an externally mutable object into BlockIdCommand.blockIds</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='37' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java'><Message>At BlockIdCommand.java:[lines 37-47]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockIdCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' signature='(ILjava/lang/String;[J)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='99' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='37' end='40' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlockIdCommand(int, String, long[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' signature='[J' name='blockIds' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java'><Message>In BlockIdCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockIdCommand.blockIds</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='12' name='blockIds' register='3'><Message>Local variable named blockIds</Message></LocalVariable><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand' start='39' end='39' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' sourcefile='BlockIdCommand.java' startBytecode='12' primary='true'><Message>At BlockIdCommand.java:[line 39]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='85caccede3be848440cbe8580420166c' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock.getBlockIndices() may expose internal representation by returning BlockRecoveryCommand$RecoveringStripedBlock.blockIndices</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' start='109' end='124' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java'><Message>At BlockRecoveryCommand.java:[lines 109-124]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' signature='()[B' name='getBlockIndices' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' start='115' end='115' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock.getBlockIndices()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' signature='[B' name='blockIndices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java'><Message>In BlockRecoveryCommand.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock.blockIndices</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock' start='115' end='115' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' sourcefile='BlockRecoveryCommand.java' startBytecode='4' primary='true'><Message>At BlockRecoveryCommand.java:[line 115]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f220cea7a36d08565d9f76521556e34e' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.getBlocks() may expose internal representation by returning BlocksWithLocations.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='127' end='133' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 127-133]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' signature='()[Lorg/apache/hadoop/hdfs/server/protocol/BlocksWithLocations$BlockWithLocations;' name='getBlocks' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.getBlocks()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' signature='[Lorg/apache/hadoop/hdfs/server/protocol/BlocksWithLocations$BlockWithLocations;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.blocks</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='133' end='133' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='4' primary='true'><Message>At BlocksWithLocations.java:[line 133]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5da28c0951cdd0aa88d8d200a7738ba' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations(BlocksWithLocations$BlockWithLocations[]) may expose internal representation by storing an externally mutable object into BlocksWithLocations.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='127' end='133' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 127-133]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' signature='([Lorg/apache/hadoop/hdfs/server/protocol/BlocksWithLocations$BlockWithLocations;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='69' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='127' end='129' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations(BlocksWithLocations$BlockWithLocations[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' signature='[Lorg/apache/hadoop/hdfs/server/protocol/BlocksWithLocations$BlockWithLocations;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations.blocks</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='6' name='blocks' register='1'><Message>Local variable named blocks</Message></LocalVariable><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations' start='128' end='128' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='6' primary='true'><Message>At BlocksWithLocations.java:[line 128]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ecce3ccb19d3b848418d76ea890af79a' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getDatanodeUuids() may expose internal representation by returning BlocksWithLocations$BlockWithLocations.datanodeUuids</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='()[Ljava/lang/String;' name='getDatanodeUuids' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='60' end='60' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getDatanodeUuids()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Ljava/lang/String;' name='datanodeUuids' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.datanodeUuids</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='60' end='60' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='4' primary='true'><Message>At BlocksWithLocations.java:[line 60]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ddab007ff34b281407509ab0d1118a83' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getStorageIDs() may expose internal representation by returning BlocksWithLocations$BlockWithLocations.storageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='()[Ljava/lang/String;' name='getStorageIDs' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='65' end='65' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getStorageIDs()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Ljava/lang/String;' name='storageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.storageIDs</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='65' end='65' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='4' primary='true'><Message>At BlocksWithLocations.java:[line 65]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='7fd03b14b694b6c8805a00b174a61b07' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getStorageTypes() may expose internal representation by returning BlocksWithLocations$BlockWithLocations.storageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='()[Lorg/apache/hadoop/fs/StorageType;' name='getStorageTypes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='70' end='70' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.getStorageTypes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Lorg/apache/hadoop/fs/StorageType;' name='storageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.storageTypes</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='70' end='70' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='4' primary='true'><Message>At BlocksWithLocations.java:[line 70]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b6071c56881c8beb524905d229999811' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[]) may expose internal representation by storing an externally mutable object into BlocksWithLocations$BlockWithLocations.datanodeUuids</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='(Lorg/apache/hadoop/hdfs/protocol/Block;[Ljava/lang/String;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='127' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='51' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Ljava/lang/String;' name='datanodeUuids' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.datanodeUuids</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='datanodeUuids' register='2'><Message>Local variable named datanodeUuids</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='48' end='48' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='11' primary='true'><Message>At BlocksWithLocations.java:[line 48]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='9ee16e631e329da8349ae47b0358bbc8' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[]) may expose internal representation by storing an externally mutable object into BlocksWithLocations$BlockWithLocations.storageIDs</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='(Lorg/apache/hadoop/hdfs/protocol/Block;[Ljava/lang/String;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='127' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='51' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Ljava/lang/String;' name='storageIDs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.storageIDs</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='16' name='storageIDs' register='3'><Message>Local variable named storageIDs</Message></LocalVariable><SourceLine endBytecode='16' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='49' end='49' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='16' primary='true'><Message>At BlocksWithLocations.java:[line 49]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='961bfe55d65dc1064842f05e1be68b89' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[]) may expose internal representation by storing an externally mutable object into BlocksWithLocations$BlockWithLocations.storageTypes</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='91' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 46-91]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='(Lorg/apache/hadoop/hdfs/protocol/Block;[Ljava/lang/String;[Ljava/lang/String;[Lorg/apache/hadoop/fs/StorageType;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='127' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='46' end='51' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations(Block, String[], String[], StorageType[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' signature='[Lorg/apache/hadoop/fs/StorageType;' name='storageTypes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations.storageTypes</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='22' name='storageTypes' register='4'><Message>Local variable named storageTypes</Message></LocalVariable><SourceLine endBytecode='22' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations' start='50' end='50' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='22' primary='true'><Message>At BlocksWithLocations.java:[line 50]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1e5cea54c1c1726c09c7ce01c6d66ba2' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations.getIndices() may expose internal representation by returning BlocksWithLocations$StripedBlockWithLocations.indices</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='102' end='120' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 102-120]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' signature='()[B' name='getIndices' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='112' end='112' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations.getIndices()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' signature='[B' name='indices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations.indices</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='112' end='112' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='4' primary='true'><Message>At BlocksWithLocations.java:[line 112]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6cb0d1770f00953c396ae934d75cff09' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations(BlocksWithLocations$BlockWithLocations, byte[], short, int) may expose internal representation by storing an externally mutable object into BlocksWithLocations$StripedBlockWithLocations.indices</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='102' end='120' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>At BlocksWithLocations.java:[lines 102-120]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' signature='(Lorg/apache/hadoop/hdfs/server/protocol/BlocksWithLocations$BlockWithLocations;[BSI)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='200' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='102' end='109' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations(BlocksWithLocations$BlockWithLocations, byte[], short, int)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' signature='[B' name='indices' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java'><Message>In BlocksWithLocations.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations.indices</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='40' name='indices' register='2'><Message>Local variable named indices</Message></LocalVariable><SourceLine endBytecode='40' classname='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations' start='106' end='106' sourcepath='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' sourcefile='BlocksWithLocations.java' startBytecode='40' primary='true'><Message>At BlocksWithLocations.java:[line 106]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fecd46b45320e2c4c3e1599bbef3fa5e' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse.getCommands() may expose internal representation by returning HeartbeatResponse.commands</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='42' end='62' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java'><Message>At HeartbeatResponse.java:[lines 42-62]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' signature='()[Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;' name='getCommands' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='50' end='50' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse.getCommands()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' signature='[Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;' name='commands' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java'><Message>In HeartbeatResponse.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse.commands</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='50' end='50' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java' startBytecode='4' primary='true'><Message>At HeartbeatResponse.java:[line 50]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='a7f91f4af0b979539d7c03286a4a0846' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse(DatanodeCommand[], NNHAStatusHeartbeat, RollingUpgradeStatus, long) may expose internal representation by storing an externally mutable object into HeartbeatResponse.commands</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='42' end='62' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java'><Message>At HeartbeatResponse.java:[lines 42-62]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' signature='([Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;Lorg/apache/hadoop/hdfs/server/protocol/NNHAStatusHeartbeat;Lorg/apache/hadoop/hdfs/protocol/RollingUpgradeStatus;J)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='127' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='42' end='47' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse(DatanodeCommand[], NNHAStatusHeartbeat, RollingUpgradeStatus, long)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' signature='[Lorg/apache/hadoop/hdfs/server/protocol/DatanodeCommand;' name='commands' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java'><Message>In HeartbeatResponse.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse.commands</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='6' name='cmds' register='1'><Message>Local variable named cmds</Message></LocalVariable><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse' start='43' end='43' sourcepath='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' sourcefile='HeartbeatResponse.java' startBytecode='6' primary='true'><Message>At HeartbeatResponse.java:[line 43]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='444e7b359cd31b1e39cdf469eb9b6e31' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks.getBlocks() may expose internal representation by returning StorageReceivedDeletedBlocks.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='36' end='66' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>At StorageReceivedDeletedBlocks.java:[lines 36-66]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='()[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;' name='getBlocks' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='44' end='44' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks.getBlocks()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>In StorageReceivedDeletedBlocks.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks.blocks</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='44' end='44' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='4' primary='true'><Message>At StorageReceivedDeletedBlocks.java:[line 44]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='c4578419f2465af0a96633e393e25b93' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks(String, ReceivedDeletedBlockInfo[]) may expose internal representation by storing an externally mutable object into StorageReceivedDeletedBlocks.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='36' end='66' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>At StorageReceivedDeletedBlocks.java:[lines 36-66]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='(Ljava/lang/String;[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='95' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='53' end='56' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks(String, ReceivedDeletedBlockInfo[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>In StorageReceivedDeletedBlocks.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks.blocks</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='18' name='blocks' register='2'><Message>Local variable named blocks</Message></LocalVariable><SourceLine endBytecode='18' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='55' end='55' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='18' primary='true'><Message>At StorageReceivedDeletedBlocks.java:[line 55]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='2138e9126311e521a63dbff393814370' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks(DatanodeStorage, ReceivedDeletedBlockInfo[]) may expose internal representation by storing an externally mutable object into StorageReceivedDeletedBlocks.blocks</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='36' end='66' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>At StorageReceivedDeletedBlocks.java:[lines 36-66]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='(Lorg/apache/hadoop/hdfs/server/protocol/DatanodeStorage;[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='88' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='59' end='62' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks(DatanodeStorage, ReceivedDeletedBlockInfo[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' signature='[Lorg/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo;' name='blocks' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java'><Message>In StorageReceivedDeletedBlocks.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks.blocks</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='blocks' register='2'><Message>Local variable named blocks</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks' start='61' end='61' sourcepath='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' sourcefile='StorageReceivedDeletedBlocks.java' startBytecode='11' primary='true'><Message>At StorageReceivedDeletedBlocks.java:[line 61]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='908e45d01c33105dc3e88d6b252e352e' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary.getFailedStorageLocations() may expose internal representation by returning VolumeFailureSummary.failedStorageLocations</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='37' end='70' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java'><Message>At VolumeFailureSummary.java:[lines 37-70]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' signature='()[Ljava/lang/String;' name='getFailedStorageLocations' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='49' end='49' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary.getFailedStorageLocations()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' signature='[Ljava/lang/String;' name='failedStorageLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java'><Message>In VolumeFailureSummary.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary.failedStorageLocations</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='49' end='49' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java' startBytecode='4' primary='true'><Message>At VolumeFailureSummary.java:[line 49]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='aa96c6311a70f76004e2e60994a3829d' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary(String[], long, long) may expose internal representation by storing an externally mutable object into VolumeFailureSummary.failedStorageLocations</LongMessage><Class classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='37' end='70' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java'><Message>At VolumeFailureSummary.java:[lines 37-70]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' signature='([Ljava/lang/String;JJ)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='108' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='37' end='41' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary(String[], long, long)</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' signature='[Ljava/lang/String;' name='failedStorageLocations' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java'><Message>In VolumeFailureSummary.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary.failedStorageLocations</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='6' name='failedStorageLocations' register='1'><Message>Local variable named failedStorageLocations</Message></LocalVariable><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary' start='38' end='38' sourcepath='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' sourcefile='VolumeFailureSummary.java' startBytecode='6' primary='true'><Message>At VolumeFailureSummary.java:[line 38]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='13b95d093b8dbf93a5d3a826599b5a44' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.tools.DFSAdmin$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$1'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$1' start='1082' end='1087' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>At DFSAdmin.java:[lines 1082-1087]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.tools.DFSAdmin$1</Message></Class><Class classname='org.apache.hadoop.hdfs.tools.DFSAdmin' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin' start='114' end='2569' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>At DFSAdmin.java:[lines 114-2569]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.DFSAdmin</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin' signature='([Ljava/lang/String;I)I' name='fetchImage' primary='true'><SourceLine endBytecode='152' classname='org.apache.hadoop.hdfs.tools.DFSAdmin' start='1078' end='1090' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.DFSAdmin.fetchImage(String[], int)</Message></Method><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.tools.DFSAdmin' start='1082' end='1082' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='34' primary='true'><Message>At DFSAdmin.java:[line 1082]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e68bc888fa609e4b9e2a034064279ff2' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>DFSAdmin$DFSAdminCommand.dfs not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand.run(Path)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' start='126' end='138' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>At DFSAdmin.java:[lines 126-138]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' signature='Lorg/apache/hadoop/hdfs/DistributedFileSystem;' name='dfs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>In DFSAdmin.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand.dfs</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand' signature='(Lorg/apache/hadoop/fs/Path;)V' name='run' primary='true'><SourceLine endBytecode='113' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand' start='279' end='284' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand.run(Path)</Message></Method><SourceLine endBytecode='19' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand' start='280' end='280' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='19' primary='true'><Message>At DFSAdmin.java:[line 280]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='45b298685ea1b95927656bc336c9e945' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>DFSAdmin$DFSAdminCommand.dfs not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand.run(Path)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' start='126' end='138' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>At DFSAdmin.java:[lines 126-138]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' signature='Lorg/apache/hadoop/hdfs/DistributedFileSystem;' name='dfs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>In DFSAdmin.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand.dfs</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand' signature='(Lorg/apache/hadoop/fs/Path;)V' name='run' primary='true'><SourceLine endBytecode='71' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand' start='224' end='225' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand.run(Path)</Message></Method><SourceLine endBytecode='12' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand' start='224' end='224' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='12' primary='true'><Message>At DFSAdmin.java:[line 224]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ae191e7e265225a64bb36d5ad87f2174' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>DFSAdmin$DFSAdminCommand.dfs not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand.run(Path)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' start='126' end='138' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>At DFSAdmin.java:[lines 126-138]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' signature='Lorg/apache/hadoop/hdfs/DistributedFileSystem;' name='dfs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java'><Message>In DFSAdmin.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand.dfs</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand' signature='(Lorg/apache/hadoop/fs/Path;)V' name='run' primary='true'><SourceLine endBytecode='115' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand' start='356' end='361' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand.run(Path)</Message></Method><SourceLine endBytecode='20' classname='org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand' start='357' end='357' sourcepath='org/apache/hadoop/hdfs/tools/DFSAdmin.java' sourcefile='DFSAdmin.java' startBytecode='20' primary='true'><Message>At DFSAdmin.java:[line 357]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b993d6662eae95c4f5bd174ed614f64d' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.conf.Configuration to org.apache.hadoop.hdfs.HdfsConfiguration of return value in org.apache.hadoop.hdfs.tools.DFSHAAdmin.resolveTarget(String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.DFSHAAdmin' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.DFSHAAdmin' start='39' end='137' sourcepath='org/apache/hadoop/hdfs/tools/DFSHAAdmin.java' sourcefile='DFSHAAdmin.java'><Message>At DFSHAAdmin.java:[lines 39-137]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.DFSHAAdmin</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.DFSHAAdmin' signature='(Ljava/lang/String;)Lorg/apache/hadoop/ha/HAServiceTarget;' name='resolveTarget' primary='true'><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.tools.DFSHAAdmin' start='87' end='88' sourcepath='org/apache/hadoop/hdfs/tools/DFSHAAdmin.java' sourcefile='DFSHAAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.DFSHAAdmin.resolveTarget(String)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/conf/Configuration;'><SourceLine classname='org.apache.hadoop.conf.Configuration' start='223' end='3860' sourcepath='org/apache/hadoop/conf/Configuration.java' sourcefile='Configuration.java'><Message>At Configuration.java:[lines 223-3860]</Message></SourceLine><Message>Actual type org.apache.hadoop.conf.Configuration</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/HdfsConfiguration;'><SourceLine classname='org.apache.hadoop.hdfs.HdfsConfiguration' start='34' end='159' sourcepath='org/apache/hadoop/hdfs/HdfsConfiguration.java' sourcefile='HdfsConfiguration.java'><Message>At HdfsConfiguration.java:[lines 34-159]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.HdfsConfiguration</Message></Type><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.tools.DFSHAAdmin' start='87' end='87' sourcepath='org/apache/hadoop/hdfs/tools/DFSHAAdmin.java' sourcefile='DFSHAAdmin.java' startBytecode='4' primary='true'><Message>At DFSHAAdmin.java:[line 87]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='eea038a0f6880f15c131f1238e3f951' rank='20' abbrev='Dm' category='I18N' priority='3' type='DM_CONVERT_CASE' instanceOccurrenceMax='0'><ShortMessage>Consider using Locale parameterized version of invoked method</ShortMessage><LongMessage>Use of non-localized String.toUpperCase() or String.toLowerCase() in org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand.run(Configuration, List)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' start='436' end='484' sourcepath='org/apache/hadoop/hdfs/tools/ECAdmin.java' sourcefile='ECAdmin.java'><Message>At ECAdmin.java:[lines 436-484]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/util/List;)I' name='run' primary='true'><SourceLine endBytecode='464' classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' start='459' end='484' sourcepath='org/apache/hadoop/hdfs/tools/ECAdmin.java' sourcefile='ECAdmin.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand.run(Configuration, List)</Message></Method><SourceLine endBytecode='145' classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' start='475' end='475' sourcepath='org/apache/hadoop/hdfs/tools/ECAdmin.java' sourcefile='ECAdmin.java' startBytecode='145' primary='true'><Message>At ECAdmin.java:[line 475]</Message></SourceLine><SourceLine role='SOURCE_LINE_ANOTHER_INSTANCE' endBytecode='166' classname='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand' start='476' end='476' sourcepath='org/apache/hadoop/hdfs/tools/ECAdmin.java' sourcefile='ECAdmin.java' startBytecode='166'><Message>Another occurrence at ECAdmin.java:[line 476]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='86e45a174e860982dcbca0b2b4a8031' cweid='476' rank='6' abbrev='NP' category='CORRECTNESS' priority='1' type='NP_NULL_ON_SOME_PATH' instanceOccurrenceMax='0'><ShortMessage>Possible null pointer dereference</ShortMessage><LongMessage>Possible null pointer dereference of commandLine in org.apache.hadoop.hdfs.tools.JMXGet.main(String[])</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.JMXGet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.JMXGet' start='71' end='359' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java'><Message>At JMXGet.java:[lines 71-359]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.JMXGet</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.tools.JMXGet' signature='([Ljava/lang/String;)V' name='main' primary='true'><SourceLine endBytecode='634' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='296' end='359' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.JMXGet.main(String[])</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='52' name='commandLine' register='3'><Message>Value loaded from commandLine</Message></LocalVariable><SourceLine role='SOURCE_LINE_DEREF' endBytecode='54' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='316' end='316' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='54' primary='true'><Message>Dereferenced at JMXGet.java:[line 316]</Message></SourceLine><SourceLine role='SOURCE_LINE_NULL_VALUE' endBytecode='26' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='307' end='307' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='26'><Message>Null value at JMXGet.java:[line 307]</Message></SourceLine><SourceLine role='SOURCE_LINE_KNOWN_NULL' endBytecode='29' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='309' end='309' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='29'><Message>Known null at JMXGet.java:[line 309]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5701fb7c0b2c8bc6cfc87df472e120d6' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of val, which is known to be non-null in org.apache.hadoop.hdfs.tools.JMXGet.main(String[])</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.JMXGet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.JMXGet' start='71' end='359' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java'><Message>At JMXGet.java:[lines 71-359]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.JMXGet</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.tools.JMXGet' signature='([Ljava/lang/String;)V' name='main' primary='true'><SourceLine endBytecode='634' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='296' end='359' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.JMXGet.main(String[])</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='228' name='val' register='9'><Message>Value loaded from val</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.tools.JMXGet' signature='(Ljava/lang/String;)Ljava/lang/String;' name='getValue'><SourceLine endBytecode='382' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='135' end='153' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.tools.JMXGet.getValue(String) of type String</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='230' classname='org.apache.hadoop.hdfs.tools.JMXGet' start='348' end='348' sourcepath='org/apache/hadoop/hdfs/tools/JMXGet.java' sourcefile='JMXGet.java' startBytecode='230' primary='true'><Message>Redundant null check at JMXGet.java:[line 348]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='618de82312107fbefd99b120600581a2' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of nsId, which is known to be non-null in new org.apache.hadoop.hdfs.tools.NNHAServiceTarget(Configuration, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.NNHAServiceTarget' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.NNHAServiceTarget' start='44' end='187' sourcepath='org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java' sourcefile='NNHAServiceTarget.java'><Message>At NNHAServiceTarget.java:[lines 44-187]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.NNHAServiceTarget</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.NNHAServiceTarget' signature='(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;Ljava/lang/String;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='678' classname='org.apache.hadoop.hdfs.tools.NNHAServiceTarget' start='61' end='118' sourcepath='org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java' sourcefile='NNHAServiceTarget.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.tools.NNHAServiceTarget(Configuration, String, String)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='92' name='nsId' register='2'><Message>Value loaded from nsId</Message></LocalVariable><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='93' classname='org.apache.hadoop.hdfs.tools.NNHAServiceTarget' start='78' end='78' sourcepath='org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java' sourcefile='NNHAServiceTarget.java' startBytecode='93' primary='true'><Message>Redundant null check at NNHAServiceTarget.java:[line 78]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8d34a1b7cae9bb54a23d83934ba4a87d' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.cbuf not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.characters(char[], int, int)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Ljava/lang/StringBuffer;' name='cbuf' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.cbuf</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='([CII)V' name='characters' primary='true'><SourceLine endBytecode='87' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='261' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.characters(char[], int, int)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='261' end='261' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='7' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 261]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='195dd99b36387122c3aebf5e95393ffa' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.cbuf not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Ljava/lang/StringBuffer;' name='cbuf' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.cbuf</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V' name='endElement' primary='true'><SourceLine endBytecode='1021' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='180' end='257' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</Message></Method><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='180' end='180' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='4' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 180]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='f665f98863c376499fd64f90172e7547' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.stanza not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Lorg/apache/hadoop/hdfs/util/XMLUtils$Stanza;' name='stanza' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.stanza</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V' name='endElement' primary='true'><SourceLine endBytecode='1021' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='180' end='257' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</Message></Method><SourceLine endBytecode='244' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='215' end='215' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='244' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 215]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8a87f599f15e1bfb6b0bb9c88b77fdf7' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.stanzaStack not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Ljava/util/Stack;' name='stanzaStack' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.stanzaStack</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V' name='endElement' primary='true'><SourceLine endBytecode='1021' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='180' end='257' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</Message></Method><SourceLine endBytecode='251' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='216' end='216' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='251' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 216]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='62226e9f40208b0a9120c79ebe3fe574' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.stanzaStack not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.startElement(String, String, String, Attributes)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Ljava/util/Stack;' name='stanzaStack' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.stanzaStack</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/xml/sax/Attributes;)V' name='startElement' primary='true'><SourceLine endBytecode='505' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='132' end='176' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.startElement(String, String, String, Attributes)</Message></Method><SourceLine endBytecode='253' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='169' end='169' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='253' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 169]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='b0f17367fe58892e97708c31fed00819' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.state not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Lorg/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader$ParseState;' name='state' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.state</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V' name='endElement' primary='true'><SourceLine endBytecode='1021' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='180' end='257' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.endElement(String, String, String)</Message></Method><SourceLine endBytecode='34' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='182' end='182' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='34' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 182]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8ae8305511eade1eff09c377e6ff8255' rank='20' abbrev='UwF' category='STYLE' priority='3' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR' instanceOccurrenceMax='0'><ShortMessage>Field not initialized in constructor but dereferenced without null check</ShortMessage><LongMessage>OfflineEditsXmlLoader.state not initialized in constructor and dereferenced in org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.startElement(String, String, String, Attributes)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='62' end='262' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>At OfflineEditsXmlLoader.java:[lines 62-262]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader</Message></Class><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='Lorg/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader$ParseState;' name='state' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java'><Message>In OfflineEditsXmlLoader.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.state</Message></Field><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' signature='(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/xml/sax/Attributes;)V' name='startElement' primary='true'><SourceLine endBytecode='505' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='132' end='176' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader.startElement(String, String, String, Attributes)</Message></Method><SourceLine endBytecode='7' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader' start='132' end='132' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' sourcefile='OfflineEditsXmlLoader.java' startBytecode='7' primary='true'><Message>At OfflineEditsXmlLoader.java:[line 132]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='1c4121dedc718d649662586bbb36589d' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream(OutputStream[]) may expose internal representation by storing an externally mutable object into TeeOutputStream.outs</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' start='29' end='66' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java' sourcefile='TeeOutputStream.java'><Message>At TeeOutputStream.java:[lines 29-66]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' signature='([Ljava/io/OutputStream;)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='69' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' start='29' end='31' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java' sourcefile='TeeOutputStream.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream(OutputStream[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' signature='[Ljava/io/OutputStream;' name='outs' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java' sourcefile='TeeOutputStream.java'><Message>In TeeOutputStream.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream.outs</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='6' name='outs' register='1'><Message>Local variable named outs</Message></LocalVariable><SourceLine endBytecode='6' classname='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream' start='30' end='30' sourcepath='org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java' sourcefile='TeeOutputStream.java' startBytecode='6' primary='true'><Message>At TeeOutputStream.java:[line 30]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='25a4ec826f273654d0dc53fac431967c' cweid='476' rank='20' abbrev='RCN' category='STYLE' priority='3' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE' instanceOccurrenceMax='0'><ShortMessage>Redundant nullcheck of value known to be non-null</ShortMessage><LongMessage>Redundant nullcheck of path, which is known to be non-null in org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor.processDirectiveXml(OfflineImageReconstructor$Node)</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor' start='1065' end='1206' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java' sourcefile='OfflineImageReconstructor.java'><Message>At OfflineImageReconstructor.java:[lines 1065-1206]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor' signature='(Lorg/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor$Node;)V' name='processDirectiveXml' primary='true'><SourceLine endBytecode='546' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor' start='1164' end='1206' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java' sourcefile='OfflineImageReconstructor.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor.processDirectiveXml(OfflineImageReconstructor$Node)</Message></Method><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='105' name='path' register='4'><Message>Value loaded from path</Message></LocalVariable><Method isStatic='false' role='METHOD_RETURN_VALUE_OF' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node' signature='(Ljava/lang/String;)Ljava/lang/String;' name='removeChildStr'><SourceLine endBytecode='162' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node' start='279' end='287' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java' sourcefile='OfflineImageReconstructor.java' startBytecode='0'></SourceLine><Message>Return value of org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node.removeChildStr(String) of type String</Message></Method><SourceLine role='SOURCE_REDUNDANT_NULL_CHECK' endBytecode='107' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor' start='1181' end='1181' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java' sourcefile='OfflineImageReconstructor.java' startBytecode='107' primary='true'><Message>Redundant null check at OfflineImageReconstructor.java:[line 1181]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d83dfdda1776b3c405a93437fe21cd47' cweid='382' rank='19' abbrev='Dm' category='BAD_PRACTICE' priority='3' type='DM_EXIT' instanceOccurrenceMax='0'><ShortMessage>Method invokes System.exit(...)</ShortMessage><LongMessage>org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.run(String[]) invokes System.exit(...), which shuts down the entire virtual machine</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='44' end='238' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java'><Message>At OfflineImageViewerPB.java:[lines 44-238]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' signature='([Ljava/lang/String;)I' name='run' primary='true'><SourceLine endBytecode='2594' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='141' end='226' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.run(String[])</Message></Method><SourceLine endBytecode='494' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='198' end='198' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java' startBytecode='494' primary='true'><Message>At OfflineImageViewerPB.java:[line 198]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='5d6223ad396902535c9acd893442e4fc' rank='19' abbrev='OS' category='BAD_PRACTICE' priority='3' type='OS_OPEN_STREAM_EXCEPTION_PATH' instanceOccurrenceMax='0'><ShortMessage>Method may fail to close stream on exception</ShortMessage><LongMessage>org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.run(String[]) may fail to close stream on exception</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='44' end='238' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java'><Message>At OfflineImageViewerPB.java:[lines 44-238]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' signature='([Ljava/lang/String;)I' name='run' primary='true'><SourceLine endBytecode='394' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='141' end='226' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB.run(String[])</Message></Method><Type role='TYPE_CLOSEIT' descriptor='Ljava/io/OutputStream;'><SourceLine classname='java.io.OutputStream' start='46' end='152' sourcepath='java/io/OutputStream.java' sourcefile='OutputStream.java'><Message>At OutputStream.java:[lines 46-152]</Message></SourceLine><Message>Need to close java.io.OutputStream </Message></Type><SourceLine endBytecode='156' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB' start='177' end='177' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' sourcefile='OfflineImageViewerPB.java' startBytecode='156' primary='true'><Message>At OfflineImageViewerPB.java:[line 177]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='d09a8bc7b658794d702bb7a463592218' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1' start='444' end='457' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java'><Message>At PBImageTextWriter.java:[lines 444-457]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1</Message></Class><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter' start='87' end='643' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java'><Message>At PBImageTextWriter.java:[lines 87-643]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter' signature='(Ljava/io/RandomAccessFile;)V' name='visit' primary='true'><SourceLine endBytecode='890' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter' start='432' end='489' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter.visit(RandomAccessFile)</Message></Method><SourceLine endBytecode='62' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter' start='443' end='443' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java' startBytecode='62' primary='true'><Message>At PBImageTextWriter.java:[line 443]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='ef2fb3983ac93718b26e4a7a279f5b36' rank='19' abbrev='SnVI' category='BAD_PRACTICE' priority='3' type='SE_NO_SERIALVERSIONID' instanceOccurrenceMax='0'><ShortMessage>Class is Serializable, but doesn't define serialVersionUID</ShortMessage><LongMessage>org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache is Serializable; consider declaring a serialVersionUID</LongMessage><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache' start='282' end='287' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java'><Message>At PBImageTextWriter.java:[lines 282-287]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache</Message></Class><SourceLine synthetic='true' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache' start='282' end='287' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' sourcefile='PBImageTextWriter.java'><Message>At PBImageTextWriter.java:[lines 282-287]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='cd0c35109c829c3bbda36e8beb25ec3c' rank='20' abbrev='SIC' category='PERFORMANCE' priority='3' type='SIC_INNER_SHOULD_BE_STATIC_ANON' instanceOccurrenceMax='0'><ShortMessage>Could be refactored into a named static inner class</ShortMessage><LongMessage>The class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1 could be refactored into a named _static_ inner class</LongMessage><Class role='CLASS_ANONYMOUS' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1' start='308' end='318' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java' sourcefile='PBImageXmlWriter.java'><Message>At PBImageXmlWriter.java:[lines 308-318]</Message></SourceLine><Message>Anonymous class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1</Message></Class><Class classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter' start='275' end='874' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java' sourcefile='PBImageXmlWriter.java'><Message>At PBImageXmlWriter.java:[lines 275-874]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter' signature='(Ljava/io/RandomAccessFile;)V' name='visit' primary='true'><SourceLine endBytecode='962' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter' start='288' end='369' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java' sourcefile='PBImageXmlWriter.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter.visit(RandomAccessFile)</Message></Method><SourceLine endBytecode='118' classname='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter' start='308' end='308' sourcepath='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java' sourcefile='PBImageXmlWriter.java' startBytecode='118' primary='true'><Message>At PBImageXmlWriter.java:[line 308]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e0f4b4b5fbaaee13c7bf300ccd8c5c0e' cweid='374' rank='18' abbrev='EI' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by returning reference to mutable object</ShortMessage><LongMessage>org.apache.hadoop.hdfs.util.ByteArray.getBytes() may expose internal representation by returning ByteArray.bytes</LongMessage><Class classname='org.apache.hadoop.hdfs.util.ByteArray' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.ByteArray' start='29' end='53' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java'><Message>At ByteArray.java:[lines 29-53]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.ByteArray</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.util.ByteArray' signature='()[B' name='getBytes' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.util.ByteArray' start='37' end='37' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.util.ByteArray.getBytes()</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.util.ByteArray' signature='[B' name='bytes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.ByteArray' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java'><Message>In ByteArray.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.util.ByteArray.bytes</Message></Field><SourceLine endBytecode='4' classname='org.apache.hadoop.hdfs.util.ByteArray' start='37' end='37' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java' startBytecode='4' primary='true'><Message>At ByteArray.java:[line 37]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='41a41072257ce67bff98e17181eb350' cweid='374' rank='18' abbrev='EI2' category='MALICIOUS_CODE' priority='2' type='EI_EXPOSE_REP2' instanceOccurrenceMax='0'><ShortMessage>May expose internal representation by incorporating reference to mutable object</ShortMessage><LongMessage>new org.apache.hadoop.hdfs.util.ByteArray(byte[]) may expose internal representation by storing an externally mutable object into ByteArray.bytes</LongMessage><Class classname='org.apache.hadoop.hdfs.util.ByteArray' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.ByteArray' start='29' end='53' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java'><Message>At ByteArray.java:[lines 29-53]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.ByteArray</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.util.ByteArray' signature='([B)V' name='&lt;init&gt;' primary='true'><SourceLine endBytecode='78' classname='org.apache.hadoop.hdfs.util.ByteArray' start='32' end='34' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java' startBytecode='0'></SourceLine><Message>In method new org.apache.hadoop.hdfs.util.ByteArray(byte[])</Message></Method><Field isStatic='false' classname='org.apache.hadoop.hdfs.util.ByteArray' signature='[B' name='bytes' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.ByteArray' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java'><Message>In ByteArray.java</Message></SourceLine><Message>Field org.apache.hadoop.hdfs.util.ByteArray.bytes</Message></Field><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='11' name='bytes' register='1'><Message>Local variable named bytes</Message></LocalVariable><SourceLine endBytecode='11' classname='org.apache.hadoop.hdfs.util.ByteArray' start='33' end='33' sourcepath='org/apache/hadoop/hdfs/util/ByteArray.java' sourcefile='ByteArray.java' startBytecode='11' primary='true'><Message>At ByteArray.java:[line 33]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='6ec24426b257112861aec1494ce6bbdd' cweid='563' rank='20' abbrev='DLS' category='STYLE' priority='3' type='DLS_DEAD_LOCAL_STORE' instanceOccurrenceMax='0'><ShortMessage>Dead store to local variable</ShortMessage><LongMessage>Dead store to previous in org.apache.hadoop.hdfs.util.Diff.modify(Diff$Element, Diff$Element)</LongMessage><Class classname='org.apache.hadoop.hdfs.util.Diff' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.Diff' start='133' end='538' sourcepath='org/apache/hadoop/hdfs/util/Diff.java' sourcefile='Diff.java'><Message>At Diff.java:[lines 133-538]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.Diff</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.util.Diff' signature='(Lorg/apache/hadoop/hdfs/util/Diff$Element;Lorg/apache/hadoop/hdfs/util/Diff$Element;)Lorg/apache/hadoop/hdfs/util/Diff$UndoInfo;' name='modify' primary='true'><SourceLine endBytecode='82' classname='org.apache.hadoop.hdfs.util.Diff' start='315' end='337' sourcepath='org/apache/hadoop/hdfs/util/Diff.java' sourcefile='Diff.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.util.Diff.modify(Diff$Element, Diff$Element)</Message></Method><LocalVariable role='LOCAL_VARIABLE_NAMED' pc='100' name='previous' register='4'><Message>Local variable named previous</Message></LocalVariable><SourceLine endBytecode='98' classname='org.apache.hadoop.hdfs.util.Diff' start='325' end='325' sourcepath='org/apache/hadoop/hdfs/util/Diff.java' sourcefile='Diff.java' startBytecode='98' primary='true'><Message>At Diff.java:[line 325]</Message></SourceLine><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.KILLED_BY_SUBSEQUENT_STORE' value='true'></Property><Property name='edu.umd.cs.findbugs.detect.DeadLocalStoreProperty.LOCAL_NAME' value='previous'></Property></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3023bd835389b022f3544933dcfd3c23' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast of return value from method</ShortMessage><LongMessage>Unchecked/unconfirmed cast from org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedElement&lt;T&gt; to org.apache.hadoop.hdfs.util.LightWeightLinkedSet$DoubleLinkedElement of return value in org.apache.hadoop.hdfs.util.LightWeightLinkedSet.removeElem(Object)</LongMessage><Class classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet' start='35' end='299' sourcepath='org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java' sourcefile='LightWeightLinkedSet.java'><Message>At LightWeightLinkedSet.java:[lines 35-299]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.LightWeightLinkedSet</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet' signature='(Ljava/lang/Object;)Lorg/apache/hadoop/hdfs/util/LightWeightLinkedSet$DoubleLinkedElement;' name='removeElem' primary='true'><SourceLine endBytecode='58' classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet' start='134' end='158' sourcepath='org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java' sourcefile='LightWeightLinkedSet.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.util.LightWeightLinkedSet.removeElem(Object)</Message></Method><Type role='TYPE_FOUND' descriptor='Lorg/apache/hadoop/hdfs/util/LightWeightHashSet$LinkedElement;' typeParameters='&lt;T&gt;'><SourceLine classname='org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedElement' start='52' end='60' sourcepath='org/apache/hadoop/hdfs/util/LightWeightHashSet.java' sourcefile='LightWeightHashSet.java'><Message>At LightWeightHashSet.java:[lines 52-60]</Message></SourceLine><Message>Actual type org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedElement&lt;T&gt;</Message></Type><Type role='TYPE_EXPECTED' descriptor='Lorg/apache/hadoop/hdfs/util/LightWeightLinkedSet$DoubleLinkedElement;'><SourceLine classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet$DoubleLinkedElement' start='39' end='52' sourcepath='org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java' sourcefile='LightWeightLinkedSet.java'><Message>At LightWeightLinkedSet.java:[lines 39-52]</Message></SourceLine><Message>Expected org.apache.hadoop.hdfs.util.LightWeightLinkedSet$DoubleLinkedElement</Message></Type><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.util.LightWeightLinkedSet' start='135' end='135' sourcepath='org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java' sourcefile='LightWeightLinkedSet.java' startBytecode='5' primary='true'><Message>At LightWeightLinkedSet.java:[line 135]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='789f66b1a4472c478eb2d54122ef0a39' rank='19' abbrev='OS' category='BAD_PRACTICE' priority='3' type='OS_OPEN_STREAM_EXCEPTION_PATH' instanceOccurrenceMax='0'><ShortMessage>Method may fail to close stream on exception</ShortMessage><LongMessage>org.apache.hadoop.hdfs.util.MD5FileUtils.saveMD5File(File, String) may fail to close stream on exception</LongMessage><Class classname='org.apache.hadoop.hdfs.util.MD5FileUtils' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.MD5FileUtils' start='44' end='186' sourcepath='org/apache/hadoop/hdfs/util/MD5FileUtils.java' sourcefile='MD5FileUtils.java'><Message>At MD5FileUtils.java:[lines 44-186]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.MD5FileUtils</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.util.MD5FileUtils' signature='(Ljava/io/File;Ljava/lang/String;)V' name='saveMD5File' primary='true'><SourceLine endBytecode='46' classname='org.apache.hadoop.hdfs.util.MD5FileUtils' start='154' end='164' sourcepath='org/apache/hadoop/hdfs/util/MD5FileUtils.java' sourcefile='MD5FileUtils.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.util.MD5FileUtils.saveMD5File(File, String)</Message></Method><Type role='TYPE_CLOSEIT' descriptor='Ljava/io/OutputStream;'><SourceLine classname='java.io.OutputStream' start='46' end='152' sourcepath='java/io/OutputStream.java' sourcefile='OutputStream.java'><Message>At OutputStream.java:[lines 46-152]</Message></SourceLine><Message>Need to close java.io.OutputStream </Message></Type><SourceLine endBytecode='37' classname='org.apache.hadoop.hdfs.util.MD5FileUtils' start='157' end='157' sourcepath='org/apache/hadoop/hdfs/util/MD5FileUtils.java' sourcefile='MD5FileUtils.java' startBytecode='37' primary='true'><Message>At MD5FileUtils.java:[line 157]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='8a090a7d3825aa7c2c8d8162010bed4d' rank='19' abbrev='OS' category='BAD_PRACTICE' priority='3' type='OS_OPEN_STREAM_EXCEPTION_PATH' instanceOccurrenceMax='0'><ShortMessage>Method may fail to close stream on exception</ShortMessage><LongMessage>org.apache.hadoop.hdfs.util.PersistentLongFile.writeFile(File, long) may fail to close stream on exception</LongMessage><Class classname='org.apache.hadoop.hdfs.util.PersistentLongFile' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.util.PersistentLongFile' start='40' end='105' sourcepath='org/apache/hadoop/hdfs/util/PersistentLongFile.java' sourcefile='PersistentLongFile.java'><Message>At PersistentLongFile.java:[lines 40-105]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.util.PersistentLongFile</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.util.PersistentLongFile' signature='(Ljava/io/File;J)V' name='writeFile' primary='true'><SourceLine endBytecode='30' classname='org.apache.hadoop.hdfs.util.PersistentLongFile' start='78' end='89' sourcepath='org/apache/hadoop/hdfs/util/PersistentLongFile.java' sourcefile='PersistentLongFile.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.util.PersistentLongFile.writeFile(File, long)</Message></Method><Type role='TYPE_CLOSEIT' descriptor='Ljava/io/OutputStream;'><SourceLine classname='java.io.OutputStream' start='46' end='152' sourcepath='java/io/OutputStream.java' sourcefile='OutputStream.java'><Message>At OutputStream.java:[lines 46-152]</Message></SourceLine><Message>Need to close java.io.OutputStream </Message></Type><SourceLine endBytecode='0' classname='org.apache.hadoop.hdfs.util.PersistentLongFile' start='78' end='78' sourcepath='org/apache/hadoop/hdfs/util/PersistentLongFile.java' sourcefile='PersistentLongFile.java' startBytecode='0' primary='true'><Message>At PersistentLongFile.java:[line 78]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e5089d604bf4f32511ca5bf3545c153b' rank='20' abbrev='BC' category='STYLE' priority='3' type='BC_UNCONFIRMED_CAST' instanceOccurrenceMax='0'><ShortMessage>Unchecked/unconfirmed cast</ShortMessage><LongMessage>Unchecked/unconfirmed cast from javax.servlet.ServletRequest to javax.servlet.http.HttpServletRequest in org.apache.hadoop.hdfs.web.AuthFilter.doFilter(ServletRequest, ServletResponse, FilterChain)</LongMessage><Class classname='org.apache.hadoop.hdfs.web.AuthFilter' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.web.AuthFilter' start='48' end='113' sourcepath='org/apache/hadoop/hdfs/web/AuthFilter.java' sourcefile='AuthFilter.java'><Message>At AuthFilter.java:[lines 48-113]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.web.AuthFilter</Message></Class><Method isStatic='false' classname='org.apache.hadoop.hdfs.web.AuthFilter' signature='(Ljavax/servlet/ServletRequest;Ljavax/servlet/ServletResponse;Ljavax/servlet/FilterChain;)V' name='doFilter' primary='true'><SourceLine endBytecode='20' classname='org.apache.hadoop.hdfs.web.AuthFilter' start='82' end='91' sourcepath='org/apache/hadoop/hdfs/web/AuthFilter.java' sourcefile='AuthFilter.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.web.AuthFilter.doFilter(ServletRequest, ServletResponse, FilterChain)</Message></Method><Type role='TYPE_FOUND' descriptor='Ljavax/servlet/ServletRequest;'><SourceLine classname='javax.servlet.ServletRequest' sourcepath='javax/servlet/ServletRequest.java' sourcefile='ServletRequest.java'><Message>In ServletRequest.java</Message></SourceLine><Message>Actual type javax.servlet.ServletRequest</Message></Type><Type role='TYPE_EXPECTED' descriptor='Ljavax/servlet/http/HttpServletRequest;'><SourceLine classname='javax.servlet.http.HttpServletRequest' sourcepath='javax/servlet/http/HttpServletRequest.java' sourcefile='HttpServletRequest.java'><Message>In HttpServletRequest.java</Message></SourceLine><Message>Expected javax.servlet.http.HttpServletRequest</Message></Type><LocalVariable role='LOCAL_VARIABLE_VALUE_OF' pc='0' name='request' register='1'><Message>Value loaded from request</Message></LocalVariable><SourceLine endBytecode='1' classname='org.apache.hadoop.hdfs.web.AuthFilter' start='82' end='82' sourcepath='org/apache/hadoop/hdfs/web/AuthFilter.java' sourcefile='AuthFilter.java' startBytecode='1' primary='true'><Message>At AuthFilter.java:[line 82]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='3e6a2e20f59878223e79e235f918c4bc' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(List) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.web.JsonUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.web.JsonUtil' start='48' end='573' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java'><Message>At JsonUtil.java:[lines 48-573]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.web.JsonUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.web.JsonUtil' signature='(Ljava/util/List;)[Ljava/lang/Object;' name='toJsonArray' primary='true'><SourceLine endBytecode='196' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='316' end='325' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(List)</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='317' end='317' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='5' primary='true'><Message>At JsonUtil.java:[line 317]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='fbc18c6a52c084086fd6184e257ed277' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(List, XAttrCodec) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.web.JsonUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.web.JsonUtil' start='48' end='573' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java'><Message>At JsonUtil.java:[lines 48-573]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.web.JsonUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.web.JsonUtil' signature='(Ljava/util/List;Lorg/apache/hadoop/fs/XAttrCodec;)[Ljava/lang/Object;' name='toJsonArray' primary='true'><SourceLine endBytecode='207' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='437' end='446' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(List, XAttrCodec)</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='438' end='438' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='5' primary='true'><Message>At JsonUtil.java:[line 438]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='e1315f793f23ecd388259fc084fc4a26' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(StorageType[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.web.JsonUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.web.JsonUtil' start='48' end='573' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java'><Message>At JsonUtil.java:[lines 48-573]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.web.JsonUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.web.JsonUtil' signature='([Lorg/apache/hadoop/fs/StorageType;)[Ljava/lang/Object;' name='toJsonArray' primary='true'><SourceLine endBytecode='156' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='244' end='253' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(StorageType[])</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='245' end='245' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='5' primary='true'><Message>At JsonUtil.java:[line 245]</Message></SourceLine></BugInstance><BugInstance instanceOccurrenceNum='0' instanceHash='34fc805fae88f41983223056a2ee298e' rank='20' abbrev='PZLA' category='STYLE' priority='3' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS' instanceOccurrenceMax='0'><ShortMessage>Consider returning a zero length array rather than null</ShortMessage><LongMessage>Should org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(DatanodeInfo[]) return a zero length array rather than null?</LongMessage><Class classname='org.apache.hadoop.hdfs.web.JsonUtil' primary='true'><SourceLine classname='org.apache.hadoop.hdfs.web.JsonUtil' start='48' end='573' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java'><Message>At JsonUtil.java:[lines 48-573]</Message></SourceLine><Message>In class org.apache.hadoop.hdfs.web.JsonUtil</Message></Class><Method isStatic='true' classname='org.apache.hadoop.hdfs.web.JsonUtil' signature='([Lorg/apache/hadoop/hdfs/protocol/DatanodeInfo;)[Ljava/lang/Object;' name='toJsonArray' primary='true'><SourceLine endBytecode='159' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='229' end='238' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='0'></SourceLine><Message>In method org.apache.hadoop.hdfs.web.JsonUtil.toJsonArray(DatanodeInfo[])</Message></Method><SourceLine endBytecode='5' classname='org.apache.hadoop.hdfs.web.JsonUtil' start='230' end='230' sourcepath='org/apache/hadoop/hdfs/web/JsonUtil.java' sourcefile='JsonUtil.java' startBytecode='5' primary='true'><Message>At JsonUtil.java:[line 230]</Message></SourceLine></BugInstance><BugCategory category='BAD_PRACTICE'><Description>Bad practice</Description></BugCategory><BugCategory category='MALICIOUS_CODE'><Description>Malicious code vulnerability</Description></BugCategory><BugCategory category='PERFORMANCE'><Description>Performance</Description></BugCategory><BugCategory category='CORRECTNESS'><Description>Correctness</Description></BugCategory><BugCategory category='STYLE'><Description>Dodgy code</Description></BugCategory><BugCategory category='MT_CORRECTNESS'><Description>Multithreaded correctness</Description></BugCategory><BugCategory category='I18N'><Description>Internationalization</Description></BugCategory><BugPattern abbrev='No' category='MT_CORRECTNESS' type='NO_NOTIFY_NOT_NOTIFYALL'><ShortDescription>Using notify() rather than notifyAll()</ShortDescription><Details>

  &lt;p&gt; This method calls &lt;code&gt;notify()&lt;/code&gt; rather than &lt;code&gt;notifyAll()&lt;/code&gt;.&amp;nbsp;
  Java monitors are often used for multiple conditions.&amp;nbsp; Calling &lt;code&gt;notify()&lt;/code&gt;
  only wakes up one thread, meaning that the thread woken up might not be the
  one waiting for the condition that the caller just satisfied.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='BSHIFT' category='STYLE' type='ICAST_QUESTIONABLE_UNSIGNED_RIGHT_SHIFT'><ShortDescription>Unsigned right shift cast to short/byte</ShortDescription><Details>

&lt;p&gt;
The code performs an unsigned right shift, whose result is then
cast to a short or byte, which discards the upper bits of the result.
Since the upper bits are discarded, there may be no difference between
a signed and unsigned right shift (depending upon the size of the shift).
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NP' category='STYLE' type='NP_NULL_ON_SOME_PATH_FROM_RETURN_VALUE'><ShortDescription>Possible null pointer dereference due to return value of called method</ShortDescription><Details>
      
&lt;p&gt; The return value from a method is dereferenced without a null check,
and the return value of that method is one that should generally be checked
for null.  This may lead to a &lt;code&gt;NullPointerException&lt;/code&gt; when the code is executed.
&lt;/p&gt;
      
   </Details></BugPattern><BugPattern abbrev='Nm' category='BAD_PRACTICE' type='NM_METHOD_NAMING_CONVENTION'><ShortDescription>Method names should start with a lower case letter</ShortDescription><Details>

  &lt;p&gt;
Methods should be verbs, in mixed case with the first letter lowercase, with the first letter of each internal word capitalized.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='ICAST' category='STYLE' type='ICAST_INTEGER_MULTIPLY_CAST_TO_LONG'><ShortDescription>Result of integer multiplication cast to long</ShortDescription><Details>

&lt;p&gt;
This code performs integer multiply and then converts the result to a long,
as in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;long convertDaysToMilliseconds(int days) { return 1000*3600*24*days; }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
If the multiplication is done using long arithmetic, you can avoid
the possibility that the result will overflow. For example, you
could fix the above code to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;long convertDaysToMilliseconds(int days) { return 1000L*3600*24*days; }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
or
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;static final long MILLISECONDS_PER_DAY = 24L*3600*1000;
long convertDaysToMilliseconds(int days) { return days * MILLISECONDS_PER_DAY; }
&lt;/code&gt;&lt;/pre&gt;

    </Details></BugPattern><BugPattern abbrev='UrF' category='PERFORMANCE' type='URF_UNREAD_FIELD'><ShortDescription>Unread field</ShortDescription><Details>

  &lt;p&gt; This field is never read.&amp;nbsp; Consider removing it from the class.&lt;/p&gt;

    </Details></BugPattern><BugPattern cweid='396' abbrev='REC' category='STYLE' type='REC_CATCH_EXCEPTION'><ShortDescription>Exception is caught when Exception is not thrown</ShortDescription><Details>
  
  &lt;p&gt;
  This method uses a try-catch block that catches Exception objects, but Exception is not
  thrown within the try block, and RuntimeException is not explicitly caught.  It is a common bug pattern to
  say try { ... } catch (Exception e) { something } as a shorthand for catching a number of types of exception
  each of whose catch blocks is identical, but this construct also accidentally catches RuntimeException as well,
  masking potential bugs.
  &lt;/p&gt;
  &lt;p&gt;A better approach is to either explicitly catch the specific exceptions that are thrown,
  or to explicitly catch RuntimeException exception, rethrow it, and then catch all non-Runtime Exceptions, as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;try {
    ...
} catch (RuntimeException e) {
    throw e;
} catch (Exception e) {
    ... deal with all non-runtime exceptions ...
}
&lt;/code&gt;&lt;/pre&gt;
  
     </Details></BugPattern><BugPattern abbrev='RCN' category='STYLE' type='RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE'><ShortDescription>Redundant nullcheck of value known to be non-null</ShortDescription><Details>

&lt;p&gt; This method contains a redundant check of a known non-null value against
the constant null.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='MS' category='MALICIOUS_CODE' type='MS_PKGPROTECT'><ShortDescription>Field should be package protected</ShortDescription><Details>

  &lt;p&gt; A mutable static field could be changed by malicious code or
   by accident.
   The field could be made package protected to avoid
   this vulnerability.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='ST' category='STYLE' type='ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD'><ShortDescription>Write to static field from instance method</ShortDescription><Details>

  &lt;p&gt; This instance method writes to a static field. This is tricky to get
correct if multiple instances are being manipulated,
and generally bad practice.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='SIC' category='PERFORMANCE' type='SIC_INNER_SHOULD_BE_STATIC_ANON'><ShortDescription>Could be refactored into a named static inner class</ShortDescription><Details>

  &lt;p&gt; This class is an inner class, but does not use its embedded reference
  to the object which created it.&amp;nbsp; This reference makes the instances
  of the class larger, and may keep the reference to the creator object
  alive longer than necessary.&amp;nbsp; If possible, the class should be
  made into a &lt;em&gt;static&lt;/em&gt; inner class. Since anonymous inner
classes cannot be marked as static, doing this will require refactoring
the inner class so that it is a named inner class.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='DLS' category='CORRECTNESS' type='DLS_DEAD_STORE_OF_CLASS_LITERAL'><ShortDescription>Dead store of class literal</ShortDescription><Details>

&lt;p&gt;
This instruction assigns a class literal to a variable and then never uses it.
&lt;a href="http://www.oracle.com/technetwork/java/javase/compatibility-137462.html#literal"&gt;The behavior of this differs in Java 1.4 and in Java 5.&lt;/a&gt;
In Java 1.4 and earlier, a reference to &lt;code&gt;Foo.class&lt;/code&gt; would force the static initializer
for &lt;code&gt;Foo&lt;/code&gt; to be executed, if it has not been executed already.
In Java 5 and later, it does not.
&lt;/p&gt;
&lt;p&gt;See Sun's &lt;a href="http://www.oracle.com/technetwork/java/javase/compatibility-137462.html#literal"&gt;article on Java SE compatibility&lt;/a&gt;
for more details and examples, and suggestions on how to force class initialization in Java 5.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='Se' category='BAD_PRACTICE' type='SE_COMPARATOR_SHOULD_BE_SERIALIZABLE'><ShortDescription>Comparator doesn't implement Serializable</ShortDescription><Details>

  &lt;p&gt; This class implements the &lt;code&gt;Comparator&lt;/code&gt; interface. You
should consider whether or not it should also implement the &lt;code&gt;Serializable&lt;/code&gt;
interface. If a comparator is used to construct an ordered collection
such as a &lt;code&gt;TreeMap&lt;/code&gt;, then the &lt;code&gt;TreeMap&lt;/code&gt;
will be serializable only if the comparator is also serializable.
As most comparators have little or no state, making them serializable
is generally easy and good defensive programming.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UCF' category='STYLE' type='UCF_USELESS_CONTROL_FLOW'><ShortDescription>Useless control flow</ShortDescription><Details>

&lt;p&gt; This method contains a useless control flow statement, where
control flow continues onto the same place regardless of whether or not
the branch is taken. For example,
this is caused by having an empty statement
block for an &lt;code&gt;if&lt;/code&gt; statement:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (argv.length == 0) {
    // TODO: handle this case
}
&lt;/code&gt;&lt;/pre&gt;

    </Details></BugPattern><BugPattern abbrev='RR' category='BAD_PRACTICE' type='RR_NOT_CHECKED'><ShortDescription>Method ignores results of InputStream.read()</ShortDescription><Details>

  &lt;p&gt; This method ignores the return value of one of the variants of
  &lt;code&gt;java.io.InputStream.read()&lt;/code&gt; which can return multiple bytes.&amp;nbsp;
  If the return value is not checked, the caller will not be able to correctly
  handle the case where fewer bytes were read than the caller requested.&amp;nbsp;
  This is a particularly insidious kind of bug, because in many programs,
  reads from input streams usually do read the full amount of data requested,
  causing the program to fail only sporadically.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='MS' category='MALICIOUS_CODE' type='MS_MUTABLE_ARRAY'><ShortDescription>Field is a mutable array</ShortDescription><Details>

&lt;p&gt; A final static field references an array
   and can be accessed by malicious code or
        by accident from another package.
   This code can freely modify the contents of the array.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='It' category='BAD_PRACTICE' type='IT_NO_SUCH_ELEMENT'><ShortDescription>Iterator next() method can't throw NoSuchElementException</ShortDescription><Details>

  &lt;p&gt; This class implements the &lt;code&gt;java.util.Iterator&lt;/code&gt; interface.&amp;nbsp;
  However, its &lt;code&gt;next()&lt;/code&gt; method is not capable of throwing
  &lt;code&gt;java.util.NoSuchElementException&lt;/code&gt;.&amp;nbsp; The &lt;code&gt;next()&lt;/code&gt;
  method should be changed so it throws &lt;code&gt;NoSuchElementException&lt;/code&gt;
  if is called when there are no more elements to return.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NP' category='STYLE' type='NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE'><ShortDescription>Parameter must be non-null but is marked as nullable</ShortDescription><Details>

&lt;p&gt; This parameter is always used in a way that requires it to be non-null,
but the parameter is explicitly annotated as being Nullable. Either the use
of the parameter or the annotation is wrong.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='VO' category='MT_CORRECTNESS' type='VO_VOLATILE_INCREMENT'><ShortDescription>An increment to a volatile field isn't atomic</ShortDescription><Details>

&lt;p&gt;This code increments a volatile field. Increments of volatile fields aren't
atomic. If more than one thread is incrementing the field at the same time,
increments could be lost.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='MS' category='MALICIOUS_CODE' type='MS_MUTABLE_COLLECTION'><ShortDescription>Field is a mutable collection</ShortDescription><Details>

 &lt;p&gt;A mutable collection instance is assigned to a final static field,
   thus can be changed by malicious code or by accident from another package.
   Consider wrapping this field into Collections.unmodifiableSet/List/Map/etc.
   to avoid this vulnerability.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='PZLA' category='STYLE' type='PZLA_PREFER_ZERO_LENGTH_ARRAYS'><ShortDescription>Consider returning a zero length array rather than null</ShortDescription><Details>

&lt;p&gt; It is often a better design to
return a length zero array rather than a null reference to indicate that there
are no results (i.e., an empty list of results).
This way, no explicit check for null is needed by clients of the method.&lt;/p&gt;

&lt;p&gt;On the other hand, using null to indicate
"there is no answer to this question" is probably appropriate.
For example, &lt;code&gt;File.listFiles()&lt;/code&gt; returns an empty list
if given a directory containing no files, and returns null if the file
is not a directory.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='OS' category='BAD_PRACTICE' type='OS_OPEN_STREAM_EXCEPTION_PATH'><ShortDescription>Method may fail to close stream on exception</ShortDescription><Details>

&lt;p&gt; The method creates an IO stream object, does not assign it to any
fields, pass it to other methods, or return it, and does not appear to close
it on all possible exception paths out of the method.&amp;nbsp;
This may result in a file descriptor leak.&amp;nbsp; It is generally a good
idea to use a &lt;code&gt;finally&lt;/code&gt; block to ensure that streams are
closed.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UW' category='MT_CORRECTNESS' type='UW_UNCOND_WAIT'><ShortDescription>Unconditional wait</ShortDescription><Details>

  &lt;p&gt; This method contains a call to &lt;code&gt;java.lang.Object.wait()&lt;/code&gt; which
  is not guarded by conditional control flow.&amp;nbsp; The code should
    verify that condition it intends to wait for is not already satisfied
    before calling wait; any previous notifications will be ignored.
  &lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='Nm' category='BAD_PRACTICE' type='NM_CONFUSING'><ShortDescription>Confusing method names</ShortDescription><Details>

  &lt;p&gt; The referenced methods have names that differ only by capitalization.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UC' category='STYLE' type='UC_USELESS_CONDITION'><ShortDescription>Condition has no effect</ShortDescription><Details>

&lt;p&gt;This condition always produces the same result as the value of the involved variable that was narrowed before.
Probably something else was meant or the condition can be removed.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UwF' category='STYLE' type='UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR'><ShortDescription>Field not initialized in constructor but dereferenced without null check</ShortDescription><Details>

  &lt;p&gt; This field is never initialized within any constructor, and is therefore could be null after
the object is constructed. Elsewhere, it is loaded and dereferenced without a null check.
This could be a either an error or a questionable design, since
it means a null pointer exception will be generated if that field is dereferenced
before being initialized.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='DLS' category='STYLE' type='DLS_DEAD_LOCAL_STORE'><ShortDescription>Dead store to local variable</ShortDescription><Details>

&lt;p&gt;
This instruction assigns a value to a local variable,
but the value is not read or used in any subsequent instruction.
Often, this indicates an error, because the value computed is never
used.
&lt;/p&gt;
&lt;p&gt;
Note that Sun's javac compiler often generates dead stores for
final local variables.  Because SpotBugs is a bytecode-based tool,
there is no easy way to eliminate these false positives.
&lt;/p&gt;

    </Details></BugPattern><BugPattern cweid='374' abbrev='EI2' category='MALICIOUS_CODE' type='EI_EXPOSE_REP2'><ShortDescription>May expose internal representation by incorporating reference to mutable object</ShortDescription><Details>

  &lt;p&gt; This code stores a reference to an externally mutable object into the
  internal representation of the object.&amp;nbsp;
   If instances
   are accessed by untrusted code, and unchecked changes to
   the mutable object would compromise security or other
   important properties, you will need to do something different.
  Storing a copy of the object is better approach in many situations.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='WMI' category='PERFORMANCE' type='WMI_WRONG_MAP_ITERATOR'><ShortDescription>Inefficient use of keySet iterator instead of entrySet iterator</ShortDescription><Details>

&lt;p&gt; This method accesses the value of a Map entry, using a key that was retrieved from
a keySet iterator. It is more efficient to use an iterator on the entrySet of the map, to avoid the
Map.get(key) lookup.&lt;/p&gt;

        </Details></BugPattern><BugPattern abbrev='SIC' category='PERFORMANCE' type='SIC_INNER_SHOULD_BE_STATIC_NEEDS_THIS'><ShortDescription>Could be refactored into a static inner class</ShortDescription><Details>

  &lt;p&gt; This class is an inner class, but does not use its embedded reference
  to the object which created it except during construction of the
inner object.&amp;nbsp; This reference makes the instances
  of the class larger, and may keep the reference to the creator object
  alive longer than necessary.&amp;nbsp; If possible, the class should be
  made into a &lt;em&gt;static&lt;/em&gt; inner class. Since the reference to the
   outer object is required during construction of the inner instance,
   the inner class will need to be refactored so as to
   pass a reference to the outer instance to the constructor
   for the inner class.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='MS' category='MALICIOUS_CODE' type='MS_SHOULD_BE_FINAL'><ShortDescription>Field isn't final but should be</ShortDescription><Details>

   &lt;p&gt;
This static field public but not final, and
could be changed by malicious code or
        by accident from another package.
        The field could be made final to avoid
        this vulnerability.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='BC' category='STYLE' type='BC_UNCONFIRMED_CAST'><ShortDescription>Unchecked/unconfirmed cast</ShortDescription><Details>

&lt;p&gt;
This cast is unchecked, and not all instances of the type casted from can be cast to
the type it is being cast to. Check that your program logic ensures that this
cast will not fail.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='Eq' category='BAD_PRACTICE' type='EQ_COMPARETO_USE_OBJECT_EQUALS'><ShortDescription>Class defines compareTo(...) and uses Object.equals()</ShortDescription><Details>

  &lt;p&gt; This class defines a &lt;code&gt;compareTo(...)&lt;/code&gt; method but inherits its
  &lt;code&gt;equals()&lt;/code&gt; method from &lt;code&gt;java.lang.Object&lt;/code&gt;.
    Generally, the value of compareTo should return zero if and only if
    equals returns true. If this is violated, weird and unpredictable
    failures will occur in classes such as PriorityQueue.
    In Java 5 the PriorityQueue.remove method uses the compareTo method,
    while in Java 6 it uses the equals method.&lt;/p&gt;

&lt;p&gt;From the JavaDoc for the compareTo method in the Comparable interface:
&lt;blockquote&gt;
It is strongly recommended, but not strictly required that &lt;code&gt;(x.compareTo(y)==0) == (x.equals(y))&lt;/code&gt;.
Generally speaking, any class that implements the Comparable interface and violates this condition
should clearly indicate this fact. The recommended language
is "Note: this class has a natural ordering that is inconsistent with equals."
&lt;/blockquote&gt;&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='BC' category='STYLE' type='BC_UNCONFIRMED_CAST_OF_RETURN_VALUE'><ShortDescription>Unchecked/unconfirmed cast of return value from method</ShortDescription><Details>

&lt;p&gt;
This code performs an unchecked cast of the return value of a method.
The code might be calling the method in such a way that the cast is guaranteed to be
safe, but SpotBugs is unable to verify that the cast is safe.  Check that your program logic ensures that this
cast will not fail.
&lt;/p&gt;

    </Details></BugPattern><BugPattern cweid='374' abbrev='EI' category='MALICIOUS_CODE' type='EI_EXPOSE_REP'><ShortDescription>May expose internal representation by returning reference to mutable object</ShortDescription><Details>

  &lt;p&gt; Returning a reference to a mutable object value stored in one of the object's fields
  exposes the internal representation of the object.&amp;nbsp;
   If instances
   are accessed by untrusted code, and unchecked changes to
   the mutable object would compromise security or other
   important properties, you will need to do something different.
  Returning a new copy of the object is better approach in many situations.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='CI' category='STYLE' type='CI_CONFUSED_INHERITANCE'><ShortDescription>Class is final but declares protected field</ShortDescription><Details>
      
      &lt;p&gt;
      This class is declared to be final, but declares fields to be protected. Since the class
      is final, it can not be derived from, and the use of protected is confusing. The access
      modifier for the field should be changed to private or public to represent the true
      use for the field.
      &lt;/p&gt;
      
    </Details></BugPattern><BugPattern abbrev='Se' category='BAD_PRACTICE' type='SE_BAD_FIELD'><ShortDescription>Non-transient non-serializable instance field in serializable class</ShortDescription><Details>

&lt;p&gt; This Serializable class defines a non-primitive instance field which is neither transient,
Serializable, or &lt;code&gt;java.lang.Object&lt;/code&gt;, and does not appear to implement
the &lt;code&gt;Externalizable&lt;/code&gt; interface or the
&lt;code&gt;readObject()&lt;/code&gt; and &lt;code&gt;writeObject()&lt;/code&gt; methods.&amp;nbsp;
Objects of this class will not be deserialized correctly if a non-Serializable
object is stored in this field.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UrF' category='STYLE' type='URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD'><ShortDescription>Unread public/protected field</ShortDescription><Details>

  &lt;p&gt; This field is never read.&amp;nbsp;
The field is public or protected, so perhaps
    it is intended to be used with classes not seen as part of the analysis. If not,
consider removing it from the class.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NP' category='CORRECTNESS' type='NP_NULL_ON_SOME_PATH'><ShortDescription>Possible null pointer dereference</ShortDescription><Details>

&lt;p&gt; There is a branch of statement that, &lt;em&gt;if executed,&lt;/em&gt;  guarantees that
a null value will be dereferenced, which
would generate a &lt;code&gt;NullPointerException&lt;/code&gt; when the code is executed.
Of course, the problem might be that the branch or statement is infeasible and that
the null pointer exception can't ever be executed; deciding that is beyond the ability of SpotBugs.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NP' category='STYLE' type='NP_LOAD_OF_KNOWN_NULL_VALUE'><ShortDescription>Load of known null value</ShortDescription><Details>

  &lt;p&gt; The variable referenced at this point is known to be null due to an earlier
   check against null. Although this is valid, it might be a mistake (perhaps you
intended to refer to a different variable, or perhaps the earlier check to see if the
variable is null should have been a check to see if it was non-null).
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='ISC' category='BAD_PRACTICE' type='ISC_INSTANTIATE_STATIC_CLASS'><ShortDescription>Needless instantiation of class that only supplies static methods</ShortDescription><Details>

&lt;p&gt; This class allocates an object that is based on a class that only supplies static methods. This object
does not need to be created, just access the static methods directly using the class name as a qualifier.&lt;/p&gt;

        </Details></BugPattern><BugPattern abbrev='NS' category='STYLE' type='NS_DANGEROUS_NON_SHORT_CIRCUIT'><ShortDescription>Potentially dangerous use of non-short-circuit logic</ShortDescription><Details>

  &lt;p&gt; This code seems to be using non-short-circuit logic (e.g., &amp;amp;
or |)
rather than short-circuit logic (&amp;amp;&amp;amp; or ||). In addition,
it seem possible that, depending on the value of the left hand side, you might not
want to evaluate the right hand side (because it would have side effects, could cause an exception
or could be expensive.&lt;/p&gt;
&lt;p&gt;
Non-short-circuit logic causes both sides of the expression
to be evaluated even when the result can be inferred from
knowing the left-hand side. This can be less efficient and
can result in errors if the left-hand side guards cases
when evaluating the right-hand side can generate an error.
&lt;/p&gt;

&lt;p&gt;See &lt;a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.22.2"&gt;the Java
Language Specification&lt;/a&gt; for details.

&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='UPM' category='PERFORMANCE' type='UPM_UNCALLED_PRIVATE_METHOD'><ShortDescription>Private method is never called</ShortDescription><Details>

&lt;p&gt; This private method is never called. Although it is
possible that the method will be invoked through reflection,
it is more likely that the method is never used, and should be
removed.
&lt;/p&gt;

</Details></BugPattern><BugPattern abbrev='RpC' category='CORRECTNESS' type='RpC_REPEATED_CONDITIONAL_TEST'><ShortDescription>Repeated conditional tests</ShortDescription><Details>

&lt;p&gt;The code contains a conditional test is performed twice, one right after the other
(e.g., &lt;code&gt;x == 0 || x == 0&lt;/code&gt;). Perhaps the second occurrence is intended to be something else
(e.g., &lt;code&gt;x == 0 || y == 0&lt;/code&gt;).
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='Wa' category='MT_CORRECTNESS' type='WA_NOT_IN_LOOP'><ShortDescription>Wait not in loop </ShortDescription><Details>

  &lt;p&gt; This method contains a call to &lt;code&gt;java.lang.Object.wait()&lt;/code&gt;
  which is not in a loop.&amp;nbsp; If the monitor is used for multiple conditions,
  the condition the caller intended to wait for might not be the one
  that actually occurred.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='MS' category='MALICIOUS_CODE' type='MS_MUTABLE_COLLECTION_PKGPROTECT'><ShortDescription>Field is a mutable collection which should be package protected</ShortDescription><Details>

 &lt;p&gt;A mutable collection instance is assigned to a final static field,
   thus can be changed by malicious code or by accident from another package.
   The field could be made package protected to avoid this vulnerability.
   Alternatively you may wrap this field into Collections.unmodifiableSet/List/Map/etc.
   to avoid this vulnerability.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='SA' category='STYLE' type='SA_FIELD_DOUBLE_ASSIGNMENT'><ShortDescription>Double assignment of field</ShortDescription><Details>

&lt;p&gt; This method contains a double assignment of a field; e.g.
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int x,y;
public void foo() {
    x = x = 17;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assigning to a field twice is useless, and may indicate a logic error or typo.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='ME' category='BAD_PRACTICE' type='ME_ENUM_FIELD_SETTER'><ShortDescription>Public enum method unconditionally sets its field</ShortDescription><Details>

  &lt;p&gt;This public method declared in public enum unconditionally sets enum field, thus this field can be changed by malicious code
  or by accident from another package. Though mutable enum fields may be used for lazy initialization, it's a bad practice to expose them to the outer world.
  Consider removing this method or declaring it package-private.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='NS' category='STYLE' type='NS_NON_SHORT_CIRCUIT'><ShortDescription>Questionable use of non-short-circuit logic</ShortDescription><Details>

  &lt;p&gt; This code seems to be using non-short-circuit logic (e.g., &amp;amp;
or |)
rather than short-circuit logic (&amp;amp;&amp;amp; or ||).
Non-short-circuit logic causes both sides of the expression
to be evaluated even when the result can be inferred from
knowing the left-hand side. This can be less efficient and
can result in errors if the left-hand side guards cases
when evaluating the right-hand side can generate an error.

&lt;p&gt;See &lt;a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.22.2"&gt;the Java
Language Specification&lt;/a&gt; for details.

&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='IS' category='MT_CORRECTNESS' type='IS2_INCONSISTENT_SYNC'><ShortDescription>Inconsistent synchronization</ShortDescription><Details>

  &lt;p&gt; The fields of this class appear to be accessed inconsistently with respect
  to synchronization.&amp;nbsp; This bug report indicates that the bug pattern detector
  judged that
  &lt;/p&gt;
  &lt;ul&gt;
  &lt;li&gt; The class contains a mix of locked and unlocked accesses,&lt;/li&gt;
  &lt;li&gt; The class is &lt;b&gt;not&lt;/b&gt; annotated as javax.annotation.concurrent.NotThreadSafe,&lt;/li&gt;
  &lt;li&gt; At least one locked access was performed by one of the class's own methods, and&lt;/li&gt;
  &lt;li&gt; The number of unsynchronized field accesses (reads and writes) was no more than
       one third of all accesses, with writes being weighed twice as high as reads&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt; A typical bug matching this bug pattern is forgetting to synchronize
  one of the methods in a class that is intended to be thread-safe.&lt;/p&gt;

  &lt;p&gt; You can select the nodes labeled "Unsynchronized access" to show the
  code locations where the detector believed that a field was accessed
  without synchronization.&lt;/p&gt;

  &lt;p&gt; Note that there are various sources of inaccuracy in this detector;
  for example, the detector cannot statically detect all situations in which
  a lock is held.&amp;nbsp; Also, even when the detector is accurate in
  distinguishing locked vs. unlocked accesses, the code in question may still
  be correct.&lt;/p&gt;


    </Details></BugPattern><BugPattern cweid='382' abbrev='Dm' category='BAD_PRACTICE' type='DM_EXIT'><ShortDescription>Method invokes System.exit(...)</ShortDescription><Details>

  &lt;p&gt; Invoking System.exit shuts down the entire Java virtual machine. This
   should only been done when it is appropriate. Such calls make it
   hard or impossible for your code to be invoked by other code.
   Consider throwing a RuntimeException instead.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='SnVI' category='BAD_PRACTICE' type='SE_NO_SERIALVERSIONID'><ShortDescription>Class is Serializable, but doesn't define serialVersionUID</ShortDescription><Details>

  &lt;p&gt; This class implements the &lt;code&gt;Serializable&lt;/code&gt; interface, but does
  not define a &lt;code&gt;serialVersionUID&lt;/code&gt; field.&amp;nbsp;
  A change as simple as adding a reference to a .class object
    will add synthetic fields to the class,
   which will unfortunately change the implicit
   serialVersionUID (e.g., adding a reference to &lt;code&gt;String.class&lt;/code&gt;
   will generate a static field &lt;code&gt;class$java$lang$String&lt;/code&gt;).
   Also, different source code to bytecode compilers may use different
   naming conventions for synthetic variables generated for
   references to class objects or inner classes.
   To ensure interoperability of Serializable across versions,
   consider adding an explicit serialVersionUID.&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='HE' category='BAD_PRACTICE' type='HE_HASHCODE_USE_OBJECT_EQUALS'><ShortDescription>Class defines hashCode() and uses Object.equals()</ShortDescription><Details>

  &lt;p&gt; This class defines a &lt;code&gt;hashCode()&lt;/code&gt; method but inherits its
  &lt;code&gt;equals()&lt;/code&gt; method from &lt;code&gt;java.lang.Object&lt;/code&gt;
  (which defines equality by comparing object references).&amp;nbsp; Although
  this will probably satisfy the contract that equal objects must have
  equal hashcodes, it is probably not what was intended by overriding
  the &lt;code&gt;hashCode()&lt;/code&gt; method.&amp;nbsp; (Overriding &lt;code&gt;hashCode()&lt;/code&gt;
  implies that the object's identity is based on criteria more complicated
  than simple reference equality.)&lt;/p&gt;
&lt;p&gt;If you don't think instances of this class will ever be inserted into a HashMap/HashTable,
the recommended &lt;code&gt;hashCode&lt;/code&gt; implementation to use is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public int hashCode() {
    assert false : "hashCode not designed";
    return 42; // any arbitrary constant will do
}
&lt;/code&gt;&lt;/pre&gt;

    </Details></BugPattern><BugPattern abbrev='OS' category='BAD_PRACTICE' type='OS_OPEN_STREAM'><ShortDescription>Method may fail to close stream</ShortDescription><Details>

&lt;p&gt; The method creates an IO stream object, does not assign it to any
fields, pass it to other methods that might close it,
or return it, and does not appear to close
the stream on all paths out of the method.&amp;nbsp; This may result in
a file descriptor leak.&amp;nbsp; It is generally a good
idea to use a &lt;code&gt;finally&lt;/code&gt; block to ensure that streams are
closed.&lt;/p&gt;

    </Details></BugPattern><BugPattern cweid='253' abbrev='RV' category='BAD_PRACTICE' type='RV_RETURN_VALUE_IGNORED_BAD_PRACTICE'><ShortDescription>Method ignores exceptional return value</ShortDescription><Details>

   &lt;p&gt; This method returns a value that is not checked. The return value should be checked
since it can indicate an unusual or unexpected function execution. For
example, the &lt;code&gt;File.delete()&lt;/code&gt; method returns false
if the file could not be successfully deleted (rather than
throwing an Exception).
If you don't check the result, you won't notice if the method invocation
signals unexpected behavior by returning an atypical return value.
&lt;/p&gt;

    </Details></BugPattern><BugPattern abbrev='Dm' category='I18N' type='DM_CONVERT_CASE'><ShortDescription>Consider using Locale parameterized version of invoked method</ShortDescription><Details>

  &lt;p&gt; A String is being converted to upper or lowercase, using the platform's default encoding. This may
      result in improper conversions when used with international characters. Use the &lt;/p&gt;
      &lt;ul&gt;
    &lt;li&gt;String.toUpperCase( Locale l )&lt;/li&gt;
    &lt;li&gt;String.toLowerCase( Locale l )&lt;/li&gt;
    &lt;/ul&gt;
      &lt;p&gt;versions instead.&lt;/p&gt;

    </Details></BugPattern><BugCode abbrev='RR'><Description>Method ignores results of InputStream.read()</Description></BugCode><BugCode abbrev='BC'><Description>Bad casts of object references</Description></BugCode><BugCode cweid='476' abbrev='NP'><Description>Null pointer dereference</Description></BugCode><BugCode cweid='440' abbrev='RV'><Description>Bad use of return value from method</Description></BugCode><BugCode abbrev='UwF'><Description>Unwritten field</Description></BugCode><BugCode abbrev='NS'><Description>Suspicious use of non-short-circuit boolean operator</Description></BugCode><BugCode abbrev='SnVI'><Description>Serializable class with no Version ID</Description></BugCode><BugCode cweid='563' abbrev='DLS'><Description>Dead local store</Description></BugCode><BugCode abbrev='It'><Description>Incorrect definition of Iterator</Description></BugCode><BugCode abbrev='Eq'><Description>Problems with implementation of equals()</Description></BugCode><BugCode abbrev='SA'><Description>Useless self-operation</Description></BugCode><BugCode abbrev='UC'><Description>Useless code</Description></BugCode><BugCode abbrev='EI2'><Description>Storing reference to mutable object</Description></BugCode><BugCode abbrev='UPM'><Description>Private method is never called</Description></BugCode><BugCode abbrev='ISC'><Description>Instantiated Static Class</Description></BugCode><BugCode abbrev='ME'><Description>Mutable enum field</Description></BugCode><BugCode abbrev='PZLA'><Description>Prefer zero length arrays to null to indicate no results</Description></BugCode><BugCode abbrev='Nm'><Description>Confusing method name</Description></BugCode><BugCode abbrev='ST'><Description>Misuse of static fields</Description></BugCode><BugCode abbrev='No'><Description>Using notify() rather than notifyAll()</Description></BugCode><BugCode abbrev='UW'><Description>Unconditional wait</Description></BugCode><BugCode abbrev='EI'><Description>Method returning array may expose internal representation</Description></BugCode><BugCode abbrev='OS'><Description>Stream not closed on all paths</Description></BugCode><BugCode abbrev='RpC'><Description>Repeated conditional test</Description></BugCode><BugCode cweid='218' abbrev='MS'><Description>Mutable static field</Description></BugCode><BugCode abbrev='CI'><Description>Confused Inheritance</Description></BugCode><BugCode abbrev='UrF'><Description>Unread field</Description></BugCode><BugCode abbrev='Dm'><Description>Dubious method used</Description></BugCode><BugCode cweid='366' abbrev='IS'><Description>Inconsistent synchronization</Description></BugCode><BugCode abbrev='Wa'><Description>Wait not in loop</Description></BugCode><BugCode abbrev='SIC'><Description>Inner class could be made static</Description></BugCode><BugCode cweid='192' abbrev='ICAST'><Description>Casting from integer values</Description></BugCode><BugCode abbrev='REC'><Description>RuntimeException capture</Description></BugCode><BugCode abbrev='Se'><Description>Incorrect definition of Serializable class</Description></BugCode><BugCode abbrev='WMI'><Description>Inefficient Map Iterator</Description></BugCode><BugCode abbrev='BSHIFT'><Description>Bad shift</Description></BugCode><BugCode abbrev='UCF'><Description>Useless control flow</Description></BugCode><BugCode abbrev='VO'><Description>Use of volatile</Description></BugCode><BugCode cweid='476' abbrev='RCN'><Description>Redundant comparison to null</Description></BugCode><BugCode abbrev='HE'><Description>Equal objects must have equal hashcodes</Description></BugCode><Errors missingClasses='0' errors='0'></Errors><FindBugsSummary num_packages='51' total_classes='2207' priority_1='185' priority_2='358' priority_3='420' total_size='173490' clock_seconds='76.78' referenced_classes='4564' vm_version='25.222-b10' total_bugs='963' java_version='1.8.0_222' gc_seconds='22.49' alloc_mbytes='455.50' cpu_seconds='337.93' peak_mbytes='604.47' timestamp='Wed, 11 Sep 2019 10:35:30 +0200'><FileStats path='org/apache/hadoop/hdfs/DFSConfigKeys.java' size='852' bugHash='a900ccc68eed4b6c4f1997951625f3ee' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/DFSUtil.java' size='655' bugHash='e1ee5642866e5ddc0096d4cd386e1a74' bugCount='7'></FileStats><FileStats path='org/apache/hadoop/hdfs/DeprecatedUTF8.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/HAUtil.java' size='110' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/HDFSPolicyProvider.java' size='8' bugHash='825a3ec7c43f72ecac3415e09bc864fc' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/HdfsDtFetcher.java' size='22' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/NameNodeProxies.java' size='72' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/SWebHdfsDtFetcher.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/WebHdfsDtFetcher.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/net/DFSNetworkTopology.java' size='137' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java' size='222' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/net/DomainPeerServer.java' size='40' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/net/PeerServer.java' size='6' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/net/TcpPeerServer.java' size='37' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/BlockListAsLongs.java' size='304' bugHash='eab81cbf3794811882bd8ffd080c806e' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/CacheDirective.java' size='143' bugHash='27bd4f19fd01873338b0bb1ebfad6e7d' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/FSLimitException.java' size='41' bugHash='3d415db63c3d6406d0ec8b22b2d3484a' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/LayoutFlags.java' size='14' bugHash='620d5821202abf67564b1f6969a1ae10' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/LayoutVersion.java' size='191' bugHash='b3f8281ae09b3e26a22c8037ed4e21d5' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/RecoveryInProgressException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/RollingUpgradeException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/SnapshotException.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/SnapshotInfo.java' size='46' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/UnregisteredNodeException.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java' size='44' bugHash='50cbe7599ddb918b6aff729b43fbedbe' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java' size='202' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.java' size='39' bugHash='41d302ae185cdba251381f796ddcd276' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/package-info.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/sasl/InvalidMagicNumberException.java' size='10' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java' size='155' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/AliasMapProtocolProtos.java' size='3446' bugHash='426fcb08fc11c95b880daa70fb2af0f0' bugCount='32'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/DatanodeLifelineProtocolProtos.java' size='360' bugHash='7b2f8f6e081165971a96f5c531df0a17' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/DatanodeProtocolProtos.java' size='17937' bugHash='510ae490285d0153d76e66d6cadbd744' bugCount='101'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/EditLogProtos.java' size='1073' bugHash='12d3430010e6d1e794018b3ac5ec27a2' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/HdfsServerProtos.java' size='8382' bugHash='8b9f8954f5d5960ed448c703495e9191' bugCount='45'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/InterDatanodeProtocolProtos.java' size='1809' bugHash='16949d75e7797bdfaf51313413f8adc8' bugCount='14'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/JournalProtocolProtos.java' size='2992' bugHash='3648040d694958b6d699f37209738451' bugCount='27'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocol/proto/NamenodeProtocolProtos.java' size='8089' bugHash='f237a5f5f423600709895286b0930264' bugCount='87'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/AliasMapProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/AliasMapProtocolServerSideTranslatorPB.java' size='51' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java' size='149' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java' size='842' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolClientSideTranslatorPB.java' size='38' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolServerSideTranslatorPB.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java' size='156' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java' size='158' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java' size='74' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolServerSideTranslatorPB.java' size='29' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java' size='39' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/JournalProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/JournalProtocolServerSideTranslatorPB.java' size='34' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/JournalProtocolTranslatorPB.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java' size='108' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolTranslatorPB.java' size='111' bugHash='97610fcd6d458445e175e91ce268b599' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/PBHelper.java' size='658' bugHash='0fab5e163cf35682742691eec1a75b54' bugCount='7'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideTranslatorPB.java' size='26' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideUtils.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java' size='26' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java' size='197' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java' size='469' bugHash='df1c82e59ce7d29b9678489b4c81f48a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannelMetrics.java' size='67' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/LoggerTooFarBehindException.java' size='4' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/QuorumCall.java' size='146' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/QuorumException.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java' size='326' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/QuorumOutputStream.java' size='63' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/client/SegmentRecoveryComparator.java' size='29' bugHash='92a02d8b972aa194317783651e9d6893' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocol.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.java' size='124' bugHash='54f583abc89523382ff15b79b33f9d69' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/JournalNotFormattedException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/JournalOutOfSyncException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java' size='21' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.java' size='17101' bugHash='7a06cf7a4d6a26098cfd6e1c84f3aceb' bugCount='144'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocol/RequestInfo.java' size='32' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolServerSideTranslatorPB.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolTranslatorPB.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolPB.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java' size='156' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java' size='221' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java' size='125' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JNStorage.java' size='120' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/Journal.java' size='509' bugHash='112c6cdab5aa15b33f62ced8782eddba' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector.java' size='12' bugHash='65d858a047cbe525be54badb910abed8' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalMetrics.java' size='51' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalNode.java' size='245' bugHash='c09521a3dc23c282e05cc0a863d02f63' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalNodeHttpServer.java' size='66' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalNodeMXBean.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java' size='132' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java' size='315' bugHash='48f1453d93b7d387b1cab1336403ae37' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/security/token/block/BlockKey.java' size='10' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/security/token/block/BlockPoolTokenSecretManager.java' size='44' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java' size='230' bugHash='7070568b5ee74dd6618b7ad4d11d866d' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java' size='56' bugHash='5fc1ffa560b99894538432168ef8d1d4' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java' size='280' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java' size='103' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMapProtocol.java' size='17' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java' size='64' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/Balancer.java' size='443' bugHash='2f5d7f560cd66f995df96be8647ac75b' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/BalancerParameters.java' size='106' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/BalancingPolicy.java' size='80' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/Dispatcher.java' size='841' bugHash='e626365330718ab7de1d0c5de26a9052' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/ExitStatus.java' size='31' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/KeyManager.java' size='102' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/Matcher.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/MovedBlocks.java' size='55' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java' size='125' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java' size='50' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockCollection.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java' size='91' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java' size='137' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java' size='45' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java' size='156' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java' size='2543' bugHash='26dfd41fd0ad121064131768266590c3' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.java' size='14' bugHash='d8a20e33701615eb1ee4f00f5d29f767' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java' size='329' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicies.java' size='23' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java' size='56' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java' size='540' bugHash='fe8fbfcf2b47a7539c090840401d5408' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java' size='103' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java' size='167' bugHash='44285cf9df5e1a7b6dccdbbb8c57f6e9' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithUpgradeDomain.java' size='99' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatus.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusDefault.java' size='18' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusWithNodeGroup.java' size='27' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusWithUpgradeDomain.java' size='31' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockReconstructionWork.java' size='59' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java' size='194' bugHash='cface0ff31f989eddd0a759dab2f7f05' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockStatsMXBean.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java' size='57' bugHash='06ea4ead46f9b4d70f741e838d6e232f' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockToMarkCorrupt.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java' size='180' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java' size='113' bugHash='aa491b99465504e786cf909654161e50' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java' size='433' bugHash='865af911cea2820f4e71445624665f4a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java' size='157' bugHash='49320753ad038cf784e5a3f1920fc09d' bugCount='10'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java' size='120' bugHash='1d7471712efe3e0f7bec9f61d7fd42d0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java' size='329' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java' size='563' bugHash='af247e339cbd388cb119f4e7bfd18ff4' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java' size='892' bugHash='631d2f8cd735a40cb664e1ad993d1d80' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStatistics.java' size='18' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java' size='151' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java' size='221' bugHash='6930222aa8bcab34a7cb8ea7b5eeb896' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/ErasureCodingWork.java' size='91' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/ExcessRedundancyMap.java' size='43' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/FSClusterStats.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java' size='231' bugHash='124fde498a16e848a903617ecc31b8ad' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/Host2NodesMap.java' size='112' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/HostConfigManager.java' size='10' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.java' size='69' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java' size='53' bugHash='5b7c64816aba132d7fc9af6957a790f3' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java' size='156' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/LocatedBlockBuilder.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java' size='207' bugHash='eb7ce75241ce297746ceb7facdedbc70' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.java' size='57' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/OutOfLegacyGenerationStampsException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java' size='75' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java' size='172' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/PendingRecoveryBlocks.java' size='64' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java' size='272' bugHash='e057043f65fbee789a96af98fac37cbf' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/ReplicaUnderConstruction.java' size='39' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/ReplicationWork.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/SequentialBlockGroupIdGenerator.java' size='22' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/SequentialBlockIdGenerator.java' size='17' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java' size='146' bugHash='a6e163307ebbae9dab9477d4ee03506b' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java' size='96' bugHash='61a5bbe3c3dc5542d3f6b9d511c77e46' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/StorageTypeStats.java' size='89' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/blockmanagement/UnresolvedTopologyException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/BlockAlias.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/FileRegion.java' size='28' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/GenerationStamp.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/HdfsServerConstants.java' size='249' bugHash='122a2d7cc73efcf3fff57784231b2232' bugCount='8'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/HttpGetFailedException.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/HttpPutFailedException.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/InconsistentFSStateException.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/IncorrectVersionException.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/JspHelper.java' size='99' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.java' size='77' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/Storage.java' size='608' bugHash='567f43857f4e4bc7d55eeefe23f48ffb' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/StorageErrorReporter.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/StorageInfo.java' size='122' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/Util.java' size='195' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java' size='23' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java' size='87' bugHash='f24b5e938c837629a45aa6a6e548733f' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java' size='131' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java' size='281' bugHash='ab64c2d87222c7c8d325a9a92455e31a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/common/blockaliasmap/package-info.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BPOfferService.java' size='405' bugHash='88881fe5f1d09ef02a4e776898696795' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java' size='701' bugHash='1eeb07f7fe3afdd009b66e8d7eba5ceb' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BPServiceActorAction.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BPServiceActorActionException.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java' size='395' bugHash='c2d1c8185c21d22535d68b11f0c17b75' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java' size='182' bugHash='0fa12d4460e8b17168b3bc62ad3fb45b' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java' size='424' bugHash='8dabbe165123cdbe94007aa72833171f' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java' size='874' bugHash='0a4c040e9d7f0cb0b894f2c0f2766ba4' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java' size='356' bugHash='da579c20f2dff2a15b943539bbbdd722' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockScanner.java' size='165' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/BlockSender.java' size='378' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.java' size='12' bugHash='1ba631c0fcb0331657b7c2a9d1559845' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DNConf.java' size='144' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataNode.java' size='1723' bugHash='e3c607081e4dd70c7821c3e26d4904ce' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataNodeFaultInjector.java' size='41' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.java' size='37' bugHash='6d2bbf28b25b326cde58b10320b3a79e' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataNodeMXBean.java' size='17' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataStorage.java' size='732' bugHash='e172e7b2456de116087ccabd733af32d' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataXceiver.java' size='788' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java' size='164' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java' size='45' bugHash='fcb541052cf04a5c52850e4d2d94cfb5' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java' size='324' bugHash='60c3045d796ca3ad2ac276c156a0786d' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java' size='512' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ErrorReportAction.java' size='37' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/FaultInjectorFileIoEvents.java' size='12' bugHash='bef6c4f4911bef45c14168bbc8441355' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java' size='493' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/FinalizedProvidedReplica.java' size='35' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java' size='50' bugHash='de219501d38a33f14bfae144e28ba90e' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java' size='144' bugHash='ffa58d3b46d01fe7898e32b97aa67852' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/LocalReplica.java' size='226' bugHash='29fc08520e8a80d8e2ec5db7e1b9a8ae' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java' size='183' bugHash='3dd494b70d9b048145e40ca69245a64a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ProfilingFileIoEvents.java' size='68' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ProvidedReplica.java' size='136' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/Replica.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaAlreadyExistsException.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java' size='21' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java' size='202' bugHash='f44b18b0c8c9be747b4754cd48641214' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaHandler.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java' size='73' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java' size='62' bugHash='90f99f0e9aeef2b3ef7abf63d93bbdc0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java' size='30' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ReportBadBlockAction.java' size='53' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.java' size='93' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java' size='199' bugHash='ecfe94618a6dccca5342e796487bf14a' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/StorageLocation.java' size='112' bugHash='83840e2ceb6cad1c6a051b7bcac562b4' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/UnexpectedReplicaStateException.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java' size='429' bugHash='488da43ede00759ab60fac04b9f3c6db' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java' size='533' bugHash='7cbac3cb530c5380af602797495ce85c' bugCount='9'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/AsyncChecker.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/Checkable.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java' size='192' bugHash='fa2edd3ad0d8f0c1cff23b319b472f74' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java' size='85' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java' size='107' bugHash='3548d774910e66888522c75b892cb879' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/TimeoutFuture.java' size='46' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/VolumeCheckResult.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/checker/package-info.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java' size='80' bugHash='1d152c574e0bc9fa6c21867d625329ef' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java' size='27' bugHash='71b971dd25223aee514311c2b1bba609' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumMd5CrcReconstructor.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java' size='107' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReader.java' size='114' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java' size='66' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockWriter.java' size='120' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReader.java' size='251' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java' size='44' bugHash='26f751490ba2f4b3a149d83493dc6458' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java' size='111' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedWriter.java' size='178' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/erasurecode/package-info.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java' size='146' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/DataNodeVolumeMetrics.java' size='138' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java' size='119' bugHash='3d31beaee6373a0f4402285cf72e89e6' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeReference.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java' size='116' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/LengthInputStream.java' size='10' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaInputStreams.java' size='85' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.java' size='70' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.java' size='38' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/VolumeChoosingPolicy.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java' size='444' bugHash='21914f9636cb87e92bef8db8e20c4c2b' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java' size='197' bugHash='6a5f0a6e8444de3851febb44a7551f56' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java' size='319' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetFactory.java' size='6' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java' size='1658' bugHash='b738ea07ded06c919f859ebd158bd15f' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java' size='87' bugHash='fd84d2f34b23c2e7ae509ce864737405' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java' size='809' bugHash='12db6f41b6eda1ac0e90793dea6a5cf7' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImplBuilder.java' size='39' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java' size='265' bugHash='9feeda3d0a4df744d060de96f8786cc1' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java' size='81' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java' size='364' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java' size='127' bugHash='a11a2f0a50d63ea60f0ca370b19463fe' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java' size='106' bugHash='3e47fe6f17f59addcce212460c29c308' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java' size='107' bugHash='dd351b87df83f1eac101301b6703d4d0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaMap.java' size='106' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java' size='86' bugHash='4e3cecbc7d36a9a39fbfa058f131914e' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/VolumeFailureInfo.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java' size='113' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetricHelper.java' size='34' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java' size='325' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/DataNodePeerMetrics.java' size='40' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java' size='15' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/metrics/OutlierDetector.java' size='52' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java' size='206' bugHash='5167f9c633150dfd23269203751edcb0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/RestCsrfPreventionFilterHandler.java' size='44' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java' size='109' bugHash='98e8cea9d23731bb6819fd4e28228dcd' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/URLDispatcher.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/DataNodeUGIProvider.java' size='82' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ExceptionHandler.java' size='50' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/HdfsWriter.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java' size='75' bugHash='2943bc5e90013ff81ec10abfc54536d9' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java' size='201' bugHash='a4d5c5d8f89c25799341a8c981811b53' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerConstants.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.java' size='55' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java' size='55' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java' size='229' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java' size='43' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java' size='39' bugHash='bf6f67bb29723d5174d546acbf8e8a15' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java' size='125' bugHash='fe5030b9e9345d5f3486144e060baa50' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java' size='40' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java' size='109' bugHash='7446e9b30a23889e71d5431d20b539a5' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/connectors/ClusterConnector.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java' size='16' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java' size='56' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/connectors/JsonNodeConnector.java' size='25' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java' size='133' bugHash='5e47a08391800fe83862ed315730388e' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java' size='92' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolume.java' size='103' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java' size='126' bugHash='5daed6bba3024701c74732d2233d18a0' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java' size='120' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/MoveStep.java' size='68' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/NodePlan.java' size='60' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/Planner.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/PlannerFactory.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/diskbalancer/planner/Step.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/mover/Mover.java' size='528' bugHash='95940d924372e0dfce9295c0f970d99a' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/AclEntryStatusFormat.java' size='76' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/AclFeature.java' size='31' bugHash='9cad3aa4674a11a58a1b8fab6ea88d41' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/AclStorage.java' size='133' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/AclTransformation.java' size='213' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/AuditLogger.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/BackupImage.java' size='175' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/BackupNode.java' size='239' bugHash='7f800b99eed1bef9b6f4fe1e75575a7e' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/BackupState.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CacheManager.java' size='696' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CachePool.java' size='163' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CachedBlock.java' size='105' bugHash='74aa491357f92db6906154662f0f77ea' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CheckableNameNodeResource.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CheckpointConf.java' size='38' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.java' size='31' bugHash='bf307249ba084310e40cd19859ec2523' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/CheckpointSignature.java' size='87' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/Checkpointer.java' size='160' bugHash='d7f0fa356c636ad759fd9a2b0c516b78' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/Content.java' size='21' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ContentCounts.java' size='73' bugHash='dbdef30bca8040b78bf2ae74240f8456' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java' size='116' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java' size='12' bugHash='ef293d1d27ad49fe547f55b6b4445ec4' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/DfsServlet.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java' size='138' bugHash='64e4ce9be5be9525e59adf683deaad9f' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogBackupInputStream.java' size='70' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java' size='72' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java' size='236' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java' size='122' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogInputException.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java' size='48' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java' size='32' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java' size='100' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.java' size='22' bugHash='c4e42fad3205ef8fa20c9dc29fcc7e64' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java' size='396' bugHash='be16f576f4a899cb9654c520e6326c96' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java' size='180' bugHash='894035064c7f32b3003843f73155c14d' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirAclOp.java' size='131' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirAppendOp.java' size='106' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java' size='243' bugHash='436fb2b2bc7d807dc8409031dcf42220' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java' size='130' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirDeleteOp.java' size='120' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java' size='351' bugHash='feb9ee78626b3977247e8e44282ff850' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java' size='181' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirMkdirOp.java' size='88' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirRenameOp.java' size='404' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java' size='144' bugHash='f33377f3198e65a8e33d477d78917442' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java' size='296' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirSymlinkOp.java' size='56' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java' size='151' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirWriteFileOp.java' size='416' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java' size='220' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSDirectory.java' size='930' bugHash='4e54ea7d0cf44c4c1181b22f992de060' bugCount='6'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSEditLog.java' size='951' bugHash='6226c0ee6f822e8e319218170eb646ee' bugCount='5'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSEditLogAsync.java' size='180' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java' size='708' bugHash='a3775a23b1c0894756fb516ca8685cd9' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java' size='3492' bugHash='a7dc4f00f0572b5fc65e804812b71821' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSEditLogOpCodes.java' size='142' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImage.java' size='741' bugHash='968f35ad2d0c642f1be29541a0a49ffe' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageCompression.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java' size='672' bugHash='0255580a11fefaaf761a1c5486a5fffc' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java' size='469' bugHash='fc8b06a99acd3a8725feb85583672a8b' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java' size='450' bugHash='0716be950dd21a1c2a7a1c3497293398' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java' size='131' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageSerialization.java' size='462' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageStorageInspector.java' size='29' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java' size='84' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSImageUtil.java' size='42' bugHash='99a4131d98ff4e6acd4af9c88609de58' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSNDNCacheOp.java' size='50' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java' size='4133' bugHash='ebc850eb04d30b88cb629de9fd78da4b' bugCount='11'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java' size='156' bugHash='47aba5bc9b51c7c8317b2a30b50a0857' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java' size='306' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java' size='125' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java' size='352' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FileUnderConstructionFeature.java' size='35' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FsImageProto.java' size='14440' bugHash='cb2a24f325d6e2a8e2b4c31b34ae0ac2' bugCount='104'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/FsckServlet.java' size='36' bugHash='02fd1c62be0f334fe3156bc972f14594' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/HdfsAuditLogger.java' size='10' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INode.java' size='405' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java' size='36' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java' size='61' bugHash='1a99ad204a829def710ffa1ad28cada3' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java' size='442' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.java' size='36' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeFile.java' size='592' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeFileAttributes.java' size='44' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeId.java' size='7' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeMap.java' size='47' bugHash='eccfd2559cbbaa81416797bf4dc08e8c' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeReference.java' size='345' bugHash='657c6a05fd99bb01ce71c341ceb68d92' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java' size='55' bugHash='156e85727a8d89303b500343e2da7422' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java' size='213' bugHash='964e3a812cf6737bb6209d4c9085fc91' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/INodesInPath.java' size='231' bugHash='dc1f9dd64a9f6441944db4b6db5329df' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/IllegalReservedPathException.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ImageServlet.java' size='378' bugHash='e8f39c61857521a9d75745005df7c3bc' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java' size='121' bugHash='4ffb52f65f436fe1d2f187285a079f1d' bugCount='17'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/JournalManager.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/JournalSet.java' size='432' bugHash='ffa8b43be99f9ee3df9acefb72a10950' bugCount='6'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/LeaseExpiredException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/LeaseManager.java' size='380' bugHash='2f72d2154d6e5657fc92f81b4ed273b4' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/LogsPurgeable.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java' size='60' bugHash='6c8012d59e612d78eda5852c6feadf9e' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NNStorage.java' size='543' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java' size='144' bugHash='b852522051d3ba1b8f79093de8b7f958' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java' size='87' bugHash='e094761e6e92e98d5bd88a3db3de7bc7' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameCache.java' size='68' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNode.java' size='1044' bugHash='36a99c22f8c2e4722549b3fe23ef5ed8' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeFormatException.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java' size='137' bugHash='5434bc06bdf9630209c2eab5eae19006' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.java' size='53' bugHash='8215f2507d4dc22d2d2b405571e8c389' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeMXBean.java' size='38' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java' size='91' bugHash='53e9032eea373e6c93a93303192a3ee2' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeResourcePolicy.java' size='21' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java' size='1312' bugHash='c8ec6dd157ee01325a5c917fd0bb19c6' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeStatusMXBean.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NameNodeUtils.java' size='34' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java' size='932' bugHash='1ca0285627846d7d3bfe09fe061cdc3c' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/Namesystem.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/Quota.java' size='26' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.java' size='46' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java' size='109' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java' size='147' bugHash='cb95269c0430aa150d335f56feac01c5' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java' size='485' bugHash='0591d27dc6ba5034a7e6127a1a84960c' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java' size='349' bugHash='431d8647a0830aff060b24fc99f78fc2' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SafeMode.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SaveNamespaceCancelledException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SaveNamespaceContext.java' size='33' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java' size='592' bugHash='58fc8401a72898b070bdf26177b6de99' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SecondaryNameNodeInfoMXBean.java' size='7' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.java' size='20' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/SerialNumberMap.java' size='31' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/StartupProgressServlet.java' size='65' bugHash='ac8d5694973ed4aca521e3c91fe2fe1b' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/StoragePolicySummary.java' size='137' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/StreamLimiter.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java' size='219' bugHash='5505277684d5888bfcbf7829b774c387' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/UnsupportedActionException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/VersionInfoMXBean.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/XAttrFeature.java' size='40' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/XAttrFormat.java' size='79' bugHash='2c9048a04b6ac3e82a246da62d6c8565' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/XAttrPermissionFilter.java' size='46' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/XAttrStorage.java' size='23' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/ActiveState.java' size='26' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java' size='221' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java' size='280' bugHash='7a66d7e295b570842067e5de76d965f8' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/HAContext.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/HAState.java' size='39' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/NameNodeHAProxyFactory.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.java' size='56' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java' size='272' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/StandbyState.java' size='35' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.java' size='599' bugHash='99b910e95d0509594e9544d97c64591f' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/metrics/ECBlockGroupsMBean.java' size='7' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/metrics/FSNamesystemMBean.java' size='35' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java' size='282' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/metrics/ReplicatedBlocksMBean.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiff.java' size='43' bugHash='6b3e6764fe353ace5a3bbb8525e9bc2c' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java' size='144' bugHash='da0d4bb33d690df1994e606c5dc6802e' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffList.java' size='38' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListByArrayList.java' size='28' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java' size='256' bugHash='c4a17422d4feffdf0e952e29cef827c3' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryDiffListFactory.java' size='29' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java' size='286' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java' size='420' bugHash='b7347737f3ba0008f6c7a0246e6254a8' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java' size='396' bugHash='cf35b801f751d80079aedb69f4483da8' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java' size='56' bugHash='39afa3c1cf5d8ce10948d70cdb1c736f' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java' size='81' bugHash='ed24a68b9c1ebee670a41486fed5d061' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java' size='119' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot.java' size='107' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffInfo.java' size='126' bugHash='9cca433fc0ae8c5b0a6ed1b2dbff61be' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffListingInfo.java' size='105' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.java' size='140' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java' size='240' bugHash='7445045612b5d4b5dcb734f9da8f94b5' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotStatsMXBean.java' size='3' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/AbstractTracking.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/Phase.java' size='25' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/PhaseTracking.java' size='19' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java' size='81' bugHash='3014ac5d59348c5c042e2016009b702b' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgressMetrics.java' size='42' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgressView.java' size='90' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/Status.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/Step.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/StepTracking.java' size='15' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/StepType.java' size='31' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/startupprogress/package-info.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/top/TopAuditLogger.java' size='33' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/top/TopConf.java' size='15' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/top/metrics/TopMetrics.java' size='71' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindow.java' size='62' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java' size='135' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java' size='606' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BalancerBandwidthCommand.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlockCommand.java' size='49' bugHash='f926850766de28285e7639af6cad8f50' bugCount='8'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java' size='59' bugHash='67869124ee4c3ce2fcac24bb54297092' bugCount='9'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java' size='12' bugHash='e23711d056365fbe575a9455a8f0c359' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java' size='60' bugHash='81077ebf275ecb8c26f7fc2a42e811e7' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlockReportContext.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java' size='60' bugHash='9b6260b9dc4d1e8c48a078bcad437e05' bugCount='10'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/CheckpointCommand.java' size='15' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java' size='4' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/DatanodeLifelineProtocol.java' size='2' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java' size='27' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/DatanodeRegistration.java' size='48' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/DisallowedDatanodeException.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/FenceResponse.java' size='16' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/FencedException.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/FinalizeCommand.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/HeartbeatResponse.java' size='20' bugHash='cca0ba3b260453d68f156d51b5061e83' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/InterDatanodeProtocol.java' size='7' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/JournalInfo.java' size='28' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/JournalProtocol.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/KeyUpdateCommand.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NNHAStatusHeartbeat.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NamenodeCommand.java' size='4' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java' size='20' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NamenodeProtocols.java' size='1' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NamenodeRegistration.java' size='25' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NamespaceInfo.java' size='106' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/NodeRegistration.java' size='5' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo.java' size='67' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/RegisterCommand.java' size='7' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/RemoteEditLog.java' size='55' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/RemoteEditLogManifest.java' size='32' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java' size='16' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/ServerCommand.java' size='14' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/StorageBlockReport.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java' size='21' bugHash='c21009d2d1f9fa02e49504dd275a2fe4' bugCount='3'></FileStats><FileStats path='org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java' size='16' bugHash='5230696c547c6ce490e15517af1b3d0f' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/AdminHelper.java' size='99' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/CacheAdmin.java' size='616' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/CryptoAdmin.java' size='253' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DFSAdmin.java' size='1447' bugHash='b29b018b0516ddb7a8be0d0a7016ad5d' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DFSHAAdmin.java' size='56' bugHash='e11a916efe2432663c6c29f59fd50b22' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java' size='135' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DFSck.java' size='206' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DebugAdmin.java' size='289' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java' size='144' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/DiskBalancerCLI.java' size='209' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/ECAdmin.java' size='332' bugHash='61014d89c3e7d18c6dadc0dce6d6c668' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/GetConf.java' size='202' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/GetGroups.java' size='32' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/HDFSConcat.java' size='18' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/JMXGet.java' size='177' bugHash='1a9077899c8afa075bda20ae6b9d555b' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java' size='83' bugHash='c4d6c7e939d47a040cae3af445796fef' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java' size='163' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/BinaryEditsVisitor.java' size='17' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java' size='52' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsLoader.java' size='18' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsViewer.java' size='115' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsVisitor.java' size='4' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsVisitorFactory.java' size='24' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java' size='163' bugHash='97e3895b7d8d35317af5519342177b65' bugCount='7'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/StatisticsEditsVisitor.java' size='41' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java' size='26' bugHash='4f6248905399263b1fd884c1af3b0906' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java' size='59' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/DelimitedImageVisitor.java' size='69' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/DepthCounter.java' size='11' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java' size='84' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java' size='400' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java' size='84' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionVisitor.java' size='116' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/IgnoreSnapshotException.java' size='4' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoader.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java' size='399' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageVisitor.java' size='225' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/IndentedImageVisitor.java' size='49' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/LsImageVisitor.java' size='100' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/NameDistributionVisitor.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java' size='1181' bugHash='803ca4575830f2455dab901012903329' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java' size='110' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java' size='98' bugHash='63cb38dbec197e8702d39cb095da586b' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageDelimitedTextWriter.java' size='101' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java' size='359' bugHash='b3403cda8ca2b23b4eba4cfb95353f7e' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java' size='611' bugHash='0b42b8d0545801bfcf10523ed5036f16' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/TextWriterImageVisitor.java' size='36' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.java' size='64' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/offlineImageViewer/XmlImageVisitor.java' size='40' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/snapshot/LsSnapshottableDir.java' size='25' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/tools/snapshot/SnapshotDiff.java' size='42' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/AtomicFileOutputStream.java' size='44' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/BestEffortLongFile.java' size='47' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/ByteArray.java' size='18' bugHash='0f04795bbe91ad6ce5dd268a5c270d67' bugCount='2'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/Canceler.java' size='12' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/CyclicIteration.java' size='52' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/DataTransferThrottler.java' size='51' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/Diff.java' size='224' bugHash='219fece7bd1a5c440d8f96a457761b56' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/EnumCounters.java' size='86' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/EnumDoubles.java' size='58' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/FoldedTreeSet.java' size='800' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/Holder.java' size='8' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/LightWeightHashSet.java' size='327' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java' size='148' bugHash='f7024caeb5c60881b9dbc17a8775ff6f' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/MD5FileUtils.java' size='71' bugHash='504303d0331872872a5c6f8e6737cd2c' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/PersistentLongFile.java' size='45' bugHash='5c4eadf42c8f9f011ae1efb2b12a9f33' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/ReadOnlyList.java' size='100' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/ReferenceCountMap.java' size='33' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/RwLock.java' size='9' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/util/XMLUtils.java' size='169' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/AuthFilter.java' size='67' bugHash='5a8957ed7340d32bf824db5f49c03022' bugCount='1'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/JsonUtil.java' size='343' bugHash='ff740eeff1b62fc799f5d52c458f7adf' bugCount='4'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/ParamFilter.java' size='36' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/ExceptionHandler.java' size='54' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java' size='16' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/TokenKindParam.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/TokenServiceParam.java' size='13' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/UriFsPathParam.java' size='15' bugCount='0'></FileStats><FileStats path='org/apache/hadoop/hdfs/web/resources/UserProvider.java' size='18' bugCount='0'></FileStats><PackageStats package='org.apache.hadoop.hdfs' priority_1='1' total_bugs='9' priority_2='4' priority_3='4' total_size='1751' total_types='16'><ClassStats bugs='1' size='852' priority_1='1' interface='false' sourceFile='DFSConfigKeys.java' class='org.apache.hadoop.hdfs.DFSConfigKeys'></ClassStats><ClassStats bugs='3' size='587' priority_2='1' priority_3='2' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$1'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$2'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$3'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$AddressMatcher'></ClassStats><ClassStats bugs='2' size='20' priority_3='2' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$ConfiguredNNAddress'></ClassStats><ClassStats bugs='1' size='15' priority_2='1' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$ServiceAndStaleComparator'></ClassStats><ClassStats bugs='1' size='14' priority_2='1' interface='false' sourceFile='DFSUtil.java' class='org.apache.hadoop.hdfs.DFSUtil$ServiceComparator'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='DeprecatedUTF8.java' class='org.apache.hadoop.hdfs.DeprecatedUTF8'></ClassStats><ClassStats bugs='0' size='110' interface='false' sourceFile='HAUtil.java' class='org.apache.hadoop.hdfs.HAUtil'></ClassStats><ClassStats bugs='1' size='8' priority_2='1' interface='false' sourceFile='HDFSPolicyProvider.java' class='org.apache.hadoop.hdfs.HDFSPolicyProvider'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='HdfsDtFetcher.java' class='org.apache.hadoop.hdfs.HdfsDtFetcher'></ClassStats><ClassStats bugs='0' size='72' interface='false' sourceFile='NameNodeProxies.java' class='org.apache.hadoop.hdfs.NameNodeProxies'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='SWebHdfsDtFetcher.java' class='org.apache.hadoop.hdfs.SWebHdfsDtFetcher'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='WebHdfsDtFetcher.java' class='org.apache.hadoop.hdfs.WebHdfsDtFetcher'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.net' total_bugs='0' total_size='442' total_types='7'><ClassStats bugs='0' size='137' interface='false' sourceFile='DFSNetworkTopology.java' class='org.apache.hadoop.hdfs.net.DFSNetworkTopology'></ClassStats><ClassStats bugs='0' size='213' interface='false' sourceFile='DFSTopologyNodeImpl.java' class='org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DFSTopologyNodeImpl.java' class='org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DFSTopologyNodeImpl.java' class='org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl$Factory'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='DomainPeerServer.java' class='org.apache.hadoop.hdfs.net.DomainPeerServer'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='PeerServer.java' class='org.apache.hadoop.hdfs.net.PeerServer'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='TcpPeerServer.java' class='org.apache.hadoop.hdfs.net.TcpPeerServer'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.protocol' priority_1='1' total_bugs='9' priority_2='4' priority_3='4' total_size='772' total_types='25'><ClassStats bugs='1' size='69' priority_1='1' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$2'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BlockReportReplica'></ClassStats><ClassStats bugs='0' size='62' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder'></ClassStats><ClassStats bugs='1' size='29' priority_3='1' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$Builder'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$LongsDecoder'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='BlockListAsLongs.java' class='org.apache.hadoop.hdfs.protocol.BlockListAsLongs$LongsDecoder$1'></ClassStats><ClassStats bugs='1' size='143' priority_2='1' interface='false' sourceFile='CacheDirective.java' class='org.apache.hadoop.hdfs.protocol.CacheDirective'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='FSLimitException.java' class='org.apache.hadoop.hdfs.protocol.FSLimitException'></ClassStats><ClassStats bugs='1' size='13' priority_3='1' interface='false' sourceFile='FSLimitException.java' class='org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException'></ClassStats><ClassStats bugs='1' size='18' priority_3='1' interface='false' sourceFile='FSLimitException.java' class='org.apache.hadoop.hdfs.protocol.FSLimitException$PathComponentTooLongException'></ClassStats><ClassStats bugs='1' size='14' priority_3='1' interface='false' sourceFile='LayoutFlags.java' class='org.apache.hadoop.hdfs.protocol.LayoutFlags'></ClassStats><ClassStats bugs='0' size='57' interface='false' sourceFile='LayoutVersion.java' class='org.apache.hadoop.hdfs.protocol.LayoutVersion'></ClassStats><ClassStats bugs='1' size='94' priority_2='1' interface='false' sourceFile='LayoutVersion.java' class='org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature'></ClassStats><ClassStats bugs='1' size='31' priority_2='1' interface='false' sourceFile='LayoutVersion.java' class='org.apache.hadoop.hdfs.protocol.LayoutVersion$FeatureInfo'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='LayoutVersion.java' class='org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeature'></ClassStats><ClassStats bugs='1' size='7' priority_2='1' interface='false' sourceFile='LayoutVersion.java' class='org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeatureComparator'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='RecoveryInProgressException.java' class='org.apache.hadoop.hdfs.protocol.RecoveryInProgressException'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='RollingUpgradeException.java' class='org.apache.hadoop.hdfs.protocol.RollingUpgradeException'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='SnapshotException.java' class='org.apache.hadoop.hdfs.protocol.SnapshotException'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='SnapshotInfo.java' class='org.apache.hadoop.hdfs.protocol.SnapshotInfo'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='SnapshotInfo.java' class='org.apache.hadoop.hdfs.protocol.SnapshotInfo$Bean'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='UnregisteredNodeException.java' class='org.apache.hadoop.hdfs.protocol.UnregisteredNodeException'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.protocol.datatransfer' total_bugs='4' priority_3='4' total_size='286' total_types='5'><ClassStats bugs='2' size='44' priority_3='2' interface='false' sourceFile='BlackListBasedTrustedChannelResolver.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver'></ClassStats><ClassStats bugs='0' size='198' interface='false' sourceFile='Receiver.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.Receiver'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='Receiver.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.Receiver$1'></ClassStats><ClassStats bugs='2' size='39' priority_3='2' interface='false' sourceFile='WhitelistBasedTrustedChannelResolver.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='package-info.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.package-info'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.protocol.datatransfer.sasl' total_bugs='0' total_size='165' total_types='6'><ClassStats bugs='0' size='10' interface='false' sourceFile='InvalidMagicNumberException.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException'></ClassStats><ClassStats bugs='0' size='115' interface='false' sourceFile='SaslDataTransferServer.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='SaslDataTransferServer.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='SaslDataTransferServer.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$2'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='SaslDataTransferServer.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$PasswordFunction'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='SaslDataTransferServer.java' class='org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$SaslServerCallbackHandler'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.protocol.proto' priority_1='96' total_bugs='315' priority_2='135' priority_3='84' total_size='44088' total_types='460'><ClassStats bugs='0' size='71' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$1'></ClassStats><ClassStats bugs='4' size='54' priority_3='4' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$1'></ClassStats><ClassStats bugs='4' size='33' priority_3='4' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$2'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$AliasMapProtocolService$Stub'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProtoOrBuilder'></ClassStats><ClassStats bugs='5' size='181' priority_1='1' priority_2='1' priority_3='3' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$1'></ClassStats><ClassStats bugs='1' size='137' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='210' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$1'></ClassStats><ClassStats bugs='0' size='233' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='175' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$1'></ClassStats><ClassStats bugs='0' size='160' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='212' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$1'></ClassStats><ClassStats bugs='0' size='307' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='175' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$1'></ClassStats><ClassStats bugs='0' size='160' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='AliasMapProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$1'></ClassStats><ClassStats bugs='1' size='36' priority_3='1' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$1'></ClassStats><ClassStats bugs='1' size='24' priority_3='1' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$Stub'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeLifelineProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='217' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos'></ClassStats><ClassStats bugs='0' size='138' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$1'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='351' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$1'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Action'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Action$1'></ClassStats><ClassStats bugs='0' size='746' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='26' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='174' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='261' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$1'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Action'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Action$1'></ClassStats><ClassStats bugs='1' size='210' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='255' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$1'></ClassStats><ClassStats bugs='0' size='358' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='174' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='263' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$1'></ClassStats><ClassStats bugs='1' size='203' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder'></ClassStats><ClassStats bugs='0' size='11' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='291' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$1'></ClassStats><ClassStats bugs='0' size='431' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='175' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$1'></ClassStats><ClassStats bugs='0' size='160' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='268' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$1'></ClassStats><ClassStats bugs='0' size='257' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='175' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$1'></ClassStats><ClassStats bugs='0' size='160' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='350' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$1'></ClassStats><ClassStats bugs='0' size='463' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='21' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='446' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$1'></ClassStats><ClassStats bugs='0' size='695' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='74' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type$1'></ClassStats><ClassStats bugs='0' size='27' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProtoOrBuilder'></ClassStats><ClassStats bugs='9' size='84' priority_3='9' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$1'></ClassStats><ClassStats bugs='9' size='48' priority_3='9' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='73' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$Stub'></ClassStats><ClassStats bugs='3' size='292' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$1'></ClassStats><ClassStats bugs='0' size='359' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder'></ClassStats><ClassStats bugs='0' size='13' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='244' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$1'></ClassStats><ClassStats bugs='0' size='235' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode$1'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='181' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$1'></ClassStats><ClassStats bugs='1' size='137' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='452' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$1'></ClassStats><ClassStats bugs='0' size='801' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='34' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='306' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$1'></ClassStats><ClassStats bugs='0' size='476' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='17' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='246' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$1'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$BlockStatus'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$BlockStatus$1'></ClassStats><ClassStats bugs='0' size='236' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='174' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='247' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$1'></ClassStats><ClassStats bugs='1' size='201' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='201' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$1'></ClassStats><ClassStats bugs='1' size='157' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='279' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$1'></ClassStats><ClassStats bugs='0' size='279' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='254' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$1'></ClassStats><ClassStats bugs='0' size='357' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='224' priority_1='1' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$1'></ClassStats><ClassStats bugs='1' size='194' priority_2='1' interface='false' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='DatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$1'></ClassStats><ClassStats bugs='2' size='218' priority_1='1' priority_2='1' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$1'></ClassStats><ClassStats bugs='0' size='284' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$1'></ClassStats><ClassStats bugs='0' size='282' interface='false' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='EditLogProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='114' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos'></ClassStats><ClassStats bugs='0' size='70' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$1'></ClassStats><ClassStats bugs='3' size='214' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$1'></ClassStats><ClassStats bugs='1' size='160' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='360' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$1'></ClassStats><ClassStats bugs='0' size='399' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder'></ClassStats><ClassStats bugs='0' size='21' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='174' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='202' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$1'></ClassStats><ClassStats bugs='0' size='185' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='271' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$1'></ClassStats><ClassStats bugs='0' size='259' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder'></ClassStats><ClassStats bugs='0' size='11' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='291' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$1'></ClassStats><ClassStats bugs='0' size='380' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProtoOrBuilder'></ClassStats><ClassStats bugs='4' size='195' priority_1='1' priority_2='1' priority_3='2' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$1'></ClassStats><ClassStats bugs='1' size='138' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State$1'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='231' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$1'></ClassStats><ClassStats bugs='0' size='211' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Type'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Type$1'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='288' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$1'></ClassStats><ClassStats bugs='0' size='286' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$NamenodeRoleProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$NamenodeRoleProto$1'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='379' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$1'></ClassStats><ClassStats bugs='0' size='382' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder'></ClassStats><ClassStats bugs='0' size='19' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='295' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$1'></ClassStats><ClassStats bugs='0' size='356' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='199' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$1'></ClassStats><ClassStats bugs='0' size='256' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='214' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$1'></ClassStats><ClassStats bugs='1' size='157' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='54' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto$1'></ClassStats><ClassStats bugs='3' size='259' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$1'></ClassStats><ClassStats bugs='1' size='209' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='HdfsServerProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$1'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='228' priority_1='1' priority_2='1' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='42' priority_3='2' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$1'></ClassStats><ClassStats bugs='2' size='27' priority_3='2' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$Stub'></ClassStats><ClassStats bugs='3' size='251' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$1'></ClassStats><ClassStats bugs='0' size='231' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='178' priority_1='1' priority_2='1' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$1'></ClassStats><ClassStats bugs='1' size='135' priority_2='1' interface='false' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='InterDatanodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$1'></ClassStats><ClassStats bugs='2' size='241' priority_1='1' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$1'></ClassStats><ClassStats bugs='0' size='233' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='208' priority_1='1' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$1'></ClassStats><ClassStats bugs='1' size='153' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProtoOrBuilder'></ClassStats><ClassStats bugs='4' size='227' priority_1='1' priority_2='1' priority_3='2' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$1'></ClassStats><ClassStats bugs='1' size='181' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='48' priority_3='3' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$1'></ClassStats><ClassStats bugs='3' size='30' priority_3='3' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$2'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalProtocolService$Stub'></ClassStats><ClassStats bugs='3' size='280' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$1'></ClassStats><ClassStats bugs='0' size='260' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='228' priority_1='1' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='JournalProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='162' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos'></ClassStats><ClassStats bugs='0' size='102' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$1'></ClassStats><ClassStats bugs='2' size='214' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$1'></ClassStats><ClassStats bugs='0' size='235' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='244' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$1'></ClassStats><ClassStats bugs='0' size='235' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='175' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$1'></ClassStats><ClassStats bugs='0' size='160' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='225' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$1'></ClassStats><ClassStats bugs='0' size='208' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProtoOrBuilder'></ClassStats><ClassStats bugs='13' size='108' priority_3='13' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$1'></ClassStats><ClassStats bugs='13' size='60' priority_3='13' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='101' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$Stub'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='NamenodeProtocolProtos.java' class='org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProtoOrBuilder'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.protocolPB' total_bugs='8' priority_3='8' total_size='2580' total_types='24'><ClassStats bugs='0' size='1' interface='true' sourceFile='AliasMapProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolPB'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='AliasMapProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='149' interface='false' sourceFile='ClientDatanodeProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='842' interface='false' sourceFile='ClientNamenodeProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='DatanodeLifelineProtocolClientSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolClientSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeLifelineProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolPB'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='DatanodeLifelineProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='156' interface='false' sourceFile='DatanodeProtocolClientSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='DatanodeProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB'></ClassStats><ClassStats bugs='0' size='158' interface='false' sourceFile='DatanodeProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='74' interface='false' sourceFile='InMemoryAliasMapProtocolClientSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='InterDatanodeProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolPB'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='InterDatanodeProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='InterDatanodeProtocolTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='JournalProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.JournalProtocolPB'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='JournalProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.JournalProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='JournalProtocolTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.JournalProtocolTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocolPB.java' class='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB'></ClassStats><ClassStats bugs='0' size='108' interface='false' sourceFile='NamenodeProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='1' size='111' priority_3='1' interface='false' sourceFile='NamenodeProtocolTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB'></ClassStats><ClassStats bugs='7' size='632' priority_3='7' interface='false' sourceFile='PBHelper.java' class='org.apache.hadoop.hdfs.protocolPB.PBHelper'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='PBHelper.java' class='org.apache.hadoop.hdfs.protocolPB.PBHelper$1'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='ReconfigurationProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.protocolPB.ReconfigurationProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ReconfigurationProtocolServerSideUtils.java' class='org.apache.hadoop.hdfs.protocolPB.ReconfigurationProtocolServerSideUtils'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.qjournal.client' total_bugs='2' priority_3='2' total_size='1357' total_types='34'><ClassStats bugs='0' size='24' interface='true' sourceFile='AsyncLogger.java' class='org.apache.hadoop.hdfs.qjournal.client.AsyncLogger'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='AsyncLogger.java' class='org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory'></ClassStats><ClassStats bugs='0' size='197' interface='false' sourceFile='AsyncLoggerSet.java' class='org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet'></ClassStats><ClassStats bugs='1' size='240' priority_3='1' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$1'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$11'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$12'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$13'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$14'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$15'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$16'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$17'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$18'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$19'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$2'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$20'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$21'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$22'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$3'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$4'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$5'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$6'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$8'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='IPCLoggerChannel.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$9'></ClassStats><ClassStats bugs='0' size='67' interface='false' sourceFile='IPCLoggerChannelMetrics.java' class='org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannelMetrics'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='LoggerTooFarBehindException.java' class='org.apache.hadoop.hdfs.qjournal.client.LoggerTooFarBehindException'></ClassStats><ClassStats bugs='0' size='135' interface='false' sourceFile='QuorumCall.java' class='org.apache.hadoop.hdfs.qjournal.client.QuorumCall'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='QuorumCall.java' class='org.apache.hadoop.hdfs.qjournal.client.QuorumCall$1'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='QuorumException.java' class='org.apache.hadoop.hdfs.qjournal.client.QuorumException'></ClassStats><ClassStats bugs='0' size='326' interface='false' sourceFile='QuorumJournalManager.java' class='org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager'></ClassStats><ClassStats bugs='0' size='63' interface='false' sourceFile='QuorumOutputStream.java' class='org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream'></ClassStats><ClassStats bugs='1' size='29' priority_3='1' interface='false' sourceFile='SegmentRecoveryComparator.java' class='org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.qjournal.protocol' priority_1='42' total_bugs='146' priority_2='61' priority_3='43' total_size='17291' total_types='191'><ClassStats bugs='0' size='3' interface='true' sourceFile='InterQJournalProtocol.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocol'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$1'></ClassStats><ClassStats bugs='1' size='36' priority_3='1' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$1'></ClassStats><ClassStats bugs='1' size='24' priority_3='1' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$2'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='InterQJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$InterQJournalProtocolService$Stub'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='JournalNotFormattedException.java' class='org.apache.hadoop.hdfs.qjournal.protocol.JournalNotFormattedException'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='JournalOutOfSyncException.java' class='org.apache.hadoop.hdfs.qjournal.protocol.JournalOutOfSyncException'></ClassStats><ClassStats bugs='0' size='21' interface='true' sourceFile='QJournalProtocol.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol'></ClassStats><ClassStats bugs='0' size='270' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos'></ClassStats><ClassStats bugs='0' size='174' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$1'></ClassStats><ClassStats bugs='2' size='255' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$1'></ClassStats><ClassStats bugs='0' size='285' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProtoOrBuilder'></ClassStats><ClassStats bugs='4' size='315' priority_1='1' priority_2='1' priority_3='2' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$1'></ClassStats><ClassStats bugs='0' size='381' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='241' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$1'></ClassStats><ClassStats bugs='0' size='233' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='214' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$1'></ClassStats><ClassStats bugs='0' size='235' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='228' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='252' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$1'></ClassStats><ClassStats bugs='0' size='283' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='264' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$1'></ClassStats><ClassStats bugs='0' size='255' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='11' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='241' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$1'></ClassStats><ClassStats bugs='0' size='233' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='230' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$1'></ClassStats><ClassStats bugs='1' size='183' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$1'></ClassStats><ClassStats bugs='0' size='161' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='215' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='165' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$1'></ClassStats><ClassStats bugs='1' size='111' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='181' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$1'></ClassStats><ClassStats bugs='1' size='137' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='319' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$1'></ClassStats><ClassStats bugs='0' size='308' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='278' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$1'></ClassStats><ClassStats bugs='0' size='307' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='162' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$1'></ClassStats><ClassStats bugs='1' size='109' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='202' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$1'></ClassStats><ClassStats bugs='0' size='185' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosDataOrBuilder'></ClassStats><ClassStats bugs='2' size='202' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$1'></ClassStats><ClassStats bugs='0' size='185' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='246' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$1'></ClassStats><ClassStats bugs='0' size='228' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='10' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='202' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$1'></ClassStats><ClassStats bugs='0' size='185' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProtoOrBuilder'></ClassStats><ClassStats bugs='19' size='144' priority_3='19' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$1'></ClassStats><ClassStats bugs='19' size='78' priority_3='19' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2'></ClassStats><ClassStats bugs='0' size='20' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$BlockingInterface'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$BlockingStub'></ClassStats><ClassStats bugs='0' size='20' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$Interface'></ClassStats><ClassStats bugs='0' size='143' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$Stub'></ClassStats><ClassStats bugs='2' size='290' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$1'></ClassStats><ClassStats bugs='0' size='279' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder'></ClassStats><ClassStats bugs='0' size='13' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='217' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$1'></ClassStats><ClassStats bugs='1' size='159' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='225' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$1'></ClassStats><ClassStats bugs='0' size='207' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='137' priority_1='1' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolProtos.java' class='org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='RequestInfo.java' class='org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.qjournal.protocolPB' total_bugs='0' total_size='423' total_types='6'><ClassStats bugs='0' size='1' interface='true' sourceFile='InterQJournalProtocolPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.InterQJournalProtocolPB'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='InterQJournalProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.InterQJournalProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='InterQJournalProtocolTranslatorPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.InterQJournalProtocolTranslatorPB'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='QJournalProtocolPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolPB'></ClassStats><ClassStats bugs='0' size='156' interface='false' sourceFile='QJournalProtocolServerSideTranslatorPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB'></ClassStats><ClassStats bugs='0' size='221' interface='false' sourceFile='QJournalProtocolTranslatorPB.java' class='org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.qjournal.server' priority_1='1' total_bugs='6' priority_2='1' priority_3='4' total_size='1577' total_types='15'><ClassStats bugs='0' size='125' interface='false' sourceFile='GetJournalEditServlet.java' class='org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='JNStorage.java' class='org.apache.hadoop.hdfs.qjournal.server.JNStorage'></ClassStats><ClassStats bugs='1' size='487' priority_2='1' interface='false' sourceFile='Journal.java' class='org.apache.hadoop.hdfs.qjournal.server.Journal'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='Journal.java' class='org.apache.hadoop.hdfs.qjournal.server.Journal$1'></ClassStats><ClassStats bugs='1' size='12' priority_1='1' interface='false' sourceFile='JournalFaultInjector.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalFaultInjector'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='JournalMetrics.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalMetrics'></ClassStats><ClassStats bugs='1' size='230' priority_3='1' interface='false' sourceFile='JournalNode.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNode'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='JournalNode.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNode$1'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='JournalNode.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter'></ClassStats><ClassStats bugs='0' size='66' interface='false' sourceFile='JournalNodeHttpServer.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='JournalNodeMXBean.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean'></ClassStats><ClassStats bugs='0' size='132' interface='false' sourceFile='JournalNodeRpcServer.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer'></ClassStats><ClassStats bugs='2' size='284' priority_3='2' interface='false' sourceFile='JournalNodeSyncer.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer'></ClassStats><ClassStats bugs='1' size='18' priority_3='1' interface='false' sourceFile='JournalNodeSyncer.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='JournalNodeSyncer.java' class='org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy$1'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.security.token.block' total_bugs='2' priority_2='1' priority_3='1' total_size='340' total_types='5'><ClassStats bugs='0' size='10' interface='false' sourceFile='BlockKey.java' class='org.apache.hadoop.hdfs.security.token.block.BlockKey'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='BlockPoolTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager'></ClassStats><ClassStats bugs='1' size='230' priority_3='1' interface='false' sourceFile='BlockTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager'></ClassStats><ClassStats bugs='1' size='51' priority_2='1' interface='false' sourceFile='ExportedBlockKeys.java' class='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='ExportedBlockKeys.java' class='org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys$1'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.security.token.delegation' total_bugs='0' total_size='280' total_types='4'><ClassStats bugs='0' size='188' interface='false' sourceFile='DelegationTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DelegationTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DelegationTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SecretManagerState'></ClassStats><ClassStats bugs='0' size='81' interface='false' sourceFile='DelegationTokenSecretManager.java' class='org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.aliasmap' total_bugs='0' total_size='184' total_types='5'><ClassStats bugs='0' size='101' interface='false' sourceFile='InMemoryAliasMap.java' class='org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='InMemoryAliasMap.java' class='org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap$CheckedFunction2'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='InMemoryAliasMapProtocol.java' class='org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMapProtocol'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='InMemoryAliasMapProtocol.java' class='org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMapProtocol$IterationResult'></ClassStats><ClassStats bugs='0' size='64' interface='false' sourceFile='InMemoryLevelDBAliasMapServer.java' class='org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.balancer' total_bugs='6' priority_2='1' priority_3='5' total_size='1813' total_types='33'><ClassStats bugs='1' size='321' priority_3='1' interface='false' sourceFile='Balancer.java' class='org.apache.hadoop.hdfs.server.balancer.Balancer'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='Balancer.java' class='org.apache.hadoop.hdfs.server.balancer.Balancer$Cli'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='Balancer.java' class='org.apache.hadoop.hdfs.server.balancer.Balancer$Result'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='BalancerParameters.java' class='org.apache.hadoop.hdfs.server.balancer.BalancerParameters'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='BalancerParameters.java' class='org.apache.hadoop.hdfs.server.balancer.BalancerParameters$1'></ClassStats><ClassStats bugs='0' size='55' interface='false' sourceFile='BalancerParameters.java' class='org.apache.hadoop.hdfs.server.balancer.BalancerParameters$Builder'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='BalancingPolicy.java' class='org.apache.hadoop.hdfs.server.balancer.BalancingPolicy'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='BalancingPolicy.java' class='org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Node'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='BalancingPolicy.java' class='org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Pool'></ClassStats><ClassStats bugs='2' size='257' priority_3='2' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$1'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$2'></ClassStats><ClassStats bugs='1' size='25' priority_3='1' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock'></ClassStats><ClassStats bugs='1' size='25' priority_2='1' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped'></ClassStats><ClassStats bugs='0' size='86' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$GlobalBlockMap'></ClassStats><ClassStats bugs='0' size='154' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove'></ClassStats><ClassStats bugs='1' size='147' priority_3='1' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$StorageGroupMap'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Task'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='Dispatcher.java' class='org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='ExitStatus.java' class='org.apache.hadoop.hdfs.server.balancer.ExitStatus'></ClassStats><ClassStats bugs='0' size='70' interface='false' sourceFile='KeyManager.java' class='org.apache.hadoop.hdfs.server.balancer.KeyManager'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='KeyManager.java' class='org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='Matcher.java' class='org.apache.hadoop.hdfs.server.balancer.Matcher'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='Matcher.java' class='org.apache.hadoop.hdfs.server.balancer.Matcher$1'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='Matcher.java' class='org.apache.hadoop.hdfs.server.balancer.Matcher$2'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='Matcher.java' class='org.apache.hadoop.hdfs.server.balancer.Matcher$3'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='MovedBlocks.java' class='org.apache.hadoop.hdfs.server.balancer.MovedBlocks'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='MovedBlocks.java' class='org.apache.hadoop.hdfs.server.balancer.MovedBlocks$Locations'></ClassStats><ClassStats bugs='0' size='125' interface='false' sourceFile='NameNodeConnector.java' class='org.apache.hadoop.hdfs.server.balancer.NameNodeConnector'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.blockmanagement' priority_1='1' total_bugs='39' priority_2='2' priority_3='36' total_size='10072' total_types='129'><ClassStats bugs='0' size='50' interface='false' sourceFile='AvailableSpaceBlockPlacementPolicy.java' class='org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='BlockCollection.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection'></ClassStats><ClassStats bugs='0' size='87' interface='false' sourceFile='BlockIdManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockIdManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager$1'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='BlockInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='BlockInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo$1'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='BlockInfoContiguous.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='BlockInfoStriped.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='BlockInfoStriped.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped$1'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='BlockInfoStriped.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped$1$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='BlockInfoStriped.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped$StorageAndBlockIndex'></ClassStats><ClassStats bugs='5' size='2337' priority_3='5' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$3'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockInfoToAdd'></ClassStats><ClassStats bugs='0' size='55' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MisReplicationResult'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo'></ClassStats><ClassStats bugs='0' size='57' interface='false' sourceFile='BlockManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter'></ClassStats><ClassStats bugs='1' size='14' priority_1='1' interface='false' sourceFile='BlockManagerFaultInjector.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerFaultInjector'></ClassStats><ClassStats bugs='0' size='282' interface='false' sourceFile='BlockManagerSafeMode.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockManagerSafeMode.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='BlockManagerSafeMode.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$BMSafeModeStatus'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='BlockManagerSafeMode.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$SafeModeMonitor'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='BlockPlacementPolicies.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicies'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockPlacementPolicies.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicies$1'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='BlockPlacementPolicy.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='BlockPlacementPolicy.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException'></ClassStats><ClassStats bugs='1' size='510' priority_3='1' interface='false' sourceFile='BlockPlacementPolicyDefault.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='BlockPlacementPolicyDefault.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$1'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='BlockPlacementPolicyDefault.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='BlockPlacementPolicyRackFaultTolerant.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant'></ClassStats><ClassStats bugs='2' size='167' priority_3='2' interface='false' sourceFile='BlockPlacementPolicyWithNodeGroup.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup'></ClassStats><ClassStats bugs='0' size='99' interface='false' sourceFile='BlockPlacementPolicyWithUpgradeDomain.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='BlockPlacementStatus.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatus'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='BlockPlacementStatusDefault.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusDefault'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='BlockPlacementStatusWithNodeGroup.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='BlockPlacementStatusWithUpgradeDomain.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain'></ClassStats><ClassStats bugs='0' size='59' interface='false' sourceFile='BlockReconstructionWork.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork'></ClassStats><ClassStats bugs='2' size='155' priority_3='2' interface='false' sourceFile='BlockReportLeaseManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager'></ClassStats><ClassStats bugs='1' size='39' priority_3='1' interface='false' sourceFile='BlockReportLeaseManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='BlockStatsMXBean.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockStatsMXBean'></ClassStats><ClassStats bugs='1' size='57' priority_2='1' interface='false' sourceFile='BlockStoragePolicySuite.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='BlockToMarkCorrupt.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt'></ClassStats><ClassStats bugs='0' size='167' interface='false' sourceFile='BlockUnderConstructionFeature.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='BlockUnderConstructionFeature.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature$1'></ClassStats><ClassStats bugs='2' size='98' priority_3='2' interface='false' sourceFile='BlocksMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='BlocksMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='BlocksMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$2'></ClassStats><ClassStats bugs='1' size='433' priority_2='1' interface='false' sourceFile='CacheReplicationMonitor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor'></ClassStats><ClassStats bugs='1' size='53' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager'></ClassStats><ClassStats bugs='4' size='33' priority_3='4' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties'></ClassStats><ClassStats bugs='1' size='9' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$1'></ClassStats><ClassStats bugs='1' size='10' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$2'></ClassStats><ClassStats bugs='1' size='9' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$3'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$4'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5'></ClassStats><ClassStats bugs='1' size='7' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$5$1'></ClassStats><ClassStats bugs='1' size='10' priority_3='1' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$6'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='CombinedHostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties$HostIterator'></ClassStats><ClassStats bugs='1' size='101' priority_3='1' interface='false' sourceFile='CorruptReplicasMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='CorruptReplicasMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason'></ClassStats><ClassStats bugs='0' size='163' interface='false' sourceFile='DatanodeAdminManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager'></ClassStats><ClassStats bugs='0' size='166' interface='false' sourceFile='DatanodeAdminManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager$Monitor'></ClassStats><ClassStats bugs='1' size='428' priority_3='1' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$1'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockIterator'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockQueue'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockTargetPair'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$CachedBlocksList'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$CachedBlocksList$Type'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='DatanodeDescriptor.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$LeavingServiceStatus'></ClassStats><ClassStats bugs='2' size='874' priority_3='2' interface='false' sourceFile='DatanodeManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='DatanodeManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager$1'></ClassStats><ClassStats bugs='0' size='18' interface='true' sourceFile='DatanodeStatistics.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStatistics'></ClassStats><ClassStats bugs='0' size='110' interface='false' sourceFile='DatanodeStats.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStats'></ClassStats><ClassStats bugs='0' size='41' interface='false' sourceFile='DatanodeStats.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStats$StorageTypeStatsMap'></ClassStats><ClassStats bugs='2' size='191' priority_3='2' interface='false' sourceFile='DatanodeStorageInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeStorageInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='DatanodeStorageInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$1$1'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='DatanodeStorageInfo.java' class='org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$AddBlockResult'></ClassStats><ClassStats bugs='0' size='91' interface='false' sourceFile='ErasureCodingWork.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ErasureCodingWork'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='ExcessRedundancyMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='FSClusterStats.java' class='org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats'></ClassStats><ClassStats bugs='0' size='199' interface='false' sourceFile='HeartbeatManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='HeartbeatManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$1'></ClassStats><ClassStats bugs='1' size='31' priority_3='1' interface='false' sourceFile='HeartbeatManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor'></ClassStats><ClassStats bugs='0' size='112' interface='false' sourceFile='Host2NodesMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='HostConfigManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HostConfigManager'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='HostFileManager.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager'></ClassStats><ClassStats bugs='1' size='30' priority_3='1' interface='false' sourceFile='HostSet.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HostSet'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='HostSet.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HostSet$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='HostSet.java' class='org.apache.hadoop.hdfs.server.blockmanagement.HostSet$2'></ClassStats><ClassStats bugs='0' size='156' interface='false' sourceFile='InvalidateBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='LocatedBlockBuilder.java' class='org.apache.hadoop.hdfs.server.blockmanagement.LocatedBlockBuilder'></ClassStats><ClassStats bugs='1' size='190' priority_3='1' interface='false' sourceFile='LowRedundancyBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='LowRedundancyBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks$1'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='NumberReplicas.java' class='org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='NumberReplicas.java' class='org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas$StoredReplicaState'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='OutOfLegacyGenerationStampsException.java' class='org.apache.hadoop.hdfs.server.blockmanagement.OutOfLegacyGenerationStampsException'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='PendingDataNodeMessages.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingDataNodeMessages'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='PendingDataNodeMessages.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingDataNodeMessages$ReportedBlockInfo'></ClassStats><ClassStats bugs='0' size='111' interface='false' sourceFile='PendingReconstructionBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='PendingReconstructionBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingBlockInfo'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='PendingReconstructionBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='PendingRecoveryBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='PendingRecoveryBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks$1'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='PendingRecoveryBlocks.java' class='org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks$BlockRecoveryAttempt'></ClassStats><ClassStats bugs='0' size='94' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedBlockList'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedBlockList$1'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedBlocksBuilder'></ClassStats><ClassStats bugs='2' size='20' priority_3='2' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo'></ClassStats><ClassStats bugs='0' size='79' interface='false' sourceFile='ProvidedStorageMap.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='ReplicaUnderConstruction.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='ReplicationWork.java' class='org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='SequentialBlockGroupIdGenerator.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockGroupIdGenerator'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='SequentialBlockIdGenerator.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockIdGenerator'></ClassStats><ClassStats bugs='1' size='100' priority_3='1' interface='false' sourceFile='SlowDiskTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='SlowDiskTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='SlowDiskTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$2'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='SlowDiskTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker$DiskLatency'></ClassStats><ClassStats bugs='1' size='74' priority_3='1' interface='false' sourceFile='SlowPeerTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='SlowPeerTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='SlowPeerTracker.java' class='org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker$ReportForJson'></ClassStats><ClassStats bugs='0' size='89' interface='false' sourceFile='StorageTypeStats.java' class='org.apache.hadoop.hdfs.server.blockmanagement.StorageTypeStats'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='UnresolvedTopologyException.java' class='org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.common' priority_1='1' total_bugs='9' priority_2='6' priority_3='2' total_size='1431' total_types='28'><ClassStats bugs='0' size='2' interface='true' sourceFile='BlockAlias.java' class='org.apache.hadoop.hdfs.server.common.BlockAlias'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='FileRegion.java' class='org.apache.hadoop.hdfs.server.common.FileRegion'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='GenerationStamp.java' class='org.apache.hadoop.hdfs.server.common.GenerationStamp'></ClassStats><ClassStats bugs='2' size='24' priority_1='1' priority_2='1' interface='true' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$1'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState'></ClassStats><ClassStats bugs='1' size='18' priority_3='1' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NodeType'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$RollingUpgradeStartupOption'></ClassStats><ClassStats bugs='5' size='111' priority_2='4' priority_3='1' interface='false' sourceFile='HdfsServerConstants.java' class='org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='HttpGetFailedException.java' class='org.apache.hadoop.hdfs.server.common.HttpGetFailedException'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='HttpPutFailedException.java' class='org.apache.hadoop.hdfs.server.common.HttpPutFailedException'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='InconsistentFSStateException.java' class='org.apache.hadoop.hdfs.server.common.InconsistentFSStateException'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='IncorrectVersionException.java' class='org.apache.hadoop.hdfs.server.common.IncorrectVersionException'></ClassStats><ClassStats bugs='0' size='99' interface='false' sourceFile='JspHelper.java' class='org.apache.hadoop.hdfs.server.common.JspHelper'></ClassStats><ClassStats bugs='0' size='77' interface='false' sourceFile='MetricsLoggerTask.java' class='org.apache.hadoop.hdfs.server.common.MetricsLoggerTask'></ClassStats><ClassStats bugs='1' size='207' priority_2='1' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$2'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$DirIterator'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$FormatConfirmable'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$StorageDirType'></ClassStats><ClassStats bugs='0' size='312' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='Storage.java' class='org.apache.hadoop.hdfs.server.common.Storage$StorageState'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='StorageErrorReporter.java' class='org.apache.hadoop.hdfs.server.common.StorageErrorReporter'></ClassStats><ClassStats bugs='0' size='122' interface='false' sourceFile='StorageInfo.java' class='org.apache.hadoop.hdfs.server.common.StorageInfo'></ClassStats><ClassStats bugs='0' size='195' interface='false' sourceFile='Util.java' class='org.apache.hadoop.hdfs.server.common.Util'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.common.blockaliasmap' total_bugs='0' total_size='24' total_types='7'><ClassStats bugs='0' size='7' interface='false' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$ImmutableIterator'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Reader'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Reader$Options'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Writer'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='BlockAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Writer$Options'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='package-info.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.package-info'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl' total_bugs='4' priority_3='4' total_size='499' total_types='19'><ClassStats bugs='3' size='35' priority_3='3' interface='false' sourceFile='InMemoryLevelDBAliasMapClient.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='InMemoryLevelDBAliasMapClient.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbReader'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='InMemoryLevelDBAliasMapClient.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbReader$LevelDbIterator'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='InMemoryLevelDBAliasMapClient.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbWriter'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBOptions'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBReader'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBReader$FRIterator'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBReader$Options'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBWriter'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='LevelDBFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBWriter$Options'></ClassStats><ClassStats bugs='0' size='75' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$ReaderOptions'></ClassStats><ClassStats bugs='1' size='83' priority_3='1' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader$FRIterator'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader$Options'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextWriter'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextWriter$Options'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='TextFileRegionAliasMap.java' class='org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$WriterOptions'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode' priority_1='1' total_bugs='42' priority_2='7' priority_3='34' total_size='11143' total_types='122'><ClassStats bugs='1' size='405' priority_3='1' interface='false' sourceFile='BPOfferService.java' class='org.apache.hadoop.hdfs.server.datanode.BPOfferService'></ClassStats><ClassStats bugs='0' size='505' interface='false' sourceFile='BPServiceActor.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActor'></ClassStats><ClassStats bugs='1' size='84' priority_3='1' interface='false' sourceFile='BPServiceActor.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='BPServiceActor.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='BPServiceActor.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$RunningState'></ClassStats><ClassStats bugs='0' size='92' interface='false' sourceFile='BPServiceActor.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='BPServiceActorAction.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActorAction'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='BPServiceActorActionException.java' class='org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$1'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer'></ClassStats><ClassStats bugs='2' size='65' priority_3='2' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer'></ClassStats><ClassStats bugs='0' size='186' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer$LiveBlockInfo'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='BlockChecksumHelper.java' class='org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$ReplicatedBlockChecksumComputer'></ClassStats><ClassStats bugs='2' size='163' priority_2='1' priority_3='1' interface='false' sourceFile='BlockPoolManager.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='BlockPoolManager.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='BlockPoolManager.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2'></ClassStats><ClassStats bugs='3' size='376' priority_3='3' interface='false' sourceFile='BlockPoolSliceStorage.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='BlockPoolSliceStorage.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$1'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='BlockPoolSliceStorage.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$2'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='BlockPoolSliceStorage.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$3'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockPoolSliceStorage.java' class='org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage$4'></ClassStats><ClassStats bugs='0' size='596' interface='false' sourceFile='BlockReceiver.java' class='org.apache.hadoop.hdfs.server.datanode.BlockReceiver'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockReceiver.java' class='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$1'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='BlockReceiver.java' class='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet'></ClassStats><ClassStats bugs='3' size='247' priority_3='3' interface='false' sourceFile='BlockReceiver.java' class='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='BlockReceiver.java' class='org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponderType'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker'></ClassStats><ClassStats bugs='1' size='17' priority_3='1' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$2'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$BlockRecord'></ClassStats><ClassStats bugs='0' size='148' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous'></ClassStats><ClassStats bugs='0' size='113' interface='false' sourceFile='BlockRecoveryWorker.java' class='org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskStriped'></ClassStats><ClassStats bugs='0' size='106' interface='false' sourceFile='BlockScanner.java' class='org.apache.hadoop.hdfs.server.datanode.BlockScanner'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='BlockScanner.java' class='org.apache.hadoop.hdfs.server.datanode.BlockScanner$Conf'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='BlockScanner.java' class='org.apache.hadoop.hdfs.server.datanode.BlockScanner$Servlet'></ClassStats><ClassStats bugs='0' size='378' interface='false' sourceFile='BlockSender.java' class='org.apache.hadoop.hdfs.server.datanode.BlockSender'></ClassStats><ClassStats bugs='2' size='12' priority_2='2' interface='false' sourceFile='ChunkChecksum.java' class='org.apache.hadoop.hdfs.server.datanode.ChunkChecksum'></ClassStats><ClassStats bugs='0' size='144' interface='false' sourceFile='DNConf.java' class='org.apache.hadoop.hdfs.server.datanode.DNConf'></ClassStats><ClassStats bugs='5' size='1562' priority_3='5' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$2'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$3'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$4'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$5'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$ChangedVolumes'></ClassStats><ClassStats bugs='0' size='90' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$ShortCircuitFdsUnsupportedException'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='DataNode.java' class='org.apache.hadoop.hdfs.server.datanode.DataNode$ShortCircuitFdsVersionException'></ClassStats><ClassStats bugs='0' size='41' interface='false' sourceFile='DataNodeFaultInjector.java' class='org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector'></ClassStats><ClassStats bugs='1' size='15' priority_1='1' interface='false' sourceFile='DataNodeLayoutVersion.java' class='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='DataNodeLayoutVersion.java' class='org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature'></ClassStats><ClassStats bugs='0' size='17' interface='true' sourceFile='DataNodeMXBean.java' class='org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean'></ClassStats><ClassStats bugs='2' size='606' priority_3='2' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$1'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$2'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$3'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$4'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$5'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$6'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$7'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$LinkArgs'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$UpgradeTask'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='DataStorage.java' class='org.apache.hadoop.hdfs.server.datanode.DataStorage$VolumeBuilder'></ClassStats><ClassStats bugs='0' size='788' interface='false' sourceFile='DataXceiver.java' class='org.apache.hadoop.hdfs.server.datanode.DataXceiver'></ClassStats><ClassStats bugs='0' size='137' interface='false' sourceFile='DataXceiverServer.java' class='org.apache.hadoop.hdfs.server.datanode.DataXceiverServer'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DataXceiverServer.java' class='org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$1'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='DataXceiverServer.java' class='org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler'></ClassStats><ClassStats bugs='1' size='45' priority_3='1' interface='false' sourceFile='DatanodeUtil.java' class='org.apache.hadoop.hdfs.server.datanode.DatanodeUtil'></ClassStats><ClassStats bugs='2' size='218' priority_3='2' interface='false' sourceFile='DirectoryScanner.java' class='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='DirectoryScanner.java' class='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$BlockDirFilter'></ClassStats><ClassStats bugs='1' size='46' priority_3='1' interface='false' sourceFile='DirectoryScanner.java' class='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='DirectoryScanner.java' class='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ScanInfoPerBlockPool'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='DirectoryScanner.java' class='org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$Stats'></ClassStats><ClassStats bugs='0' size='255' interface='false' sourceFile='DiskBalancer.java' class='org.apache.hadoop.hdfs.server.datanode.DiskBalancer'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='DiskBalancer.java' class='org.apache.hadoop.hdfs.server.datanode.DiskBalancer$1'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='DiskBalancer.java' class='org.apache.hadoop.hdfs.server.datanode.DiskBalancer$BlockMover'></ClassStats><ClassStats bugs='0' size='199' interface='false' sourceFile='DiskBalancer.java' class='org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='DiskBalancer.java' class='org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='ErrorReportAction.java' class='org.apache.hadoop.hdfs.server.datanode.ErrorReportAction'></ClassStats><ClassStats bugs='1' size='12' priority_3='1' interface='false' sourceFile='FaultInjectorFileIoEvents.java' class='org.apache.hadoop.hdfs.server.datanode.FaultInjectorFileIoEvents'></ClassStats><ClassStats bugs='0' size='305' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider$1'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider$OPERATION'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileInputStream'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream'></ClassStats><ClassStats bugs='0' size='65' interface='false' sourceFile='FileIoProvider.java' class='org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='FinalizedProvidedReplica.java' class='org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica'></ClassStats><ClassStats bugs='2' size='50' priority_2='2' interface='false' sourceFile='FinalizedReplica.java' class='org.apache.hadoop.hdfs.server.datanode.FinalizedReplica'></ClassStats><ClassStats bugs='1' size='102' priority_3='1' interface='false' sourceFile='IncrementalBlockReportManager.java' class='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='IncrementalBlockReportManager.java' class='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$1'></ClassStats><ClassStats bugs='1' size='38' priority_3='1' interface='false' sourceFile='IncrementalBlockReportManager.java' class='org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR'></ClassStats><ClassStats bugs='1' size='218' priority_3='1' interface='false' sourceFile='LocalReplica.java' class='org.apache.hadoop.hdfs.server.datanode.LocalReplica'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='LocalReplica.java' class='org.apache.hadoop.hdfs.server.datanode.LocalReplica$ReplicaDirInfo'></ClassStats><ClassStats bugs='1' size='183' priority_2='1' interface='false' sourceFile='LocalReplicaInPipeline.java' class='org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline'></ClassStats><ClassStats bugs='0' size='64' interface='false' sourceFile='ProfilingFileIoEvents.java' class='org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='ProfilingFileIoEvents.java' class='org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents$1'></ClassStats><ClassStats bugs='0' size='136' interface='false' sourceFile='ProvidedReplica.java' class='org.apache.hadoop.hdfs.server.datanode.ProvidedReplica'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='Replica.java' class='org.apache.hadoop.hdfs.server.datanode.Replica'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ReplicaAlreadyExistsException.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='ReplicaBeingWritten.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten'></ClassStats><ClassStats bugs='1' size='198' priority_2='1' interface='false' sourceFile='ReplicaBuilder.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='ReplicaBuilder.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaBuilder$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='ReplicaHandler.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaHandler'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='ReplicaInPipeline.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline'></ClassStats><ClassStats bugs='0' size='73' interface='false' sourceFile='ReplicaInfo.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaInfo'></ClassStats><ClassStats bugs='1' size='62' priority_3='1' interface='false' sourceFile='ReplicaUnderRecovery.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ReplicaWaitingToBeRecovered.java' class='org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered'></ClassStats><ClassStats bugs='0' size='53' interface='false' sourceFile='ReportBadBlockAction.java' class='org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='SecureDataNodeStarter.java' class='org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='SecureDataNodeStarter.java' class='org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources'></ClassStats><ClassStats bugs='1' size='163' priority_3='1' interface='false' sourceFile='ShortCircuitRegistry.java' class='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='ShortCircuitRegistry.java' class='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='ShortCircuitRegistry.java' class='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='ShortCircuitRegistry.java' class='org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$Visitor'></ClassStats><ClassStats bugs='1' size='101' priority_3='1' interface='false' sourceFile='StorageLocation.java' class='org.apache.hadoop.hdfs.server.datanode.StorageLocation'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='StorageLocation.java' class='org.apache.hadoop.hdfs.server.datanode.StorageLocation$CheckContext'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='UnexpectedReplicaStateException.java' class='org.apache.hadoop.hdfs.server.datanode.UnexpectedReplicaStateException'></ClassStats><ClassStats bugs='4' size='355' priority_3='4' interface='false' sourceFile='VolumeScanner.java' class='org.apache.hadoop.hdfs.server.datanode.VolumeScanner'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='VolumeScanner.java' class='org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler'></ClassStats><ClassStats bugs='0' size='49' interface='false' sourceFile='VolumeScanner.java' class='org.apache.hadoop.hdfs.server.datanode.VolumeScanner$Statistics'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.checker' priority_1='1' total_bugs='11' priority_2='2' priority_3='8' total_size='988' total_types='32'><ClassStats bugs='8' size='294' priority_1='1' priority_2='2' priority_3='5' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$1'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$AtomicHelper'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Cancellation'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$DirectExecutor'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Failure'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Failure$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Listener'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SafeAtomicHelper'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SetFuture'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$TrustedFuture'></ClassStats><ClassStats bugs='1' size='59' priority_3='1' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper$1'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='AbstractFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Waiter'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='AsyncChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.AsyncChecker'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='Checkable.java' class='org.apache.hadoop.hdfs.server.datanode.checker.Checkable'></ClassStats><ClassStats bugs='1' size='116' priority_3='1' interface='false' sourceFile='DatasetVolumeChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DatasetVolumeChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='DatasetVolumeChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$2'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='DatasetVolumeChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$Callback'></ClassStats><ClassStats bugs='0' size='62' interface='false' sourceFile='DatasetVolumeChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler'></ClassStats><ClassStats bugs='0' size='81' interface='false' sourceFile='StorageLocationChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='StorageLocationChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker$1'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='ThrottledAsyncChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ThrottledAsyncChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='ThrottledAsyncChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$2'></ClassStats><ClassStats bugs='1' size='20' priority_3='1' interface='false' sourceFile='ThrottledAsyncChecker.java' class='org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='TimeoutFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.TimeoutFuture'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='TimeoutFuture.java' class='org.apache.hadoop.hdfs.server.datanode.checker.TimeoutFuture$Fire'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='VolumeCheckResult.java' class='org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='package-info.java' class='org.apache.hadoop.hdfs.server.datanode.checker.package-info'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.erasurecode' total_bugs='3' priority_2='1' priority_3='2' total_size='1123' total_types='15'><ClassStats bugs='1' size='62' priority_3='1' interface='false' sourceFile='ErasureCodingWorker.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='ErasureCodingWorker.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ErasureCodingWorker.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker$2'></ClassStats><ClassStats bugs='1' size='27' priority_2='1' interface='false' sourceFile='StripedBlockChecksumCompositeCrcReconstructor.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='StripedBlockChecksumMd5CrcReconstructor.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumMd5CrcReconstructor'></ClassStats><ClassStats bugs='0' size='107' interface='false' sourceFile='StripedBlockChecksumReconstructor.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumReconstructor'></ClassStats><ClassStats bugs='0' size='95' interface='false' sourceFile='StripedBlockReader.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='StripedBlockReader.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader$1'></ClassStats><ClassStats bugs='0' size='66' interface='false' sourceFile='StripedBlockReconstructor.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='StripedBlockWriter.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter'></ClassStats><ClassStats bugs='0' size='251' interface='false' sourceFile='StripedReader.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReader'></ClassStats><ClassStats bugs='1' size='44' priority_3='1' interface='false' sourceFile='StripedReconstructionInfo.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructionInfo'></ClassStats><ClassStats bugs='0' size='111' interface='false' sourceFile='StripedReconstructor.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructor'></ClassStats><ClassStats bugs='0' size='178' interface='false' sourceFile='StripedWriter.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedWriter'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='package-info.java' class='org.apache.hadoop.hdfs.server.datanode.erasurecode.package-info'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.fsdataset' total_bugs='1' priority_3='1' total_size='727' total_types='18'><ClassStats bugs='0' size='87' interface='false' sourceFile='AvailableSpaceVolumeChoosingPolicy.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='AvailableSpaceVolumeChoosingPolicy.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy$AvailableSpaceVolumeList'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='AvailableSpaceVolumeChoosingPolicy.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy$AvailableSpaceVolumePair'></ClassStats><ClassStats bugs='0' size='138' interface='false' sourceFile='DataNodeVolumeMetrics.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.DataNodeVolumeMetrics'></ClassStats><ClassStats bugs='0' size='62' interface='true' sourceFile='FsDatasetSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='FsDatasetSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$Factory'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='FsDatasetSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences'></ClassStats><ClassStats bugs='1' size='17' priority_3='1' interface='false' sourceFile='FsDatasetSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='FsVolumeReference.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference'></ClassStats><ClassStats bugs='0' size='20' interface='true' sourceFile='FsVolumeSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='FsVolumeSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$BlockIterator'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='FsVolumeSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo'></ClassStats><ClassStats bugs='0' size='3' interface='false' sourceFile='FsVolumeSpi.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$VolumeCheckContext'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='LengthInputStream.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.LengthInputStream'></ClassStats><ClassStats bugs='0' size='85' interface='false' sourceFile='ReplicaInputStreams.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaInputStreams'></ClassStats><ClassStats bugs='0' size='70' interface='false' sourceFile='ReplicaOutputStreams.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='RoundRobinVolumeChoosingPolicy.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='VolumeChoosingPolicy.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.VolumeChoosingPolicy'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl' total_bugs='17' priority_3='17' total_size='4820' total_types='57'><ClassStats bugs='2' size='432' priority_3='2' interface='false' sourceFile='BlockPoolSlice.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='BlockPoolSlice.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='BlockPoolSlice.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice$2'></ClassStats><ClassStats bugs='1' size='107' priority_3='1' interface='false' sourceFile='FsDatasetAsyncDiskService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='FsDatasetAsyncDiskService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='FsDatasetAsyncDiskService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$2'></ClassStats><ClassStats bugs='0' size='62' interface='false' sourceFile='FsDatasetAsyncDiskService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask'></ClassStats><ClassStats bugs='0' size='122' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$1'></ClassStats><ClassStats bugs='0' size='78' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$PageRounder'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$State'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UsedBytesCount'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FsDatasetCache.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$Value'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsDatasetFactory.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory'></ClassStats><ClassStats bugs='3' size='1543' priority_3='3' interface='false' sourceFile='FsDatasetImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FsDatasetImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$1'></ClassStats><ClassStats bugs='1' size='85' priority_3='1' interface='false' sourceFile='FsDatasetImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FsDatasetImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$MustStopExistingWriter'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='FsDatasetImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$VolumeInfo'></ClassStats><ClassStats bugs='1' size='70' priority_3='1' interface='false' sourceFile='FsDatasetUtil.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FsDatasetUtil.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$1'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='FsDatasetUtil.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2'></ClassStats><ClassStats bugs='2' size='586' priority_3='2' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockFileFilter'></ClassStats><ClassStats bugs='0' size='159' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorState'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$FsVolumeReferenceImpl'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='FsVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$SubdirFilter'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='FsVolumeImplBuilder.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImplBuilder'></ClassStats><ClassStats bugs='3' size='221' priority_3='3' interface='false' sourceFile='FsVolumeList.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='FsVolumeList.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='FsVolumeList.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2'></ClassStats><ClassStats bugs='0' size='81' interface='false' sourceFile='MappableBlock.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlock'></ClassStats><ClassStats bugs='0' size='175' interface='false' sourceFile='ProvidedVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='ProvidedVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockIteratorState'></ClassStats><ClassStats bugs='0' size='96' interface='false' sourceFile='ProvidedVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockPoolSlice'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='ProvidedVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedVolumeDF'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='ProvidedVolumeImpl.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl'></ClassStats><ClassStats bugs='0' size='81' interface='false' sourceFile='RamDiskAsyncLazyPersistService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='RamDiskAsyncLazyPersistService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$1'></ClassStats><ClassStats bugs='1' size='37' priority_3='1' interface='false' sourceFile='RamDiskAsyncLazyPersistService.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask'></ClassStats><ClassStats bugs='1' size='92' priority_3='1' interface='false' sourceFile='RamDiskReplicaLruTracker.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='RamDiskReplicaLruTracker.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='RamDiskReplicaLruTracker.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker$RamDiskReplicaLru'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='RamDiskReplicaTracker.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker'></ClassStats><ClassStats bugs='1' size='82' priority_3='1' interface='false' sourceFile='RamDiskReplicaTracker.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica'></ClassStats><ClassStats bugs='0' size='99' interface='false' sourceFile='ReplicaMap.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='ReplicaMap.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap$1'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator'></ClassStats><ClassStats bugs='1' size='20' priority_3='1' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAggressive'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorConservative'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ReservedSpaceCalculator.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorPercentage'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='VolumeFailureInfo.java' class='org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.VolumeFailureInfo'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.metrics' total_bugs='0' total_size='579' total_types='7'><ClassStats bugs='0' size='73' interface='false' sourceFile='DataNodeDiskMetrics.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='DataNodeDiskMetrics.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics$1'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='DataNodeMetricHelper.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetricHelper'></ClassStats><ClassStats bugs='0' size='325' interface='false' sourceFile='DataNodeMetrics.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='DataNodePeerMetrics.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='FSDatasetMBean.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.FSDatasetMBean'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='OutlierDetector.java' class='org.apache.hadoop.hdfs.server.datanode.metrics.OutlierDetector'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.web' total_bugs='2' priority_3='2' total_size='383' total_types='14'><ClassStats bugs='1' size='141' priority_3='1' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$2$1'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$3'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='DatanodeHttpServer.java' class='org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$MapBasedFilterConfig'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='RestCsrfPreventionFilterHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='RestCsrfPreventionFilterHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler$NettyHttpInteraction'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='SimpleHttpProxyHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='SimpleHttpProxyHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$1'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='SimpleHttpProxyHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$2'></ClassStats><ClassStats bugs='1' size='20' priority_3='1' interface='false' sourceFile='SimpleHttpProxyHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='SimpleHttpProxyHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='URLDispatcher.java' class='org.apache.hadoop.hdfs.server.datanode.web.URLDispatcher'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.datanode.web.webhdfs' total_bugs='2' priority_3='2' total_size='455' total_types='9'><ClassStats bugs='0' size='64' interface='false' sourceFile='DataNodeUGIProvider.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DataNodeUGIProvider.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DataNodeUGIProvider.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider$2'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='ExceptionHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='HdfsWriter.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter'></ClassStats><ClassStats bugs='1' size='75' priority_3='1' interface='false' sourceFile='ParameterParser.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser'></ClassStats><ClassStats bugs='1' size='172' priority_3='1' interface='false' sourceFile='WebHdfsHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='WebHdfsHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$1'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='WebHdfsHandler.java' class='org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.diskbalancer' total_bugs='0' total_size='63' total_types='3'><ClassStats bugs='0' size='8' interface='false' sourceFile='DiskBalancerConstants.java' class='org.apache.hadoop.hdfs.server.diskbalancer.DiskBalancerConstants'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='DiskBalancerException.java' class='org.apache.hadoop.hdfs.server.diskbalancer.DiskBalancerException'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='DiskBalancerException.java' class='org.apache.hadoop.hdfs.server.diskbalancer.DiskBalancerException$Result'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.diskbalancer.command' total_bugs='3' priority_3='3' total_size='640' total_types='7'><ClassStats bugs='0' size='55' interface='false' sourceFile='CancelCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand'></ClassStats><ClassStats bugs='0' size='229' interface='false' sourceFile='Command.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.Command'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='ExecuteCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand'></ClassStats><ClassStats bugs='1' size='39' priority_3='1' interface='false' sourceFile='HelpCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand'></ClassStats><ClassStats bugs='1' size='125' priority_3='1' interface='false' sourceFile='PlanCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='QueryCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand'></ClassStats><ClassStats bugs='1' size='109' priority_3='1' interface='false' sourceFile='ReportCommand.java' class='org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.diskbalancer.connectors' total_bugs='0' total_size='100' total_types='4'><ClassStats bugs='0' size='3' interface='true' sourceFile='ClusterConnector.java' class='org.apache.hadoop.hdfs.server.diskbalancer.connectors.ClusterConnector'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='ConnectorFactory.java' class='org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='DBNameNodeConnector.java' class='org.apache.hadoop.hdfs.server.diskbalancer.connectors.DBNameNodeConnector'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='JsonNodeConnector.java' class='org.apache.hadoop.hdfs.server.diskbalancer.connectors.JsonNodeConnector'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.diskbalancer.datamodel' total_bugs='2' priority_3='2' total_size='454' total_types='6'><ClassStats bugs='1' size='121' priority_3='1' interface='false' sourceFile='DiskBalancerCluster.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DiskBalancerCluster.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster$1'></ClassStats><ClassStats bugs='0' size='92' interface='false' sourceFile='DiskBalancerDataNode.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='DiskBalancerVolume.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume'></ClassStats><ClassStats bugs='0' size='119' interface='false' sourceFile='DiskBalancerVolumeSet.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet'></ClassStats><ClassStats bugs='1' size='7' priority_3='1' interface='false' sourceFile='DiskBalancerVolumeSet.java' class='org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet$MinHeap'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.diskbalancer.planner' total_bugs='0' total_size='282' total_types='6'><ClassStats bugs='0' size='120' interface='false' sourceFile='GreedyPlanner.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner'></ClassStats><ClassStats bugs='0' size='68' interface='false' sourceFile='MoveStep.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.MoveStep'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='NodePlan.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='Planner.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.Planner'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='PlannerFactory.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.PlannerFactory'></ClassStats><ClassStats bugs='0' size='13' interface='true' sourceFile='Step.java' class='org.apache.hadoop.hdfs.server.diskbalancer.planner.Step'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.mover' total_bugs='2' priority_3='2' total_size='528' total_types='8'><ClassStats bugs='1' size='155' priority_3='1' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$1'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$Cli'></ClassStats><ClassStats bugs='1' size='18' priority_3='1' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$MLocation'></ClassStats><ClassStats bugs='0' size='181' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$Processor'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$Result'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$StorageMap'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='Mover.java' class='org.apache.hadoop.hdfs.server.mover.Mover$StorageTypeDiff'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode' priority_1='38' total_bugs='232' priority_2='90' priority_3='104' total_size='47013' total_types='531'><ClassStats bugs='0' size='76' interface='false' sourceFile='AclEntryStatusFormat.java' class='org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat'></ClassStats><ClassStats bugs='1' size='31' priority_2='1' interface='false' sourceFile='AclFeature.java' class='org.apache.hadoop.hdfs.server.namenode.AclFeature'></ClassStats><ClassStats bugs='0' size='133' interface='false' sourceFile='AclStorage.java' class='org.apache.hadoop.hdfs.server.namenode.AclStorage'></ClassStats><ClassStats bugs='0' size='183' interface='false' sourceFile='AclTransformation.java' class='org.apache.hadoop.hdfs.server.namenode.AclTransformation'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='AclTransformation.java' class='org.apache.hadoop.hdfs.server.namenode.AclTransformation$1'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='AclTransformation.java' class='org.apache.hadoop.hdfs.server.namenode.AclTransformation$ValidatedAclSpec'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='AuditLogger.java' class='org.apache.hadoop.hdfs.server.namenode.AuditLogger'></ClassStats><ClassStats bugs='0' size='156' interface='false' sourceFile='BackupImage.java' class='org.apache.hadoop.hdfs.server.namenode.BackupImage'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='BackupImage.java' class='org.apache.hadoop.hdfs.server.namenode.BackupImage$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='BackupImage.java' class='org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='BackupJournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.BackupJournalManager'></ClassStats><ClassStats bugs='1' size='169' priority_3='1' interface='false' sourceFile='BackupNode.java' class='org.apache.hadoop.hdfs.server.namenode.BackupNode'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='BackupNode.java' class='org.apache.hadoop.hdfs.server.namenode.BackupNode$1'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='BackupNode.java' class='org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext'></ClassStats><ClassStats bugs='1' size='40' priority_3='1' interface='false' sourceFile='BackupNode.java' class='org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='BackupState.java' class='org.apache.hadoop.hdfs.server.namenode.BackupState'></ClassStats><ClassStats bugs='0' size='613' interface='false' sourceFile='CacheManager.java' class='org.apache.hadoop.hdfs.server.namenode.CacheManager'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='CacheManager.java' class='org.apache.hadoop.hdfs.server.namenode.CacheManager$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='CacheManager.java' class='org.apache.hadoop.hdfs.server.namenode.CacheManager$PersistState'></ClassStats><ClassStats bugs='0' size='72' interface='false' sourceFile='CacheManager.java' class='org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat'></ClassStats><ClassStats bugs='0' size='152' interface='false' sourceFile='CachePool.java' class='org.apache.hadoop.hdfs.server.namenode.CachePool'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='CachePool.java' class='org.apache.hadoop.hdfs.server.namenode.CachePool$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='CachePool.java' class='org.apache.hadoop.hdfs.server.namenode.CachePool$DirectiveList'></ClassStats><ClassStats bugs='1' size='105' priority_2='1' interface='false' sourceFile='CachedBlock.java' class='org.apache.hadoop.hdfs.server.namenode.CachedBlock'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='CheckableNameNodeResource.java' class='org.apache.hadoop.hdfs.server.namenode.CheckableNameNodeResource'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='CheckpointConf.java' class='org.apache.hadoop.hdfs.server.namenode.CheckpointConf'></ClassStats><ClassStats bugs='1' size='31' priority_2='1' interface='false' sourceFile='CheckpointFaultInjector.java' class='org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector'></ClassStats><ClassStats bugs='0' size='87' interface='false' sourceFile='CheckpointSignature.java' class='org.apache.hadoop.hdfs.server.namenode.CheckpointSignature'></ClassStats><ClassStats bugs='3' size='160' priority_3='3' interface='false' sourceFile='Checkpointer.java' class='org.apache.hadoop.hdfs.server.namenode.Checkpointer'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='Content.java' class='org.apache.hadoop.hdfs.server.namenode.Content'></ClassStats><ClassStats bugs='1' size='41' priority_3='1' interface='false' sourceFile='ContentCounts.java' class='org.apache.hadoop.hdfs.server.namenode.ContentCounts'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='ContentCounts.java' class='org.apache.hadoop.hdfs.server.namenode.ContentCounts$1'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='ContentCounts.java' class='org.apache.hadoop.hdfs.server.namenode.ContentCounts$Builder'></ClassStats><ClassStats bugs='0' size='116' interface='false' sourceFile='ContentSummaryComputationContext.java' class='org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext'></ClassStats><ClassStats bugs='2' size='12' priority_3='2' interface='false' sourceFile='DefaultINodeAttributesProvider.java' class='org.apache.hadoop.hdfs.server.namenode.DefaultINodeAttributesProvider'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='DfsServlet.java' class='org.apache.hadoop.hdfs.server.namenode.DfsServlet'></ClassStats><ClassStats bugs='1' size='110' priority_3='1' interface='false' sourceFile='DirectoryWithQuotaFeature.java' class='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DirectoryWithQuotaFeature.java' class='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature$1'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='DirectoryWithQuotaFeature.java' class='org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature$Builder'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='EditLogBackupInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='EditLogBackupInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream$ByteBufferInputStream'></ClassStats><ClassStats bugs='0' size='72' interface='false' sourceFile='EditLogBackupOutputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream'></ClassStats><ClassStats bugs='0' size='147' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$FileLog'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$LogHeaderCorruptException'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$LogSource'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$State'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='EditLogFileInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog$1'></ClassStats><ClassStats bugs='0' size='122' interface='false' sourceFile='EditLogFileOutputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='EditLogInputException.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogInputException'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='EditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogInputStream'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='EditLogOutputStream.java' class='org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='EditsDoubleBuffer.java' class='org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='EditsDoubleBuffer.java' class='org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer$TxnBuffer'></ClassStats><ClassStats bugs='1' size='22' priority_1='1' interface='false' sourceFile='EncryptionFaultInjector.java' class='org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector'></ClassStats><ClassStats bugs='0' size='355' interface='false' sourceFile='EncryptionZoneManager.java' class='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager'></ClassStats><ClassStats bugs='1' size='41' priority_3='1' interface='false' sourceFile='EncryptionZoneManager.java' class='org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt'></ClassStats><ClassStats bugs='5' size='180' priority_1='1' priority_2='2' priority_3='2' interface='false' sourceFile='ErasureCodingPolicyManager.java' class='org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager'></ClassStats><ClassStats bugs='0' size='131' interface='false' sourceFile='FSDirAclOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirAclOp'></ClassStats><ClassStats bugs='0' size='106' interface='false' sourceFile='FSDirAppendOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp'></ClassStats><ClassStats bugs='4' size='243' priority_3='4' interface='false' sourceFile='FSDirAttrOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp'></ClassStats><ClassStats bugs='0' size='130' interface='false' sourceFile='FSDirConcatOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp'></ClassStats><ClassStats bugs='0' size='120' interface='false' sourceFile='FSDirDeleteOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp'></ClassStats><ClassStats bugs='0' size='276' interface='false' sourceFile='FSDirEncryptionZoneOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='FSDirEncryptionZoneOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$1'></ClassStats><ClassStats bugs='0' size='53' interface='false' sourceFile='FSDirEncryptionZoneOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader'></ClassStats><ClassStats bugs='1' size='12' priority_3='1' interface='false' sourceFile='FSDirEncryptionZoneOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EncryptionKeyInfo'></ClassStats><ClassStats bugs='0' size='181' interface='false' sourceFile='FSDirErasureCodingOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp'></ClassStats><ClassStats bugs='0' size='88' interface='false' sourceFile='FSDirMkdirOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp'></ClassStats><ClassStats bugs='0' size='257' interface='false' sourceFile='FSDirRenameOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp'></ClassStats><ClassStats bugs='0' size='135' interface='false' sourceFile='FSDirRenameOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameOperation'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='FSDirRenameOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult'></ClassStats><ClassStats bugs='1' size='144' priority_3='1' interface='false' sourceFile='FSDirSnapshotOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp'></ClassStats><ClassStats bugs='0' size='283' interface='false' sourceFile='FSDirStatAndListingOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='FSDirStatAndListingOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='FSDirStatAndListingOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp$GetBlockLocationsResult'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='FSDirSymlinkOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp'></ClassStats><ClassStats bugs='0' size='139' interface='false' sourceFile='FSDirTruncateOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='FSDirTruncateOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp$TruncateResult'></ClassStats><ClassStats bugs='0' size='381' interface='false' sourceFile='FSDirWriteFileOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='FSDirWriteFileOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp$FileState'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='FSDirWriteFileOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp$ValidateAddBlockResult'></ClassStats><ClassStats bugs='0' size='220' interface='false' sourceFile='FSDirXAttrOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp'></ClassStats><ClassStats bugs='2' size='851' priority_2='2' interface='false' sourceFile='FSDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirectory'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirectory$1'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='FSDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirectory$DirOp'></ClassStats><ClassStats bugs='4' size='56' priority_2='3' priority_3='1' interface='false' sourceFile='FSDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask'></ClassStats><ClassStats bugs='5' size='922' priority_1='1' priority_2='3' priority_3='1' interface='false' sourceFile='FSEditLog.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLog'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLog.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLog$1'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='FSEditLog.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLog$State'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLog.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLog$TransactionId'></ClassStats><ClassStats bugs='0' size='117' interface='false' sourceFile='FSEditLogAsync.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLogAsync.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='FSEditLogAsync.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$Edit'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='FSEditLogAsync.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$RpcEdit'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='FSEditLogAsync.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$SyncEdit'></ClassStats><ClassStats bugs='1' size='632' priority_2='1' interface='false' sourceFile='FSEditLogLoader.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSEditLogLoader.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$1'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='FSEditLogLoader.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$EditLogValidation'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='FSEditLogLoader.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader$PositionTrackingInputStream'></ClassStats><ClassStats bugs='2' size='273' priority_2='2' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$1'></ClassStats><ClassStats bugs='0' size='55' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AclEditLogUtil'></ClassStats><ClassStats bugs='0' size='75' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddBlockOp'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddCacheDirectiveInfoOp'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddCachePoolOp'></ClassStats><ClassStats bugs='0' size='266' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddCloseOp'></ClassStats><ClassStats bugs='0' size='77' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddErasureCodingPolicyOp'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AllocateBlockIdOp'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AllowSnapshotOp'></ClassStats><ClassStats bugs='0' size='65' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AppendOp'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$BlockListUpdatingOp'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$BlockTwo'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$BlockTwo$1'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CancelDelegationTokenOp'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ChecksummedReader'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ClearNSQuotaOp'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ConcatDeleteOp'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CreateSnapshotOp'></ClassStats><ClassStats bugs='0' size='65' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteOp'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteSnapshotOp'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DisableErasureCodingPolicyOp'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DisallowSnapshotOp'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$EnableErasureCodingPolicyOp'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$EndLogSegmentOp'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$GetDelegationTokenOp'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$InvalidOp'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LegacyReader'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LogSegmentOp'></ClassStats><ClassStats bugs='0' size='117' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$MkdirOp'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ModifyCacheDirectiveInfoOp'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ModifyCachePoolOp'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$1'></ClassStats><ClassStats bugs='1' size='6' priority_3='1' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$OpInstanceCache$OpInstanceCacheMap'></ClassStats><ClassStats bugs='0' size='66' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader'></ClassStats><ClassStats bugs='0' size='57' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ReassignLeaseOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveCacheDirectiveInfoOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveCachePoolOp'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveErasureCodingPolicyOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveXAttrOp'></ClassStats><ClassStats bugs='0' size='76' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOldOp'></ClassStats><ClassStats bugs='0' size='115' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOp'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameSnapshotOp'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenewDelegationTokenOp'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeFinalizeOp'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeOp'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeOp$RollbackException'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeStartOp'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetAclOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetGenstampV1Op'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetGenstampV2Op'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetNSQuotaOp'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetOwnerOp'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetPermissionsOp'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetQuotaByStorageTypeOp'></ClassStats><ClassStats bugs='0' size='59' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetQuotaOp'></ClassStats><ClassStats bugs='0' size='49' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetReplicationOp'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetStoragePolicyOp'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetXAttrOp'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$StartLogSegmentOp'></ClassStats><ClassStats bugs='0' size='114' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SymlinkOp'></ClassStats><ClassStats bugs='0' size='73' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp'></ClassStats><ClassStats bugs='0' size='103' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TruncateOp'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$UpdateBlocksOp'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$UpdateMasterKeyOp'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='FSEditLogOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Writer'></ClassStats><ClassStats bugs='0' size='142' interface='false' sourceFile='FSEditLogOpCodes.java' class='org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes'></ClassStats><ClassStats bugs='3' size='711' priority_3='3' interface='false' sourceFile='FSImage.java' class='org.apache.hadoop.hdfs.server.namenode.FSImage'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSImage.java' class='org.apache.hadoop.hdfs.server.namenode.FSImage$1'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='FSImage.java' class='org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='FSImageCompression.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageCompression'></ClassStats><ClassStats bugs='1' size='81' priority_2='1' interface='false' sourceFile='FSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormat'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='FSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$AbstractLoader'></ClassStats><ClassStats bugs='0' size='401' interface='false' sourceFile='FSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader'></ClassStats><ClassStats bugs='2' size='30' priority_3='2' interface='false' sourceFile='FSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator'></ClassStats><ClassStats bugs='0' size='157' interface='false' sourceFile='FSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver'></ClassStats><ClassStats bugs='0' size='37' interface='false' sourceFile='FSImageFormatPBINode.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSImageFormatPBINode.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$1'></ClassStats><ClassStats bugs='1' size='233' priority_3='1' interface='false' sourceFile='FSImageFormatPBINode.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader'></ClassStats><ClassStats bugs='0' size='195' interface='false' sourceFile='FSImageFormatPBINode.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$1'></ClassStats><ClassStats bugs='1' size='156' priority_3='1' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1'></ClassStats><ClassStats bugs='1' size='12' priority_2='1' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$LoaderContext'></ClassStats><ClassStats bugs='0' size='175' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SaverContext'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SaverContext$DeduplicationMap'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='FSImageFormatProtobuf.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName'></ClassStats><ClassStats bugs='0' size='131' interface='false' sourceFile='FSImagePreTransactionalStorageInspector.java' class='org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector'></ClassStats><ClassStats bugs='0' size='440' interface='false' sourceFile='FSImageSerialization.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageSerialization'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSImageSerialization.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageSerialization$1'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='FSImageSerialization.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageSerialization$TLData'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FSImageStorageInspector.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='FSImageStorageInspector.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='FSImageTransactionalStorageInspector.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector'></ClassStats><ClassStats bugs='1' size='42' priority_2='1' interface='false' sourceFile='FSImageUtil.java' class='org.apache.hadoop.hdfs.server.namenode.FSImageUtil'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='FSNDNCacheOp.java' class='org.apache.hadoop.hdfs.server.namenode.FSNDNCacheOp'></ClassStats><ClassStats bugs='9' size='3902' priority_3='9' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$CorruptFileBlockInfo'></ClassStats><ClassStats bugs='0' size='76' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger$1'></ClassStats><ClassStats bugs='1' size='58' priority_3='1' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller'></ClassStats><ClassStats bugs='1' size='25' priority_3='1' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='FSNamesystem.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystem$RecoverLeaseOp'></ClassStats><ClassStats bugs='1' size='149' priority_3='1' interface='false' sourceFile='FSNamesystemLock.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='FSNamesystemLock.java' class='org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock$1'></ClassStats><ClassStats bugs='0' size='295' interface='false' sourceFile='FSPermissionChecker.java' class='org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='FSPermissionChecker.java' class='org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker$TraverseAccessControlException'></ClassStats><ClassStats bugs='0' size='122' interface='false' sourceFile='FSTreeTraverser.java' class='org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser'></ClassStats><ClassStats bugs='0' size='3' interface='false' sourceFile='FSTreeTraverser.java' class='org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo'></ClassStats><ClassStats bugs='0' size='268' interface='false' sourceFile='FileJournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.FileJournalManager'></ClassStats><ClassStats bugs='0' size='75' interface='false' sourceFile='FileJournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='FileJournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile$1'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='FileUnderConstructionFeature.java' class='org.apache.hadoop.hdfs.server.namenode.FileUnderConstructionFeature'></ClassStats><ClassStats bugs='0' size='223' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto'></ClassStats><ClassStats bugs='0' size='142' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$1'></ClassStats><ClassStats bugs='2' size='176' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$1'></ClassStats><ClassStats bugs='1' size='159' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='149' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='217' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$1'></ClassStats><ClassStats bugs='0' size='327' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder'></ClassStats><ClassStats bugs='2' size='183' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$1'></ClassStats><ClassStats bugs='1' size='179' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$SectionOrBuilder'></ClassStats><ClassStats bugs='0' size='13' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummaryOrBuilder'></ClassStats><ClassStats bugs='2' size='117' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder'></ClassStats><ClassStats bugs='2' size='167' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$1'></ClassStats><ClassStats bugs='1' size='157' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntryOrBuilder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='117' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder'></ClassStats><ClassStats bugs='2' size='224' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1'></ClassStats><ClassStats bugs='1' size='201' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntryOrBuilder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySectionOrBuilder'></ClassStats><ClassStats bugs='2' size='117' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder'></ClassStats><ClassStats bugs='3' size='183' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$1'></ClassStats><ClassStats bugs='1' size='178' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReferenceOrBuilder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='151' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$1'></ClassStats><ClassStats bugs='2' size='160' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$1'></ClassStats><ClassStats bugs='2' size='131' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProtoOrBuilder'></ClassStats><ClassStats bugs='1' size='131' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder'></ClassStats><ClassStats bugs='2' size='183' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$1'></ClassStats><ClassStats bugs='1' size='183' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeatureOrBuilder'></ClassStats><ClassStats bugs='3' size='258' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$1'></ClassStats><ClassStats bugs='0' size='379' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Type'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Type$1'></ClassStats><ClassStats bugs='2' size='264' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$1'></ClassStats><ClassStats bugs='0' size='391' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder'></ClassStats><ClassStats bugs='0' size='18' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectoryOrBuilder'></ClassStats><ClassStats bugs='2' size='359' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$1'></ClassStats><ClassStats bugs='0' size='626' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder'></ClassStats><ClassStats bugs='0' size='31' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFileOrBuilder'></ClassStats><ClassStats bugs='0' size='16' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeOrBuilder'></ClassStats><ClassStats bugs='3' size='183' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$1'></ClassStats><ClassStats bugs='1' size='178' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlinkOrBuilder'></ClassStats><ClassStats bugs='2' size='161' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$1'></ClassStats><ClassStats bugs='1' size='138' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='149' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProtoOrBuilder'></ClassStats><ClassStats bugs='3' size='154' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$1'></ClassStats><ClassStats bugs='1' size='136' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProtoOrBuilder'></ClassStats><ClassStats bugs='2' size='149' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$1'></ClassStats><ClassStats bugs='1' size='232' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProtoOrBuilder'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='247' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$1'></ClassStats><ClassStats bugs='1' size='263' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder'></ClassStats><ClassStats bugs='0' size='17' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='183' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$1'></ClassStats><ClassStats bugs='1' size='175' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder'></ClassStats><ClassStats bugs='3' size='167' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$1'></ClassStats><ClassStats bugs='1' size='156' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKeyOrBuilder'></ClassStats><ClassStats bugs='2' size='311' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$1'></ClassStats><ClassStats bugs='1' size='363' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder'></ClassStats><ClassStats bugs='0' size='22' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistTokenOrBuilder'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='117' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$1'></ClassStats><ClassStats bugs='1' size='83' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder'></ClassStats><ClassStats bugs='3' size='135' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$1'></ClassStats><ClassStats bugs='1' size='112' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntryOrBuilder'></ClassStats><ClassStats bugs='2' size='174' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$1'></ClassStats><ClassStats bugs='1' size='158' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Type'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Type$1'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntryOrBuilder'></ClassStats><ClassStats bugs='3' size='316' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$1'></ClassStats><ClassStats bugs='0' size='365' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder'></ClassStats><ClassStats bugs='0' size='20' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiffOrBuilder'></ClassStats><ClassStats bugs='3' size='227' priority_1='1' priority_2='1' priority_3='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$1'></ClassStats><ClassStats bugs='0' size='376' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder'></ClassStats><ClassStats bugs='0' size='15' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiffOrBuilder'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='195' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$1'></ClassStats><ClassStats bugs='1' size='177' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder'></ClassStats><ClassStats bugs='2' size='164' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$1'></ClassStats><ClassStats bugs='0' size='182' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$SnapshotOrBuilder'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSectionOrBuilder'></ClassStats><ClassStats bugs='2' size='135' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$1'></ClassStats><ClassStats bugs='1' size='109' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder'></ClassStats><ClassStats bugs='2' size='167' priority_1='1' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$1'></ClassStats><ClassStats bugs='1' size='157' priority_2='1' interface='false' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder'></ClassStats><ClassStats bugs='0' size='6' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$EntryOrBuilder'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='FsImageProto.java' class='org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSectionOrBuilder'></ClassStats><ClassStats bugs='1' size='17' priority_3='1' interface='false' sourceFile='FsckServlet.java' class='org.apache.hadoop.hdfs.server.namenode.FsckServlet'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='FsckServlet.java' class='org.apache.hadoop.hdfs.server.namenode.FsckServlet$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='HdfsAuditLogger.java' class='org.apache.hadoop.hdfs.server.namenode.HdfsAuditLogger'></ClassStats><ClassStats bugs='0' size='291' interface='false' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo$UpdatedReplicationInfo'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode$Feature'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode$QuotaDelta'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='INode.java' class='org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='INodeAttributeProvider.java' class='org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='INodeAttributeProvider.java' class='org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider$AccessControlEnforcer'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='INodeAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeAttributes'></ClassStats><ClassStats bugs='1' size='49' priority_2='1' interface='false' sourceFile='INodeAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeAttributes$SnapshotCopy'></ClassStats><ClassStats bugs='0' size='414' interface='false' sourceFile='INodeDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectory'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='INodeDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectory$1'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='INodeDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectory$1$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='INodeDirectory.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectory$SnapshotAndINode'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='INodeDirectoryAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='INodeDirectoryAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes$CopyWithQuota'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='INodeDirectoryAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes$SnapshotCopy'></ClassStats><ClassStats bugs='0' size='514' interface='false' sourceFile='INodeFile.java' class='org.apache.hadoop.hdfs.server.namenode.INodeFile'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='INodeFile.java' class='org.apache.hadoop.hdfs.server.namenode.INodeFile$1'></ClassStats><ClassStats bugs='0' size='74' interface='false' sourceFile='INodeFile.java' class='org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='INodeFileAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='INodeFileAttributes.java' class='org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes$SnapshotCopy'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='INodeId.java' class='org.apache.hadoop.hdfs.server.namenode.INodeId'></ClassStats><ClassStats bugs='1' size='29' priority_3='1' interface='false' sourceFile='INodeMap.java' class='org.apache.hadoop.hdfs.server.namenode.INodeMap'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='INodeMap.java' class='org.apache.hadoop.hdfs.server.namenode.INodeMap$1'></ClassStats><ClassStats bugs='0' size='151' interface='false' sourceFile='INodeReference.java' class='org.apache.hadoop.hdfs.server.namenode.INodeReference'></ClassStats><ClassStats bugs='0' size='54' interface='false' sourceFile='INodeReference.java' class='org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference'></ClassStats><ClassStats bugs='0' size='59' interface='false' sourceFile='INodeReference.java' class='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='INodeReference.java' class='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount$1'></ClassStats><ClassStats bugs='2' size='75' priority_2='2' interface='false' sourceFile='INodeReference.java' class='org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName'></ClassStats><ClassStats bugs='1' size='55' priority_2='1' interface='false' sourceFile='INodeSymlink.java' class='org.apache.hadoop.hdfs.server.namenode.INodeSymlink'></ClassStats><ClassStats bugs='3' size='177' priority_2='3' interface='false' sourceFile='INodeWithAdditionalFields.java' class='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='INodeWithAdditionalFields.java' class='org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat'></ClassStats><ClassStats bugs='1' size='231' priority_2='1' interface='false' sourceFile='INodesInPath.java' class='org.apache.hadoop.hdfs.server.namenode.INodesInPath'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='IllegalReservedPathException.java' class='org.apache.hadoop.hdfs.server.namenode.IllegalReservedPathException'></ClassStats><ClassStats bugs='2' size='160' priority_2='1' priority_3='1' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet$1'></ClassStats><ClassStats bugs='0' size='50' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet$2'></ClassStats><ClassStats bugs='0' size='64' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet$GetImageParams'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet$ImageUploadRequest'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='ImageServlet.java' class='org.apache.hadoop.hdfs.server.namenode.ImageServlet$PutImageParams'></ClassStats><ClassStats bugs='17' size='117' priority_3='17' interface='false' sourceFile='InotifyFSEditLogOpTranslator.java' class='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='InotifyFSEditLogOpTranslator.java' class='org.apache.hadoop.hdfs.server.namenode.InotifyFSEditLogOpTranslator$1'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='JournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.JournalManager'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='JournalManager.java' class='org.apache.hadoop.hdfs.server.namenode.JournalManager$CorruptionException'></ClassStats><ClassStats bugs='6' size='211' priority_3='6' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$1'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$2'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$3'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$4'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$5'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$6'></ClassStats><ClassStats bugs='0' size='59' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$1'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$2'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$3'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$4'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$5'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$6'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$7'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='JournalSet.java' class='org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$8'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='LeaseExpiredException.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException'></ClassStats><ClassStats bugs='1' size='285' priority_3='1' interface='false' sourceFile='LeaseManager.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseManager'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='LeaseManager.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseManager$1'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='LeaseManager.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseManager$2'></ClassStats><ClassStats bugs='1' size='39' priority_3='1' interface='false' sourceFile='LeaseManager.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='LeaseManager.java' class='org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='LogsPurgeable.java' class='org.apache.hadoop.hdfs.server.namenode.LogsPurgeable'></ClassStats><ClassStats bugs='1' size='55' priority_2='1' interface='false' sourceFile='MetaRecoveryContext.java' class='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='MetaRecoveryContext.java' class='org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext$RequestStopException'></ClassStats><ClassStats bugs='0' size='489' interface='false' sourceFile='NNStorage.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorage'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='NNStorage.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeDirType'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='NNStorage.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile'></ClassStats><ClassStats bugs='2' size='108' priority_3='2' interface='false' sourceFile='NNStorageRetentionManager.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='NNStorageRetentionManager.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$1'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='NNStorageRetentionManager.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$2'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='NNStorageRetentionManager.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='NNStorageRetentionManager.java' class='org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$StoragePurger'></ClassStats><ClassStats bugs='0' size='75' interface='false' sourceFile='NNUpgradeUtil.java' class='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil'></ClassStats><ClassStats bugs='1' size='12' priority_2='1' interface='false' sourceFile='NNUpgradeUtil.java' class='org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil$1'></ClassStats><ClassStats bugs='0' size='54' interface='false' sourceFile='NameCache.java' class='org.apache.hadoop.hdfs.server.namenode.NameCache'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='NameCache.java' class='org.apache.hadoop.hdfs.server.namenode.NameCache$UseCount'></ClassStats><ClassStats bugs='3' size='957' priority_2='2' priority_3='1' interface='false' sourceFile='NameNode.java' class='org.apache.hadoop.hdfs.server.namenode.NameNode'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='NameNode.java' class='org.apache.hadoop.hdfs.server.namenode.NameNode$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='NameNode.java' class='org.apache.hadoop.hdfs.server.namenode.NameNode$2'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='NameNode.java' class='org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='NameNode.java' class='org.apache.hadoop.hdfs.server.namenode.NameNode$OperationCategory'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='NameNodeFormatException.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeFormatException'></ClassStats><ClassStats bugs='3' size='137' priority_3='3' interface='false' sourceFile='NameNodeHttpServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer'></ClassStats><ClassStats bugs='1' size='17' priority_1='1' interface='false' sourceFile='NameNodeLayoutVersion.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='NameNodeLayoutVersion.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature'></ClassStats><ClassStats bugs='0' size='38' interface='true' sourceFile='NameNodeMXBean.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean'></ClassStats><ClassStats bugs='1' size='55' priority_3='1' interface='false' sourceFile='NameNodeResourceChecker.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker'></ClassStats><ClassStats bugs='1' size='9' priority_3='1' interface='false' sourceFile='NameNodeResourceChecker.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$1'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='NameNodeResourceChecker.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='NameNodeResourcePolicy.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeResourcePolicy'></ClassStats><ClassStats bugs='1' size='1270' priority_3='1' interface='false' sourceFile='NameNodeRpcServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='NameNodeRpcServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='NameNodeRpcServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$2'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='NameNodeRpcServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$3'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='NameNodeRpcServer.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$4'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='NameNodeStatusMXBean.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='NameNodeUtils.java' class='org.apache.hadoop.hdfs.server.namenode.NameNodeUtils'></ClassStats><ClassStats bugs='3' size='700' priority_3='3' interface='false' sourceFile='NamenodeFsck.java' class='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='NamenodeFsck.java' class='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$1'></ClassStats><ClassStats bugs='0' size='72' interface='false' sourceFile='NamenodeFsck.java' class='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$ErasureCodingResult'></ClassStats><ClassStats bugs='0' size='79' interface='false' sourceFile='NamenodeFsck.java' class='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$ReplicationResult'></ClassStats><ClassStats bugs='0' size='65' interface='false' sourceFile='NamenodeFsck.java' class='org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='Namesystem.java' class='org.apache.hadoop.hdfs.server.namenode.Namesystem'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='Quota.java' class='org.apache.hadoop.hdfs.server.namenode.Quota'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='Quota.java' class='org.apache.hadoop.hdfs.server.namenode.Quota$Counts'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='QuotaByStorageTypeEntry.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='QuotaByStorageTypeEntry.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry$1'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='QuotaByStorageTypeEntry.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry$Builder'></ClassStats><ClassStats bugs='0' size='78' interface='false' sourceFile='QuotaCounts.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaCounts'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='QuotaCounts.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaCounts$1'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='QuotaCounts.java' class='org.apache.hadoop.hdfs.server.namenode.QuotaCounts$Builder'></ClassStats><ClassStats bugs='1' size='114' priority_3='1' interface='false' sourceFile='RedundantEditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='RedundantEditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='RedundantEditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$2'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='RedundantEditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='RedundantEditLogInputStream.java' class='org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$State'></ClassStats><ClassStats bugs='4' size='250' priority_3='4' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$2'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionBatch'></ClassStats><ClassStats bugs='0' size='130' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='ReencryptionHandler.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ZoneTraverseInfo'></ClassStats><ClassStats bugs='2' size='249' priority_3='2' interface='false' sourceFile='ReencryptionUpdater.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ReencryptionUpdater.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$FileEdekInfo'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='ReencryptionUpdater.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='ReencryptionUpdater.java' class='org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='SafeMode.java' class='org.apache.hadoop.hdfs.server.namenode.SafeMode'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='SaveNamespaceCancelledException.java' class='org.apache.hadoop.hdfs.server.namenode.SaveNamespaceCancelledException'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='SaveNamespaceContext.java' class='org.apache.hadoop.hdfs.server.namenode.SaveNamespaceContext'></ClassStats><ClassStats bugs='0' size='377' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$3'></ClassStats><ClassStats bugs='2' size='70' priority_3='2' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$1'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage$CheckpointLogPurger'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='SecondaryNameNode.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts$Command'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='SecondaryNameNodeInfoMXBean.java' class='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodeInfoMXBean'></ClassStats><ClassStats bugs='0' size='20' interface='false' sourceFile='SerialNumberManager.java' class='org.apache.hadoop.hdfs.server.namenode.SerialNumberManager'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='SerialNumberMap.java' class='org.apache.hadoop.hdfs.server.namenode.SerialNumberMap'></ClassStats><ClassStats bugs='1' size='65' priority_3='1' interface='false' sourceFile='StartupProgressServlet.java' class='org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet'></ClassStats><ClassStats bugs='0' size='76' interface='false' sourceFile='StoragePolicySummary.java' class='org.apache.hadoop.hdfs.server.namenode.StoragePolicySummary'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='StoragePolicySummary.java' class='org.apache.hadoop.hdfs.server.namenode.StoragePolicySummary$1'></ClassStats><ClassStats bugs='0' size='55' interface='false' sourceFile='StoragePolicySummary.java' class='org.apache.hadoop.hdfs.server.namenode.StoragePolicySummary$StorageTypeAllocation'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='StreamLimiter.java' class='org.apache.hadoop.hdfs.server.namenode.StreamLimiter'></ClassStats><ClassStats bugs='1' size='190' priority_3='1' interface='false' sourceFile='TransferFsImage.java' class='org.apache.hadoop.hdfs.server.namenode.TransferFsImage'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='TransferFsImage.java' class='org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='UnsupportedActionException.java' class='org.apache.hadoop.hdfs.server.namenode.UnsupportedActionException'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='VersionInfoMXBean.java' class='org.apache.hadoop.hdfs.server.namenode.VersionInfoMXBean'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='XAttrFeature.java' class='org.apache.hadoop.hdfs.server.namenode.XAttrFeature'></ClassStats><ClassStats bugs='1' size='79' priority_3='1' interface='false' sourceFile='XAttrFormat.java' class='org.apache.hadoop.hdfs.server.namenode.XAttrFormat'></ClassStats><ClassStats bugs='0' size='46' interface='false' sourceFile='XAttrPermissionFilter.java' class='org.apache.hadoop.hdfs.server.namenode.XAttrPermissionFilter'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='XAttrStorage.java' class='org.apache.hadoop.hdfs.server.namenode.XAttrStorage'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.ha' total_bugs='1' priority_3='1' total_size='950' total_types='18'><ClassStats bugs='0' size='26' interface='false' sourceFile='ActiveState.java' class='org.apache.hadoop.hdfs.server.namenode.ha.ActiveState'></ClassStats><ClassStats bugs='0' size='212' interface='false' sourceFile='BootstrapStandby.java' class='org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='BootstrapStandby.java' class='org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1'></ClassStats><ClassStats bugs='1' size='168' priority_3='1' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='EditLogTailer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy'></ClassStats><ClassStats bugs='0' size='12' interface='true' sourceFile='HAContext.java' class='org.apache.hadoop.hdfs.server.namenode.ha.HAContext'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='HAState.java' class='org.apache.hadoop.hdfs.server.namenode.ha.HAState'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='NameNodeHAProxyFactory.java' class='org.apache.hadoop.hdfs.server.namenode.ha.NameNodeHAProxyFactory'></ClassStats><ClassStats bugs='0' size='56' interface='false' sourceFile='RemoteNameNodeInfo.java' class='org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo'></ClassStats><ClassStats bugs='0' size='167' interface='false' sourceFile='StandbyCheckpointer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='StandbyCheckpointer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$1'></ClassStats><ClassStats bugs='0' size='86' interface='false' sourceFile='StandbyCheckpointer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='StandbyCheckpointer.java' class='org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread$1'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='StandbyState.java' class='org.apache.hadoop.hdfs.server.namenode.ha.StandbyState'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.ha.proto' priority_1='1' total_bugs='3' priority_2='2' total_size='599' total_types='6'><ClassStats bugs='0' size='22' interface='false' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$1'></ClassStats><ClassStats bugs='2' size='262' priority_1='1' priority_2='1' interface='false' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$1'></ClassStats><ClassStats bugs='1' size='285' priority_2='1' interface='false' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder'></ClassStats><ClassStats bugs='0' size='14' interface='true' sourceFile='HAZKInfoProtos.java' class='org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfoOrBuilder'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.metrics' total_bugs='0' total_size='332' total_types='4'><ClassStats bugs='0' size='7' interface='true' sourceFile='ECBlockGroupsMBean.java' class='org.apache.hadoop.hdfs.server.namenode.metrics.ECBlockGroupsMBean'></ClassStats><ClassStats bugs='0' size='35' interface='true' sourceFile='FSNamesystemMBean.java' class='org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean'></ClassStats><ClassStats bugs='0' size='282' interface='false' sourceFile='NameNodeMetrics.java' class='org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics'></ClassStats><ClassStats bugs='0' size='8' interface='true' sourceFile='ReplicatedBlocksMBean.java' class='org.apache.hadoop.hdfs.server.namenode.metrics.ReplicatedBlocksMBean'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.snapshot' total_bugs='10' priority_2='1' priority_3='9' total_size='2617' total_types='41'><ClassStats bugs='1' size='43' priority_3='1' interface='false' sourceFile='AbstractINodeDiff.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiff'></ClassStats><ClassStats bugs='1' size='144' priority_3='1' interface='false' sourceFile='AbstractINodeDiffList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList'></ClassStats><ClassStats bugs='0' size='16' interface='true' sourceFile='DiffList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffList'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='DiffList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffList$1'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='DiffListByArrayList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListByArrayList'></ClassStats><ClassStats bugs='1' size='151' priority_3='1' interface='false' sourceFile='DiffListBySkipList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DiffListBySkipList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$1'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='DiffListBySkipList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$SkipDiff'></ClassStats><ClassStats bugs='0' size='73' interface='false' sourceFile='DiffListBySkipList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$SkipListNode'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='DirectoryDiffListFactory.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory'></ClassStats><ClassStats bugs='0' size='254' interface='false' sourceFile='DirectorySnapshottableFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DirectorySnapshottableFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature$1'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='DirectorySnapshottableFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature$1$1'></ClassStats><ClassStats bugs='0' size='210' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$1'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff'></ClassStats><ClassStats bugs='1' size='69' priority_3='1' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$1'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='DirectoryWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList'></ClassStats><ClassStats bugs='0' size='3' interface='false' sourceFile='FSImageFormatPBSnapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FSImageFormatPBSnapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$1'></ClassStats><ClassStats bugs='1' size='218' priority_3='1' interface='false' sourceFile='FSImageFormatPBSnapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='FSImageFormatPBSnapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Loader$1'></ClassStats><ClassStats bugs='0' size='164' interface='false' sourceFile='FSImageFormatPBSnapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Saver'></ClassStats><ClassStats bugs='1' size='56' priority_2='1' interface='false' sourceFile='FileDiff.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiff'></ClassStats><ClassStats bugs='2' size='81' priority_3='2' interface='false' sourceFile='FileDiffList.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList'></ClassStats><ClassStats bugs='0' size='119' interface='false' sourceFile='FileWithSnapshotFeature.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshotFeature'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='Snapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='Snapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='Snapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$2'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='Snapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='Snapshot.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root$1'></ClassStats><ClassStats bugs='1' size='89' priority_3='1' interface='false' sourceFile='SnapshotDiffInfo.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='SnapshotDiffInfo.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$1'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='SnapshotDiffInfo.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$RenameEntry'></ClassStats><ClassStats bugs='0' size='105' interface='false' sourceFile='SnapshotDiffListingInfo.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo'></ClassStats><ClassStats bugs='0' size='108' interface='false' sourceFile='SnapshotFSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='SnapshotFSImageFormat.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat$ReferenceMap'></ClassStats><ClassStats bugs='1' size='240' priority_3='1' interface='false' sourceFile='SnapshotManager.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='SnapshotStatsMXBean.java' class='org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotStatsMXBean'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.startupprogress' total_bugs='2' priority_3='2' total_size='376' total_types='14'><ClassStats bugs='0' size='11' interface='false' sourceFile='AbstractTracking.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.AbstractTracking'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='Phase.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='PhaseTracking.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.PhaseTracking'></ClassStats><ClassStats bugs='2' size='65' priority_3='2' interface='false' sourceFile='StartupProgress.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='StartupProgress.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='StartupProgress.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$2'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='StartupProgress.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='StartupProgressMetrics.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressMetrics'></ClassStats><ClassStats bugs='0' size='90' interface='false' sourceFile='StartupProgressView.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='Status.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.Status'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='Step.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.Step'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='StepTracking.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StepTracking'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='StepType.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='package-info.java' class='org.apache.hadoop.hdfs.server.namenode.startupprogress.package-info'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.top' total_bugs='0' total_size='48' total_types='2'><ClassStats bugs='0' size='33' interface='false' sourceFile='TopAuditLogger.java' class='org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='TopConf.java' class='org.apache.hadoop.hdfs.server.namenode.top.TopConf'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.top.metrics' total_bugs='0' total_size='71' total_types='1'><ClassStats bugs='0' size='71' interface='false' sourceFile='TopMetrics.java' class='org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.top.window' total_bugs='0' total_size='197' total_types='9'><ClassStats bugs='0' size='39' interface='false' sourceFile='RollingWindow.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='RollingWindow.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow$1'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='RollingWindow.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow$Bucket'></ClassStats><ClassStats bugs='0' size='83' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$1'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$Op'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$RollingWindowMap'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$TopWindow'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='RollingWindowManager.java' class='org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$User'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.namenode.web.resources' total_bugs='0' total_size='606' total_types='9'><ClassStats bugs='0' size='443' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$1'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$2'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$4'></ClassStats><ClassStats bugs='0' size='19' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$5'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$5$1'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$6'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='NamenodeWebHdfsMethods.java' class='org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$7'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.server.protocol' total_bugs='37' priority_2='37' total_size='897' total_types='45'><ClassStats bugs='0' size='12' interface='false' sourceFile='BalancerBandwidthCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BalancerBandwidthCommand'></ClassStats><ClassStats bugs='8' size='49' priority_2='8' interface='false' sourceFile='BlockCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockCommand'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='BlockECReconstructionCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand'></ClassStats><ClassStats bugs='9' size='45' priority_2='9' interface='false' sourceFile='BlockECReconstructionCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand$BlockECReconstructionInfo'></ClassStats><ClassStats bugs='2' size='12' priority_2='2' interface='false' sourceFile='BlockIdCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockIdCommand'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='BlockRecoveryCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='BlockRecoveryCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock'></ClassStats><ClassStats bugs='1' size='14' priority_2='1' interface='false' sourceFile='BlockRecoveryCommand.java' class='org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='BlockReportContext.java' class='org.apache.hadoop.hdfs.server.protocol.BlockReportContext'></ClassStats><ClassStats bugs='2' size='8' priority_2='2' interface='false' sourceFile='BlocksWithLocations.java' class='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations'></ClassStats><ClassStats bugs='6' size='33' priority_2='6' interface='false' sourceFile='BlocksWithLocations.java' class='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$BlockWithLocations'></ClassStats><ClassStats bugs='2' size='19' priority_2='2' interface='false' sourceFile='BlocksWithLocations.java' class='org.apache.hadoop.hdfs.server.protocol.BlocksWithLocations$StripedBlockWithLocations'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='CheckpointCommand.java' class='org.apache.hadoop.hdfs.server.protocol.CheckpointCommand'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='DatanodeCommand.java' class='org.apache.hadoop.hdfs.server.protocol.DatanodeCommand'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='DatanodeLifelineProtocol.java' class='org.apache.hadoop.hdfs.server.protocol.DatanodeLifelineProtocol'></ClassStats><ClassStats bugs='0' size='27' interface='true' sourceFile='DatanodeProtocol.java' class='org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='DatanodeRegistration.java' class='org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DisallowedDatanodeException.java' class='org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='FenceResponse.java' class='org.apache.hadoop.hdfs.server.protocol.FenceResponse'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='FencedException.java' class='org.apache.hadoop.hdfs.server.protocol.FencedException'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='FinalizeCommand.java' class='org.apache.hadoop.hdfs.server.protocol.FinalizeCommand'></ClassStats><ClassStats bugs='2' size='20' priority_2='2' interface='false' sourceFile='HeartbeatResponse.java' class='org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse'></ClassStats><ClassStats bugs='0' size='7' interface='true' sourceFile='InterDatanodeProtocol.java' class='org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='JournalInfo.java' class='org.apache.hadoop.hdfs.server.protocol.JournalInfo'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='JournalProtocol.java' class='org.apache.hadoop.hdfs.server.protocol.JournalProtocol'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='KeyUpdateCommand.java' class='org.apache.hadoop.hdfs.server.protocol.KeyUpdateCommand'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='NNHAStatusHeartbeat.java' class='org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='NamenodeCommand.java' class='org.apache.hadoop.hdfs.server.protocol.NamenodeCommand'></ClassStats><ClassStats bugs='0' size='20' interface='true' sourceFile='NamenodeProtocol.java' class='org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol'></ClassStats><ClassStats bugs='0' size='1' interface='true' sourceFile='NamenodeProtocols.java' class='org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='NamenodeRegistration.java' class='org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='NamespaceInfo.java' class='org.apache.hadoop.hdfs.server.protocol.NamespaceInfo'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='NamespaceInfo.java' class='org.apache.hadoop.hdfs.server.protocol.NamespaceInfo$Capability'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='NodeRegistration.java' class='org.apache.hadoop.hdfs.server.protocol.NodeRegistration'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='ReceivedDeletedBlockInfo.java' class='org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='ReceivedDeletedBlockInfo.java' class='org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo$BlockStatus'></ClassStats><ClassStats bugs='0' size='7' interface='false' sourceFile='RegisterCommand.java' class='org.apache.hadoop.hdfs.server.protocol.RegisterCommand'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='RemoteEditLog.java' class='org.apache.hadoop.hdfs.server.protocol.RemoteEditLog'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='RemoteEditLog.java' class='org.apache.hadoop.hdfs.server.protocol.RemoteEditLog$1'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='RemoteEditLogManifest.java' class='org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='ReplicaRecoveryInfo.java' class='org.apache.hadoop.hdfs.server.protocol.ReplicaRecoveryInfo'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='ServerCommand.java' class='org.apache.hadoop.hdfs.server.protocol.ServerCommand'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='StorageBlockReport.java' class='org.apache.hadoop.hdfs.server.protocol.StorageBlockReport'></ClassStats><ClassStats bugs='3' size='21' priority_2='3' interface='false' sourceFile='StorageReceivedDeletedBlocks.java' class='org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks'></ClassStats><ClassStats bugs='2' size='16' priority_2='2' interface='false' sourceFile='VolumeFailureSummary.java' class='org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.tools' priority_1='1' total_bugs='9' priority_3='8' total_size='4461' total_types='75'><ClassStats bugs='0' size='56' interface='false' sourceFile='AdminHelper.java' class='org.apache.hadoop.hdfs.tools.AdminHelper'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='AdminHelper.java' class='org.apache.hadoop.hdfs.tools.AdminHelper$Command'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='AdminHelper.java' class='org.apache.hadoop.hdfs.tools.AdminHelper$HelpCommand'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$1'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$AddCacheDirectiveInfoCommand'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand'></ClassStats><ClassStats bugs='0' size='77' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$ListCacheDirectiveInfoCommand'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand'></ClassStats><ClassStats bugs='0' size='69' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCacheDirectiveInfoCommand'></ClassStats><ClassStats bugs='0' size='99' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCachePoolCommand'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfoCommand'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfosCommand'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='CacheAdmin.java' class='org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCachePoolCommand'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$1'></ClassStats><ClassStats bugs='0' size='38' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$CreateZoneCommand'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand'></ClassStats><ClassStats bugs='0' size='44' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$ListReencryptionStatusCommand'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$ListZonesCommand'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$ProvisionTrashCommand'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='CryptoAdmin.java' class='org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand'></ClassStats><ClassStats bugs='1' size='1289' priority_3='1' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$2'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand'></ClassStats><ClassStats bugs='1' size='24' priority_3='1' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$DFSAdminCommand'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$RollingUpgradeCommand'></ClassStats><ClassStats bugs='1' size='20' priority_3='1' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand'></ClassStats><ClassStats bugs='1' size='34' priority_3='1' interface='false' sourceFile='DFSAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand'></ClassStats><ClassStats bugs='1' size='56' priority_3='1' interface='false' sourceFile='DFSHAAdmin.java' class='org.apache.hadoop.hdfs.tools.DFSHAAdmin'></ClassStats><ClassStats bugs='0' size='135' interface='false' sourceFile='DFSZKFailoverController.java' class='org.apache.hadoop.hdfs.tools.DFSZKFailoverController'></ClassStats><ClassStats bugs='0' size='198' interface='false' sourceFile='DFSck.java' class='org.apache.hadoop.hdfs.tools.DFSck'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='DFSck.java' class='org.apache.hadoop.hdfs.tools.DFSck$1'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin$DebugCommand'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin$HelpCommand'></ClassStats><ClassStats bugs='0' size='70' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin$RecoverLeaseCommand'></ClassStats><ClassStats bugs='0' size='90' interface='false' sourceFile='DebugAdmin.java' class='org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyMetaCommand'></ClassStats><ClassStats bugs='0' size='123' interface='false' sourceFile='DelegationTokenFetcher.java' class='org.apache.hadoop.hdfs.tools.DelegationTokenFetcher'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='DelegationTokenFetcher.java' class='org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1'></ClassStats><ClassStats bugs='0' size='209' interface='false' sourceFile='DiskBalancerCLI.java' class='org.apache.hadoop.hdfs.tools.DiskBalancerCLI'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$1'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$DisableECPolicyCommand'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$EnableECPolicyCommand'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$GetECPolicyCommand'></ClassStats><ClassStats bugs='1' size='30' priority_3='1' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$ListECPoliciesCommand'></ClassStats><ClassStats bugs='0' size='30' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$RemoveECPolicyCommand'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand'></ClassStats><ClassStats bugs='0' size='33' interface='false' sourceFile='ECAdmin.java' class='org.apache.hadoop.hdfs.tools.ECAdmin$UnsetECPolicyCommand'></ClassStats><ClassStats bugs='0' size='70' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$BackupNodesCommandHandler'></ClassStats><ClassStats bugs='0' size='48' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$Command'></ClassStats><ClassStats bugs='0' size='26' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$CommandHandler'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$JournalNodeCommandHandler'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$NNRpcAddressesCommandHandler'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$NameNodesCommandHandler'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$PrintConfKeyCommandHandler'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='GetConf.java' class='org.apache.hadoop.hdfs.tools.GetConf$SecondaryNameNodesCommandHandler'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='GetGroups.java' class='org.apache.hadoop.hdfs.tools.GetGroups'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='HDFSConcat.java' class='org.apache.hadoop.hdfs.tools.HDFSConcat'></ClassStats><ClassStats bugs='2' size='177' priority_1='1' priority_3='1' interface='false' sourceFile='JMXGet.java' class='org.apache.hadoop.hdfs.tools.JMXGet'></ClassStats><ClassStats bugs='1' size='83' priority_3='1' interface='false' sourceFile='NNHAServiceTarget.java' class='org.apache.hadoop.hdfs.tools.NNHAServiceTarget'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$1'></ClassStats><ClassStats bugs='0' size='45' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$ListStoragePoliciesCommand'></ClassStats><ClassStats bugs='0' size='34' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand'></ClassStats><ClassStats bugs='0' size='28' interface='false' sourceFile='StoragePolicyAdmin.java' class='org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$UnsetStoragePolicyCommand'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.tools.offlineEditsViewer' total_bugs='8' priority_2='1' priority_3='7' total_size='519' total_types='14'><ClassStats bugs='0' size='17' interface='false' sourceFile='BinaryEditsVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.BinaryEditsVisitor'></ClassStats><ClassStats bugs='0' size='52' interface='false' sourceFile='OfflineEditsBinaryLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='OfflineEditsLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsLoader'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='OfflineEditsLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsLoader$OfflineEditsLoaderFactory'></ClassStats><ClassStats bugs='0' size='90' interface='false' sourceFile='OfflineEditsViewer.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='OfflineEditsViewer.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer$Flags'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='OfflineEditsVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsVisitor'></ClassStats><ClassStats bugs='0' size='24' interface='false' sourceFile='OfflineEditsVisitorFactory.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsVisitorFactory'></ClassStats><ClassStats bugs='7' size='138' priority_3='7' interface='false' sourceFile='OfflineEditsXmlLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='OfflineEditsXmlLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader$1'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='OfflineEditsXmlLoader.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader$ParseState'></ClassStats><ClassStats bugs='0' size='41' interface='false' sourceFile='StatisticsEditsVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.StatisticsEditsVisitor'></ClassStats><ClassStats bugs='1' size='26' priority_2='1' interface='false' sourceFile='TeeOutputStream.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.TeeOutputStream'></ClassStats><ClassStats bugs='0' size='59' interface='false' sourceFile='XmlEditsVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineEditsViewer.XmlEditsVisitor'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.tools.offlineImageViewer' total_bugs='6' priority_3='6' total_size='4200' total_types='55'><ClassStats bugs='0' size='69' interface='false' sourceFile='DelimitedImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.DelimitedImageVisitor'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='DepthCounter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.DepthCounter'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='FSImageHandler.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler'></ClassStats><ClassStats bugs='0' size='364' interface='false' sourceFile='FSImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='FSImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$1'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='FSImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$2'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='FSImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$3'></ClassStats><ClassStats bugs='0' size='84' interface='false' sourceFile='FileDistributionCalculator.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FileDistributionCalculator'></ClassStats><ClassStats bugs='0' size='104' interface='false' sourceFile='FileDistributionVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FileDistributionVisitor'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='FileDistributionVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FileDistributionVisitor$1'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='FileDistributionVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.FileDistributionVisitor$FileContext'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='IgnoreSnapshotException.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.IgnoreSnapshotException'></ClassStats><ClassStats bugs='0' size='3' interface='true' sourceFile='ImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoader'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='ImageLoader.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoader$LoaderFactory'></ClassStats><ClassStats bugs='0' size='399' interface='false' sourceFile='ImageLoaderCurrent.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='ImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageVisitor'></ClassStats><ClassStats bugs='0' size='203' interface='false' sourceFile='ImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageVisitor$ImageElement'></ClassStats><ClassStats bugs='0' size='49' interface='false' sourceFile='IndentedImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.IndentedImageVisitor'></ClassStats><ClassStats bugs='0' size='96' interface='false' sourceFile='LsImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.LsImageVisitor'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='LsImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.LsImageVisitor$1'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='NameDistributionVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.NameDistributionVisitor'></ClassStats><ClassStats bugs='0' size='455' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$1'></ClassStats><ClassStats bugs='1' size='118' priority_3='1' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$CacheManagerSectionProcessor'></ClassStats><ClassStats bugs='0' size='60' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$ErasureCodingSectionProcessor'></ClassStats><ClassStats bugs='0' size='25' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$FilesUnderConstructionSectionProcessor'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$INodeDirectorySectionProcessor'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$INodeReferenceSectionProcessor'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$INodeSectionProcessor'></ClassStats><ClassStats bugs='0' size='43' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$NameSectionProcessor'></ClassStats><ClassStats bugs='0' size='81' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node'></ClassStats><ClassStats bugs='0' size='108' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SecretManagerSectionProcessor'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SectionProcessor'></ClassStats><ClassStats bugs='0' size='141' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor'></ClassStats><ClassStats bugs='0' size='49' interface='false' sourceFile='OfflineImageReconstructor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotSectionProcessor'></ClassStats><ClassStats bugs='0' size='110' interface='false' sourceFile='OfflineImageViewer.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer'></ClassStats><ClassStats bugs='2' size='98' priority_3='2' interface='false' sourceFile='OfflineImageViewerPB.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB'></ClassStats><ClassStats bugs='0' size='97' interface='false' sourceFile='PBImageDelimitedTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='PBImageDelimitedTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter$1'></ClassStats><ClassStats bugs='1' size='158' priority_3='1' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1'></ClassStats><ClassStats bugs='0' size='4' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$2'></ClassStats><ClassStats bugs='0' size='32' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB'></ClassStats><ClassStats bugs='0' size='31' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$Dir'></ClassStats><ClassStats bugs='0' size='68' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap'></ClassStats><ClassStats bugs='1' size='7' priority_3='1' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache'></ClassStats><ClassStats bugs='0' size='39' interface='false' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$LevelDBStore'></ClassStats><ClassStats bugs='0' size='5' interface='true' sourceFile='PBImageTextWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$MetadataMap'></ClassStats><ClassStats bugs='1' size='592' priority_3='1' interface='false' sourceFile='PBImageXmlWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='PBImageXmlWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1'></ClassStats><ClassStats bugs='0' size='6' interface='false' sourceFile='PBImageXmlWriter.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$2'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='TextWriterImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.TextWriterImageVisitor'></ClassStats><ClassStats bugs='0' size='53' interface='false' sourceFile='WebImageViewer.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer'></ClassStats><ClassStats bugs='0' size='11' interface='false' sourceFile='WebImageViewer.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer$1'></ClassStats><ClassStats bugs='0' size='40' interface='false' sourceFile='XmlImageVisitor.java' class='org.apache.hadoop.hdfs.tools.offlineImageViewer.XmlImageVisitor'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.tools.snapshot' total_bugs='0' total_size='67' total_types='2'><ClassStats bugs='0' size='25' interface='false' sourceFile='LsSnapshottableDir.java' class='org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir'></ClassStats><ClassStats bugs='0' size='42' interface='false' sourceFile='SnapshotDiff.java' class='org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.util' total_bugs='6' priority_2='2' priority_3='4' total_size='2302' total_types='42'><ClassStats bugs='0' size='44' interface='false' sourceFile='AtomicFileOutputStream.java' class='org.apache.hadoop.hdfs.util.AtomicFileOutputStream'></ClassStats><ClassStats bugs='0' size='47' interface='false' sourceFile='BestEffortLongFile.java' class='org.apache.hadoop.hdfs.util.BestEffortLongFile'></ClassStats><ClassStats bugs='2' size='18' priority_2='2' interface='false' sourceFile='ByteArray.java' class='org.apache.hadoop.hdfs.util.ByteArray'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='Canceler.java' class='org.apache.hadoop.hdfs.util.Canceler'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='CyclicIteration.java' class='org.apache.hadoop.hdfs.util.CyclicIteration'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='CyclicIteration.java' class='org.apache.hadoop.hdfs.util.CyclicIteration$1'></ClassStats><ClassStats bugs='0' size='35' interface='false' sourceFile='CyclicIteration.java' class='org.apache.hadoop.hdfs.util.CyclicIteration$CyclicIterator'></ClassStats><ClassStats bugs='0' size='51' interface='false' sourceFile='DataTransferThrottler.java' class='org.apache.hadoop.hdfs.util.DataTransferThrottler'></ClassStats><ClassStats bugs='1' size='192' priority_3='1' interface='false' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff$Container'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff$Element'></ClassStats><ClassStats bugs='0' size='2' interface='true' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff$Processor'></ClassStats><ClassStats bugs='0' size='17' interface='false' sourceFile='Diff.java' class='org.apache.hadoop.hdfs.util.Diff$UndoInfo'></ClassStats><ClassStats bugs='0' size='86' interface='false' sourceFile='EnumCounters.java' class='org.apache.hadoop.hdfs.util.EnumCounters'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='EnumDoubles.java' class='org.apache.hadoop.hdfs.util.EnumDoubles'></ClassStats><ClassStats bugs='0' size='591' interface='false' sourceFile='FoldedTreeSet.java' class='org.apache.hadoop.hdfs.util.FoldedTreeSet'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='FoldedTreeSet.java' class='org.apache.hadoop.hdfs.util.FoldedTreeSet$1'></ClassStats><ClassStats bugs='0' size='150' interface='false' sourceFile='FoldedTreeSet.java' class='org.apache.hadoop.hdfs.util.FoldedTreeSet$Node'></ClassStats><ClassStats bugs='0' size='58' interface='false' sourceFile='FoldedTreeSet.java' class='org.apache.hadoop.hdfs.util.FoldedTreeSet$TreeSetIterator'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='Holder.java' class='org.apache.hadoop.hdfs.util.Holder'></ClassStats><ClassStats bugs='0' size='278' interface='false' sourceFile='LightWeightHashSet.java' class='org.apache.hadoop.hdfs.util.LightWeightHashSet'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='LightWeightHashSet.java' class='org.apache.hadoop.hdfs.util.LightWeightHashSet$1'></ClassStats><ClassStats bugs='0' size='12' interface='false' sourceFile='LightWeightHashSet.java' class='org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedElement'></ClassStats><ClassStats bugs='0' size='36' interface='false' sourceFile='LightWeightHashSet.java' class='org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedSetIterator'></ClassStats><ClassStats bugs='1' size='109' priority_3='1' interface='false' sourceFile='LightWeightLinkedSet.java' class='org.apache.hadoop.hdfs.util.LightWeightLinkedSet'></ClassStats><ClassStats bugs='0' size='1' interface='false' sourceFile='LightWeightLinkedSet.java' class='org.apache.hadoop.hdfs.util.LightWeightLinkedSet$1'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='LightWeightLinkedSet.java' class='org.apache.hadoop.hdfs.util.LightWeightLinkedSet$DoubleLinkedElement'></ClassStats><ClassStats bugs='0' size='23' interface='false' sourceFile='LightWeightLinkedSet.java' class='org.apache.hadoop.hdfs.util.LightWeightLinkedSet$LinkedSetIterator'></ClassStats><ClassStats bugs='1' size='71' priority_3='1' interface='false' sourceFile='MD5FileUtils.java' class='org.apache.hadoop.hdfs.util.MD5FileUtils'></ClassStats><ClassStats bugs='1' size='45' priority_3='1' interface='false' sourceFile='PersistentLongFile.java' class='org.apache.hadoop.hdfs.util.PersistentLongFile'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='ReadOnlyList.java' class='org.apache.hadoop.hdfs.util.ReadOnlyList'></ClassStats><ClassStats bugs='0' size='21' interface='false' sourceFile='ReadOnlyList.java' class='org.apache.hadoop.hdfs.util.ReadOnlyList$Util'></ClassStats><ClassStats bugs='0' size='14' interface='false' sourceFile='ReadOnlyList.java' class='org.apache.hadoop.hdfs.util.ReadOnlyList$Util$1'></ClassStats><ClassStats bugs='0' size='61' interface='false' sourceFile='ReadOnlyList.java' class='org.apache.hadoop.hdfs.util.ReadOnlyList$Util$2'></ClassStats><ClassStats bugs='0' size='29' interface='false' sourceFile='ReferenceCountMap.java' class='org.apache.hadoop.hdfs.util.ReferenceCountMap'></ClassStats><ClassStats bugs='0' size='4' interface='true' sourceFile='ReferenceCountMap.java' class='org.apache.hadoop.hdfs.util.ReferenceCountMap$ReferenceCounter'></ClassStats><ClassStats bugs='0' size='9' interface='true' sourceFile='RwLock.java' class='org.apache.hadoop.hdfs.util.RwLock'></ClassStats><ClassStats bugs='0' size='99' interface='false' sourceFile='XMLUtils.java' class='org.apache.hadoop.hdfs.util.XMLUtils'></ClassStats><ClassStats bugs='0' size='5' interface='false' sourceFile='XMLUtils.java' class='org.apache.hadoop.hdfs.util.XMLUtils$InvalidXmlException'></ClassStats><ClassStats bugs='0' size='57' interface='false' sourceFile='XMLUtils.java' class='org.apache.hadoop.hdfs.util.XMLUtils$Stanza'></ClassStats><ClassStats bugs='0' size='8' interface='false' sourceFile='XMLUtils.java' class='org.apache.hadoop.hdfs.util.XMLUtils$UnmanglingError'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.web' total_bugs='5' priority_3='5' total_size='446' total_types='6'><ClassStats bugs='1' size='35' priority_3='1' interface='false' sourceFile='AuthFilter.java' class='org.apache.hadoop.hdfs.web.AuthFilter'></ClassStats><ClassStats bugs='0' size='22' interface='false' sourceFile='AuthFilter.java' class='org.apache.hadoop.hdfs.web.AuthFilter$1'></ClassStats><ClassStats bugs='0' size='10' interface='false' sourceFile='AuthFilter.java' class='org.apache.hadoop.hdfs.web.AuthFilter$1$1'></ClassStats><ClassStats bugs='4' size='343' priority_3='4' interface='false' sourceFile='JsonUtil.java' class='org.apache.hadoop.hdfs.web.JsonUtil'></ClassStats><ClassStats bugs='0' size='27' interface='false' sourceFile='ParamFilter.java' class='org.apache.hadoop.hdfs.web.ParamFilter'></ClassStats><ClassStats bugs='0' size='9' interface='false' sourceFile='ParamFilter.java' class='org.apache.hadoop.hdfs.web.ParamFilter$1'></ClassStats></PackageStats><PackageStats package='org.apache.hadoop.hdfs.web.resources' total_bugs='0' total_size='129' total_types='6'><ClassStats bugs='0' size='54' interface='false' sourceFile='ExceptionHandler.java' class='org.apache.hadoop.hdfs.web.resources.ExceptionHandler'></ClassStats><ClassStats bugs='0' size='16' interface='false' sourceFile='NamenodeAddressParam.java' class='org.apache.hadoop.hdfs.web.resources.NamenodeAddressParam'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='TokenKindParam.java' class='org.apache.hadoop.hdfs.web.resources.TokenKindParam'></ClassStats><ClassStats bugs='0' size='13' interface='false' sourceFile='TokenServiceParam.java' class='org.apache.hadoop.hdfs.web.resources.TokenServiceParam'></ClassStats><ClassStats bugs='0' size='15' interface='false' sourceFile='UriFsPathParam.java' class='org.apache.hadoop.hdfs.web.resources.UriFsPathParam'></ClassStats><ClassStats bugs='0' size='18' interface='false' sourceFile='UserProvider.java' class='org.apache.hadoop.hdfs.web.resources.UserProvider'></ClassStats></PackageStats><FindBugsProfile><ClassProfile avgMicrosecondsPerInvocation='225' totalMilliseconds='6220' name='edu.umd.cs.findbugs.ba.npe.NullDerefAndRedundantComparisonFinder' maxMicrosecondsPerInvocation='490918' standardDeviationMicrosecondsPerInvocation='5879' invocations='27616'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='138' totalMilliseconds='5089' name='edu.umd.cs.findbugs.classfile.engine.bcel.ValueNumberDataflowFactory' maxMicrosecondsPerInvocation='582378' standardDeviationMicrosecondsPerInvocation='5406' invocations='36637'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='126' totalMilliseconds='4343' name='edu.umd.cs.findbugs.classfile.engine.bcel.TypeDataflowFactory' maxMicrosecondsPerInvocation='485700' standardDeviationMicrosecondsPerInvocation='3918' invocations='34268'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='126' totalMilliseconds='4191' name='edu.umd.cs.findbugs.classfile.engine.bcel.IsNullValueDataflowFactory' maxMicrosecondsPerInvocation='241484' standardDeviationMicrosecondsPerInvocation='1412' invocations='33040'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='131' totalMilliseconds='3813' name='edu.umd.cs.findbugs.classfile.engine.bcel.UnconditionalValueDerefDataflowFactory' maxMicrosecondsPerInvocation='386849' standardDeviationMicrosecondsPerInvocation='3410' invocations='29104'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='261' totalMilliseconds='3618' name='edu.umd.cs.findbugs.classfile.engine.ClassDataAnalysisEngine' maxMicrosecondsPerInvocation='56048' standardDeviationMicrosecondsPerInvocation='531' invocations='13816'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='132' totalMilliseconds='3581' name='edu.umd.cs.findbugs.detect.FindRefComparison$SpecialTypeAnalysis' maxMicrosecondsPerInvocation='555923' standardDeviationMicrosecondsPerInvocation='5226' invocations='26982'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='82' totalMilliseconds='2739' name='edu.umd.cs.findbugs.classfile.engine.bcel.CFGFactory' maxMicrosecondsPerInvocation='321622' standardDeviationMicrosecondsPerInvocation='3203' invocations='33040'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='227' totalMilliseconds='2599' name='edu.umd.cs.findbugs.ba.obl.ObligationAnalysis' maxMicrosecondsPerInvocation='590280' standardDeviationMicrosecondsPerInvocation='5535' invocations='11429'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='793' totalMilliseconds='1750' name='edu.umd.cs.findbugs.detect.FindOpenStream' maxMicrosecondsPerInvocation='391419' standardDeviationMicrosecondsPerInvocation='12178' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='194' totalMilliseconds='1725' name='edu.umd.cs.findbugs.classfile.engine.ClassInfoAnalysisEngine' maxMicrosecondsPerInvocation='154292' standardDeviationMicrosecondsPerInvocation='1935' invocations='8873'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='338' totalMilliseconds='1545' name='edu.umd.cs.findbugs.detect.FieldItemSummary' maxMicrosecondsPerInvocation='14852' standardDeviationMicrosecondsPerInvocation='683' invocations='4564'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='1' totalMilliseconds='1448' name='edu.umd.cs.findbugs.ba.npe.TypeQualifierNullnessAnnotationDatabase' maxMicrosecondsPerInvocation='1338' standardDeviationMicrosecondsPerInvocation='2' invocations='985662'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='315' totalMilliseconds='1440' name='edu.umd.cs.findbugs.detect.FindNoSideEffectMethods' maxMicrosecondsPerInvocation='230033' standardDeviationMicrosecondsPerInvocation='3459' invocations='4564'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='26' totalMilliseconds='991' name='edu.umd.cs.findbugs.OpcodeStack$JumpInfoFactory' maxMicrosecondsPerInvocation='26904' standardDeviationMicrosecondsPerInvocation='155' invocations='37772'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='85' totalMilliseconds='981' name='edu.umd.cs.findbugs.classfile.engine.bcel.ObligationDataflowFactory' maxMicrosecondsPerInvocation='574159' standardDeviationMicrosecondsPerInvocation='6078' invocations='11429'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='426' totalMilliseconds='941' name='edu.umd.cs.findbugs.detect.UnreadFields' maxMicrosecondsPerInvocation='414143' standardDeviationMicrosecondsPerInvocation='8829' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='26' totalMilliseconds='922' name='edu.umd.cs.findbugs.classfile.engine.bcel.MethodGenFactory' maxMicrosecondsPerInvocation='377627' standardDeviationMicrosecondsPerInvocation='2177' invocations='34953'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='407' totalMilliseconds='899' name='edu.umd.cs.findbugs.detect.LoadOfKnownNullValue' maxMicrosecondsPerInvocation='275662' standardDeviationMicrosecondsPerInvocation='5911' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='30' totalMilliseconds='849' name='edu.umd.cs.findbugs.detect.FindNullDeref$CheckCallSitesAndReturnInstructions' maxMicrosecondsPerInvocation='364403' standardDeviationMicrosecondsPerInvocation='2197' invocations='27616'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='118' totalMilliseconds='849' name='edu.umd.cs.findbugs.classfile.engine.bcel.JavaClassAnalysisEngine' maxMicrosecondsPerInvocation='311950' standardDeviationMicrosecondsPerInvocation='4136' invocations='7196'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='24' totalMilliseconds='810' name='edu.umd.cs.findbugs.classfile.engine.bcel.ConstantDataflowFactory' maxMicrosecondsPerInvocation='174896' standardDeviationMicrosecondsPerInvocation='966' invocations='33040'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='361' totalMilliseconds='796' name='edu.umd.cs.findbugs.detect.FindInconsistentSync2' maxMicrosecondsPerInvocation='308684' standardDeviationMicrosecondsPerInvocation='6583' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='331' totalMilliseconds='730' name='edu.umd.cs.findbugs.detect.DefaultEncodingDetector' maxMicrosecondsPerInvocation='521096' standardDeviationMicrosecondsPerInvocation='11097' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='321' totalMilliseconds='708' name='edu.umd.cs.findbugs.detect.URLProblems' maxMicrosecondsPerInvocation='483841' standardDeviationMicrosecondsPerInvocation='10310' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='2' totalMilliseconds='667' name='edu.umd.cs.findbugs.DetectorToDetector2Adapter' maxMicrosecondsPerInvocation='427' standardDeviationMicrosecondsPerInvocation='1' invocations='269704'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='302' totalMilliseconds='667' name='edu.umd.cs.findbugs.detect.RepeatedConditionals' maxMicrosecondsPerInvocation='384085' standardDeviationMicrosecondsPerInvocation='8180' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='275' totalMilliseconds='608' name='edu.umd.cs.findbugs.detect.FindRefComparison' maxMicrosecondsPerInvocation='251154' standardDeviationMicrosecondsPerInvocation='5350' invocations='2207'></ClassProfile><ClassProfile avgMicrosecondsPerInvocation='254' totalMilliseconds='561' name='edu.umd.cs.findbugs.detect.FindBadCast2' maxMicrosecondsPerInvocation='33686' standardDeviationMicrosecondsPerInvocation='953' invocations='2207'></ClassProfile></FindBugsProfile></FindBugsSummary><ClassFeatures></ClassFeatures><History></History></BugCollection>