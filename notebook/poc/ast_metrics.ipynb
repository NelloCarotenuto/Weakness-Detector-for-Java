{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Proof of concept of the AST metrics approach\n",
    "\n",
    "This notebook shows a proof of concept of the solution based on the application of regressiong techniques on a dataset\n",
    "that stores the counts of each different type of node of the Abstract Syntax Tree (AST) of the sources to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "In order to extract numbers out of the code, the basic idea would be to simply count the number of occurrences of each\n",
    "token in the sources, building a bag-of-words.\n",
    "\n",
    "This leads to a very large number of different tokens to build a\n",
    "regression model on, probably raising overfitting problems. It's also worth to notice that in this approach many of the\n",
    "tokens lose their meaning: variable identifiers do not carry information about their type, for example.\n",
    "\n",
    "Building the AST on the sources allows treating the code as what it actually is and not as plain text, linking each\n",
    "identifier to its type and enabling the distinction between declarations and invocations, casts and argument listings\n",
    "and so on.\n",
    "\n",
    "The process of building the AST and extracting many of what we think are relevant metrics can be achieved with a Java\n",
    "parser such as [ANTLR](https://www.antlr.org/) or [javalang](https://github.com/c2nes/javalang) (the latter is preferred\n",
    "here because of its speed): once the AST is built, it can be simply visited to increase the counters associated to the\n",
    "type of nodes encountered.\n",
    "\n",
    "A demonstration of how this approach works is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Source code to be analyzed:\n\npackage greetings;\n\nimport org.example.bots.*;\nimport org.example.greetings.*;\n\n/**\n * A very simple greeting example in Java.\n */\npublic class GentleBot extends Bot implements Greetings<Person> {\n\t\n\tpublic GentleBot(String name) {\n\t    super(name);\n\t}\n\t\n\t@Override\n\tpublic String greet(Person person) {\n\t    return \"Hello \" + person.name + \", this is \" + name + \"!\";\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\tGentleBot bob = new GentleBot(\"Bob\");\n\t\tString greeting = bob.greet(\"Alice\");\n\n\t\tSystem.out.println(greeting);\n\t}\n\n}\n\nAnalyzer output:\nCounter({'LITERALS': 5, 'STATEMENTS': 3, 'METHOD_INVOCATIONS': 3, 'IMPORT_STATEMENTS': 2, 'EXPRESSION_STATEMENTS': 2, 'METHOD_DECLARATIONS': 2, 'PUBLIC_METHOD_DECLARATIONS': 2, 'VARIABLE_DECLARATIONS': 2, 'PACKAGE_DECLARATIONS': 1, 'CLASS_DECLARATIONS': 1, 'PUBLIC_TYPE_DECLARATIONS': 1, 'SUBTYPE_DECLARATIONS': 1, 'INTERFACES_IMPLEMENTED': 1, 'CONSTRUCTOR_DECLARATIONS': 1, 'ANNOTATIONS': 1, 'STATIC_METHOD_DECLARATIONS': 1, 'GENTLEBOT_VARIABLES': 1, 'STRING_VARIABLES': 1})\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from src.processing import analyzer\n",
    "\n",
    "source = \"Example.java\"\n",
    "\n",
    "print(\"Source code to be analyzed:\")\n",
    "print()\n",
    "\n",
    "with open(f\"./{source}\") as file:\n",
    "    print(file.read())\n",
    "    print()\n",
    "\n",
    "print(\"Analyzer output:\")\n",
    "print(analyzer.analyze(f\"./{source}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model building\n",
    "\n",
    "The task is about exploring the application of Logistic Regression over the data in order to identify vulnerable files.\n",
    "Building a simple model over the data gathered without any kind of fine tuning yields results to be considered as\n",
    "a benchmark for the models to be built later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n  Safe files       0.92      0.99      0.95      7349\n  Weak files       0.27      0.03      0.05       680\n\n    accuracy                           0.91      8029\n   macro avg       0.59      0.51      0.50      8029\nweighted avg       0.86      0.91      0.88      8029\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.processing import metrics_extractor\n",
    "\n",
    "features = metrics_extractor.load_features()\n",
    "\n",
    "labels = features[\"IS_WEAK\"]\n",
    "data = features.drop(\"IS_WEAK\", axis=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "report = classification_report(test_labels, predictions, target_names=[\"Safe files\", \"Weak files\"])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This simple classifier doesn't perform well and is unable to generalize due to the imbalance of samples for the two\n",
    "classes, meaning that different weights should be given to the two classes or that the dataset should be artificially\n",
    "cut in order to restore balance, performing undersampling. \n",
    "\n",
    "Adopting the second approach, the dataset can then be built considering every positive sample and the same amount of\n",
    "negative ones picked randomly. The resulting dataset is then shuffled and split using 80% of samples for training and\n",
    "the remaining 20% for testing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from src.processing.dataset_splitter import __train_data as train_data\n",
    "from src.processing.dataset_splitter import __test_data as test_data\n",
    "from src.processing.dataset_splitter import __train_labels as train_labels\n",
    "from src.processing.dataset_splitter import __test_labels as test_labels\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_data, train_labels)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "report = classification_report(test_labels, predictions, target_names=[\"Safe files\", \"Weak files\"])\n",
    "\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n  Safe files       0.68      0.85      0.76       689\n  Weak files       0.80      0.60      0.69       689\n\n    accuracy                           0.73      1378\n   macro avg       0.74      0.73      0.72      1378\nweighted avg       0.74      0.73      0.72      1378\n\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are encouraging as the simple model is capable of generalizing although the recall over weak files should be\n",
    "improved in order to avoid leaving out too many vulnerable files.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
